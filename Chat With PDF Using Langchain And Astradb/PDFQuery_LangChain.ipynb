{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Components\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Support For Dataset Retrieval With Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
    "# we can will also initialize the DB connection:\n",
    "import cassio\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from typing_extensions import Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SetUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Your Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv(), override=True)\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ.get(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "ASTRA_DB_ID = os.environ.get(\"ASTRA_DB_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader= PdfReader(r'C:\\github_repos\\gemini-pdfs\\Chat With PDF Using Langchain And Astradb\\Sridhar Alla, Suman Kalyan Adari - Beginning MLOps with MLFlow_ Deploy Models in AWS SageMaker, Google Cloud, and Microsoft Azure-Apress (2021).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Text From PDFfile\n",
    "raw_text= ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content= page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning MLOps \n",
      "with MLFlow      \n",
      "Deploy Models in AWS SageMaker,  \n",
      "Google Cloud, and Microsoft Azure\n",
      "—\n",
      "Sridhar Alla\n",
      "Suman Kalyan AdariBeginning MLOps \n",
      "with MLFlow\n",
      "Deploy Models in AWS \n",
      "SageMaker, Google Cloud, \n",
      "and Microsoft Azure\n",
      "Sridhar Alla\n",
      "Suman Kalyan AdariBeginning MLOps with MLFlow\n",
      "ISBN-13 (pbk): 978-1-4842-6548-2   ISBN-13 (electronic): 978-1-4842-6549-9\n",
      "https://doi.org/10.1007/978-1-4842-6549-9\n",
      "Copyright © 2021 by Sridhar Alla, Suman Kalyan Adari \n",
      "This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or \n",
      "part of the material is concerned, specifically the rights of translation, reprinting, reuse of \n",
      "illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, \n",
      "and transmission or information storage and retrieval, electronic adaptation, computer software, \n",
      "or by similar or dissimilar methodology now known or hereafter developed.\n",
      "Trademarked names, logos, and images may appear in this book. Rather than use a trademark \n",
      "symbol with every occurrence of a trademarked name, logo, or image we use the names, logos, \n",
      "and images only in an editorial fashion and to the benefit of the trademark owner, with no \n",
      "intention of infringement of the trademark. \n",
      "The use in this publication of trade names, trademarks, service marks, and similar terms, even if \n",
      "they are not identified as such, is not to be taken as an expression of opinion as to whether or not \n",
      "they are subject to proprietary rights.\n",
      "While the advice and information in this book are believed to be true and accurate at the date of \n",
      "publication, neither the authors nor the editors nor the publisher can accept any legal \n",
      "responsibility for any errors or omissions that may be made. The publisher makes no warranty, \n",
      "express or implied, with respect to the material contained herein.\n",
      "Managing Director, Apress Media LLC: Welmoed Spahr\n",
      "Acquisitions Editor: Celestin Suresh John\n",
      "Development Editor: Laura Berendson\n",
      "Coordinating Editor: Aditee Mirashi\n",
      "Cover designed by eStudioCalamar\n",
      "Cover image designed by Freepik (www.freepik.com)\n",
      "Distributed to the book trade worldwide by Springer Science+Business Media New York, 1 \n",
      "New York Plaza, Suite 4600, New York, NY 10004-1562, USA. Phone 1-800-SPRINGER, fax (201) \n",
      "348-4505, e-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, \n",
      "LLC is a California LLC and the sole member (owner) is Springer Science + Business Media \n",
      "Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware  corporation.\n",
      "For information on translations, please e-mail booktranslations@springernature.com; for \n",
      "reprint, paperback, or audio rights, please e-mail bookpermissions@springernature.com.\n",
      "Apress titles may be purchased in bulk for academic, corporate, or promotional use. eBook \n",
      "versions and licenses are also available for most titles. For more information, reference our Print \n",
      "and eBook Bulk Sales web page at www.apress.com/bulk-sales.\n",
      "Any source code or other supplementary material referenced by the author in this book is \n",
      "available to readers on GitHub via the book’s product page, located at www.apress.com/ \n",
      "978-1-4842-6548-2. For more detailed information, please visit www.apress.com/source-code.\n",
      "Printed on acid-free paperSridhar Alla\n",
      "Delran, NJ, USASuman Kalyan Adari\n",
      "Tampa, FL, USAiiiTable of Contents\n",
      "Chapter 1 :   Getting Started: Data Analysis  �������������������������������������������� 1\n",
      "Introduction and Premise  �������������������������������������������������������������������������������������� 1\n",
      "Credit Card Data Set ��������������������������������������������������������������������������������������������10\n",
      "Loading the Data Set  ������������������������������������������������������������������������������������� 11\n",
      "Normal Data and Fraudulent Data  ����������������������������������������������������������������� 16\n",
      "Plotting  ���������������������������������������������������������������������������������������������������������� 19\n",
      "Summary �������������������������������������������������������������������������������������������������������������39\n",
      "Chapter 2 :   Building Models  ���������������������������������������������������������������� 41\n",
      "Introduction  ��������������������������������������������������������������������������������������������������������� 41\n",
      "Scikit-Lear n �������������������������������������������������������������������������������������������������������� 42\n",
      "Data Processing ���������������������������������������������������������������������������������������������43\n",
      "Model Training  ����������������������������������������������������������������������������������������������� 52\n",
      "Model Evaluation  ������������������������������������������������������������������������������������������� 53\n",
      "Model Validation  �������������������������������������������������������������������������������������������� 58\n",
      "PySpark  ��������������������������������������������������������������������������������������������������������������� 66About the Authors  ������������������������������������������������������������������������������� vii\n",
      "About the T echnical Reviewer  ������������������������������������������������������������� ix\n",
      "Acknowledgments  ������������������������������������������������������������������������������� xi\n",
      "Introduction  ��������������������������������������������������������������������������������������� xiiiivData Processing ���������������������������������������������������������������������������������������������67\n",
      "Model Training  ����������������������������������������������������������������������������������������������� 73\n",
      "Model Evaluation  ������������������������������������������������������������������������������������������� 74\n",
      "Summary �������������������������������������������������������������������������������������������������������������77\n",
      "Chapter 3 :   What Is MLOps?  ���������������������������������������������������������������� 79\n",
      "Introduction  ��������������������������������������������������������������������������������������������������������� 79\n",
      "MLOps Setups  ����������������������������������������������������������������������������������������������������� 87\n",
      "Manual Implementation  ��������������������������������������������������������������������������������� 88\n",
      "Continuous Model Delivery  ���������������������������������������������������������������������������� 95\n",
      "Continuous Integration/Continuous Delivery of Pipelines  ���������������������������� 105\n",
      "Pipelines and Automation  ��������������������������������������������������������������������������������� 113\n",
      "Journey Through a Pipeline  ������������������������������������������������������������������������� 114\n",
      "How to Implement MLOps  ��������������������������������������������������������������������������������� 122\n",
      "Summary �����������������������������������������������������������������������������������������������������������124\n",
      "Chapter 4 :   Introduction to MLFlow  ��������������������������������������������������� 125\n",
      "Introduction  ������������������������������������������������������������������������������������������������������� 125\n",
      "MLFlo w with Scikit-Learn  ��������������������������������������������������������������������������������� 129\n",
      "Data Processing �������������������������������������������������������������������������������������������129\n",
      "Training and Evaluating with MLFlow  ���������������������������������������������������������� 136\n",
      "Logging and Viewing MLFlow Runs  ������������������������������������������������������������� 139\n",
      "Model Validation (Parameter Tuning) with MLFlow ��������������������������������������150\n",
      "MLFlow and Other Frameworks  ������������������������������������������������������������������������ 170\n",
      "MLFlow with TensorFlow 2 �0 (Keras)  ����������������������������������������������������������� 170\n",
      "MLFlow with PyTorch �����������������������������������������������������������������������������������183\n",
      "MLFlow with PySpark  ���������������������������������������������������������������������������������� 199\n",
      "Local Model Serving  ����������������������������������������������������������������������������������������� 213Table of Con TenTsvDeploying the Model  ������������������������������������������������������������������������������������ 213\n",
      "Querying the Model  ������������������������������������������������������������������������������������� 216\n",
      "Summary �����������������������������������������������������������������������������������������������������������226\n",
      "Chapter 5 :   Deplo ying in AWS  ������������������������������������������������������������ 229\n",
      "Introduction  ������������������������������������������������������������������������������������������������������� 229\n",
      "Configuring A WS ����������������������������������������������������������������������������������������������� 232\n",
      "Deploying a Model to AWS SageMaker  ������������������������������������������������������������� 238\n",
      "Making Predictions  ������������������������������������������������������������������������������������������� 243\n",
      "Switching Models ����������������������������������������������������������������������������������������������247\n",
      "Removing Deployed Model ��������������������������������������������������������������������������������250\n",
      "Summary �����������������������������������������������������������������������������������������������������������251\n",
      "Chapter 6 :   Deplo ying in Azure  ���������������������������������������������������������� 253\n",
      "Introduction  ������������������������������������������������������������������������������������������������������� 253\n",
      "Configuring Azur e ���������������������������������������������������������������������������������������������� 255\n",
      "Deploying to Azure (Dev Stage)  ������������������������������������������������������������������������� 261\n",
      "Making Predictions  ������������������������������������������������������������������������������������������� 263\n",
      "Deploying to Production  ������������������������������������������������������������������������������������ 267\n",
      "Making Predictions  ������������������������������������������������������������������������������������������� 268\n",
      "Cleaning Up  ������������������������������������������������������������������������������������������������������� 270\n",
      "Summary �����������������������������������������������������������������������������������������������������������272\n",
      "Chapter 7 :   Deplo ying in Google  �������������������������������������������������������� 275\n",
      "Introduction  ������������������������������������������������������������������������������������������������������� 275\n",
      "Configuring Google  �������������������������������������������������������������������������������������������� 277\n",
      "Bucket Storage  �������������������������������������������������������������������������������������������� 278\n",
      "Configuring the Virtual Machine  ������������������������������������������������������������������ 281\n",
      "Configuring the Firewall  ������������������������������������������������������������������������������ 288Table of Con TenTsviDeploying and Querying the Model  ������������������������������������������������������������������� 292\n",
      "Updating and Removing a Deployment  ������������������������������������������������������������� 298\n",
      "Cleaning Up  ������������������������������������������������������������������������������������������������������� 299\n",
      "Summary �����������������������������������������������������������������������������������������������������������301\n",
      "  Appendix: Databricks  ����������������������������������������������������������������������� 303\n",
      "  Introduction  ������������������������������������������������������������������������������������������������������� 303\n",
      "  Running Experiments in Databric ks ������������������������������������������������������������������ 305\n",
      "  Deploying to Azure  �������������������������������������������������������������������������������������������� 315\n",
      "  Connecting to the Workspace  ��������������������������������������������������������������������������� 316\n",
      "  Querying the Model  ������������������������������������������������������������������������������������������� 319\n",
      "  MLFlow Model Registry  ������������������������������������������������������������������������������������ 322\n",
      "  Summary �����������������������������������������������������������������������������������������������������������326\n",
      " Index  ������������������������������������������������������������������������������������������������� 327Table of Con TenTsviiAbout the Authors\n",
      "Sridhar Alla  is the founder and CTO of \n",
      "Bluewhale.one, the company behind the \n",
      "product Sas2Py ( www.sas2py.com ), which \n",
      "focuses on the automatic conversion of SAS \n",
      "code to Python. Bluewhale also focuses on \n",
      "using AI to solve key problems ranging from \n",
      "intelligent email conversation tracking to \n",
      "issues impacting the retail industry and more. \n",
      "He has deep expertise in building AI-driven \n",
      "big data analytical practices on both the public cloud and in-house \n",
      "infrastructures. He is a published author of books and an avid presenter at \n",
      "numerous Strata, Hadoop World, Spark Summit, and other conferences. \n",
      "He also has several patents filed with the US PTO on large-scale computing \n",
      "and distributed systems. He has extensive hands-on experience in most of \n",
      "the prevalent technologies, including Spark, Flink, Hadoop, AWS, Azure, \n",
      "TensorFlow, and others. He lives with his wife, Rosie, and daughters, \n",
      "Evelyn and Madelyn, in New Jersey, United States, and in his spare time \n",
      "loves to spend time training, coaching, and attending meetups. He can be \n",
      "reached at sid@bluewhale.one .  \n",
      "viiiSuman Kalyan  Adari  is a current Senior and \n",
      "undergraduate researcher at the University \n",
      "of Florida specializing in deep learning \n",
      "and its practical use in various fields such \n",
      "as computer vision, adversarial machine \n",
      "learning, natural language processing \n",
      "(conversational AI) , anomaly detection, \n",
      "and more. He was a presenter at the IEEE \n",
      "Dependable Systems and Networks International Conference workshop \n",
      "on Dependable and Secure Machine Learning held in Portland, Oregon, \n",
      "United States in June 2019. He is also a published author, having worked \n",
      "on a book focusing on the uses of deep learning in anomaly detection.  \n",
      "He can be reached at sadari@ufl.edu .  \n",
      "abouT The auThorsixAbout the Technical Reviewer\n",
      "Pramod Singh  is a Manager, Data Science \n",
      "at Bain & Company. He has over 11 years \n",
      "of rich experience in the Data Science field \n",
      "working with multiple product- and service-\n",
      "based organizations. He has been part of \n",
      "numerous ML and AI large scale projects. He \n",
      "has published three books on large scale data \n",
      "processing  and machine learning. He is also a \n",
      "regular speaker at major AI conferences such \n",
      "as the O’Reilly AI & Strata conference.  \n",
      "xiAcknowledgments\n",
      "Sridhar Alla\n",
      "I would like to thank my wonderful wife, Rosie Sarkaria, and my \n",
      "beautiful, loving daughters, Evelyn and Madelyn, for all the love and \n",
      "patience during the many months I spent writing this book. I would also \n",
      "like to thank my parents, Ravi and Lakshmi Alla, for all the support and \n",
      "encouragement they continue to bestow upon me.\n",
      "Suman Kalyan Adari\n",
      "I would like to thank my parents, Venkata and Jyothi Adari, and my \n",
      "loving dog, Pinky, for supporting me throughout the entire process.  \n",
      "I would especially like to thank my sister, Niharika Adari, for helping me \n",
      "with edits and proofreading and helping me write the appendix chapter.xiiiIntroduction\n",
      "This book is intended for all audiences ranging from beginners at machine \n",
      "learning, to advanced machine learning engineers, or even to machine \n",
      "learning researchers who wish to learn how to better organize their \n",
      "experiments.\n",
      "The first two chapters cover the premise of the problem followed by \n",
      "the book, which is that of integrating MLOps principles into an anomaly \n",
      "detector model based on the credit card dataset. The third chapter covers \n",
      "what MLOps actually is, how it works, and why it can be useful.\n",
      "The fourth chapter goes into detail about how you can implement and \n",
      "utilize MLFlow in your existing projects to reap the benefits of MLOps with \n",
      "just a few lines of code.\n",
      "The fifth, sixth, and seventh chapters all go over how you can \n",
      "operationalize your model and deploy it on AWS, Microsoft Azure, and \n",
      "Google Cloud, respectively. The seventh chapter goes over how you \n",
      "can host a model on a virtual machine and connect to the server from \n",
      "an external source to make your predictions, so should any MLFlow \n",
      "functionality described in the book become outdated, you can always go \n",
      "for this approach and simply serve models on some cluster on the cloud.\n",
      "The last chapter, Appendix, goes over how you can utilize Databricks, \n",
      "the creators of MLFlow, to organize your MLFlow experiments and deploy \n",
      "your models.\n",
      "The goal of the book is to hopefully impart to you, the reader, \n",
      "knowledge of how you can use the power of MLFlow to easily integrate \n",
      "MLOps principles into your existing projects. Furthermore, we hope that \n",
      "you will become more familiar with how you can deploy your models to \n",
      "the cloud, allowing you to make model inferences anywhere on the planet \n",
      "so as long as you are able to connect to the cloud server hosting the model.xivAt the very least, we hope that more people do begin to adopt MLFlow \n",
      "and integrate it into their workflows, since even as a tool to organize your \n",
      "workspace, it massively improves the management of your machine \n",
      "learning experiments and allows you to keep track of the entire model \n",
      "history of a project.\n",
      "Researchers may find MLFlow to be useful when conducting \n",
      "experiments, as it allows you to log plots on top of any custom-defined \n",
      "metric of your choosing. Prototyping becomes much easier, as you can \n",
      "now keep track of that one model which worked perfectly as a proof-of-  \n",
      "concept and revert back to those same weights at any time while you keep \n",
      "tuning the hyperparameters. Hyperparameter tuning becomes much \n",
      "simpler and more organized, allowing you to run a complex script that \n",
      "searches over several different hyperparameters at once and log all of the \n",
      "results using MLFlow.\n",
      "With all the benefits that MLFlow and the corresponding MLOps \n",
      "principles offer to machine learning enthusiasts of all professions, there \n",
      "really are no downsides to integrating it into current work environments. \n",
      "With that, we hope you enjoy the rest of the book!InTrodu CTIon1© Sridhar Alla, Suman Kalyan Adari 2021 \n",
      "S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \n",
      "https://doi.org/10.1007/978-1-4842-6549-9_1CHAPTER 1\n",
      "Getting Started: Data \n",
      "Analysis\n",
      "In this chapter, we will go over the premise of the problem we are attempting \n",
      "to solve with the machine learning solution we want to operationalize. We \n",
      "will also begin data analysis and feature engineering of our data set.\n",
      " Introduction and Premise\n",
      "Welcome to Beginning MLOps with MLFlow ! In this book, we will be taking \n",
      "an example problem, developing a machine learning solution to it, and \n",
      "operationalizing our model on AWS SageMaker, Microsoft Azure, Google \n",
      "Cloud, and Datarobots. The problem we will be looking at is the issue of \n",
      "performing anomaly detection on a credit card data set. In this chapter, we \n",
      "will explore this data set and show the overall structure while explaining a \n",
      "few techniques on analyzing this data. This data set can be found at  \n",
      "www.kaggle.com/mlg-ulb/creditcardfraud .\n",
      "If you are already familiar with how to analyze data and build machine \n",
      "learning models, feel free to grab the data set and skip ahead to 3 to jump \n",
      "right into MLOps.2Otherwise, we will first go over the general process of how machine \n",
      "learning solutions are generally created. The process goes something  \n",
      "like this:\n",
      " 1. Identification of the problem:  First of all, you need \n",
      "to have an idea of what the problem is, what can be \n",
      "done about it, what has been done about it, and why \n",
      "it is a problem worth solving.\n",
      "Here’s an example of a problem: an invasive snake \n",
      "species harmful to the local environment has \n",
      "infested a region. This species is highly venomous \n",
      "and looks very similar to a harmless species of snake \n",
      "native to this same environment. Furthermore, \n",
      "the invasive species is destructive to the local \n",
      "environment and is outcompeting the local species.\n",
      "In response, the local government has issued a \n",
      "statement encouraging citizens to go out and kill \n",
      "the venomous, invasive species on sight, but the \n",
      "problem is that it turns out citizens have been killing \n",
      "the local species as well due to how easy it is to \n",
      "confuse the two species.\n",
      "What can be done about this? A possible solution \n",
      "is to use the power of machine learning and build \n",
      "an application to help citizens identify the snake \n",
      "species. What has been done about it? Perhaps \n",
      "someone released an app that does a poor job at \n",
      "distinguishing the two species, which doesn’t help \n",
      "remedy the current situation. Perhaps fliers have \n",
      "been given out, but it can be hard to identify every \n",
      "member of a species correctly based on just one \n",
      "picture.Chapter 1  Gettin G Started: data analy SiS3Why is it a problem worth solving? The native \n",
      "species is important to the local environment. \n",
      "Killing the wrong species can end up exacerbating \n",
      "the situation and lead to the invasive species \n",
      "claiming the environment over the native \n",
      "species. And so building a computer vision-based \n",
      "application that can discern between the various \n",
      "snake species (and especially the two species \n",
      "relevant to the problem) could be a great way to help \n",
      "citizens get rid of the right snake species.\n",
      " 2. Collection of data:  After you’ve identified the \n",
      "problem, you want to collect the relevant data. \n",
      "In the context of the snake species classification \n",
      "problem, you want to find images of various snake \n",
      "species in your region. The location depends on \n",
      "how big of a scale your project will operate on. Is it \n",
      "going to identify any snake in the world? Just snakes \n",
      "in Florida?\n",
      "If you can afford to do so, the more data you collect, \n",
      "the better the potential training outcomes will be. \n",
      "More training examples can introduce increased \n",
      "variety to your model, making it better in the long \n",
      "run. Deep learning models scale in performance with \n",
      "large volumes of data, so keep that in mind as well.\n",
      " 3. Data analysis:  Once you’ve collected all the raw \n",
      "data, you want to clean it up, process it, and format \n",
      "it in a way that allows you to analyze the data better.\n",
      "For images, this could be something like applying an \n",
      "algorithm to crop out unnecessary parts of the image \n",
      "to focus solely on the snake. Additionally, maybe Chapter 1  Gettin G Started: data analy SiS4you want to center-crop the image to remove all the \n",
      "extra visual information in the data sample. Either \n",
      "way, raw image data is rarely ever in good enough \n",
      "condition to be used directly; it almost always \n",
      "requires processing to get the relevant data you want.\n",
      "For unstructured data like images, formatting this \n",
      "data in a way good enough to analyze it could be \n",
      "something like creating a directory with all of the \n",
      "respective snake species and the relevant image \n",
      "data. From there, you can look at the count of \n",
      "images for each snake species class that you have \n",
      "and determine if you need to retrieve more samples \n",
      "for a particular species or not.\n",
      "For structured data, say the credit-card data set, \n",
      "processing the raw data can mean something like \n",
      "getting rid of any entries with null values in them. \n",
      "Formatting them in a way so you can analyze \n",
      "them better can involve dimensionality-reduction \n",
      "techniques such as principal component analysis \n",
      "(PCA). Note: It turns out that most of the data in the \n",
      "credit card data set has actually been processed with \n",
      "PCA in part to preserve the privacy of the users the \n",
      "data has been extracted from.\n",
      "As for the analysis, you can construct multiple \n",
      "graphs of different features to get an idea of the \n",
      "overall distribution and how the features look \n",
      "plotted against each other. This way, you can see any \n",
      "significant relationships between certain features \n",
      "that you might keep in mind when creating your \n",
      "training data.Chapter 1  Gettin G Started: data analy SiS5There are some tools you can use in order to find \n",
      "out what features have the greatest influence on \n",
      "the label, such as phi-k correlation . By allowing \n",
      "you to see the different correlation values between \n",
      "the individual features and the target label, you can \n",
      "gain a deeper understanding of the relationships \n",
      "between the features in this data set. If needed, you \n",
      "can also drop features that aren’t very influential \n",
      "from the data. In this step, you really want to get a \n",
      "solid understanding of your data so you can apply a \n",
      "model architecture that is most suitable for it.\n",
      " 4. Feature engineering and data processing:  Now you \n",
      "can use the knowledge you gained from analyzing \n",
      "the various features and their relationships to each \n",
      "another to potentially construct new features from \n",
      "combinations of several existing ones. For example, \n",
      "the Titanic data set is a great example that you can \n",
      "apply feature engineering to. In this case, you can take \n",
      "information such as class, age, fare, number of siblings, \n",
      "number of parents, and so on to create as many \n",
      "features as you can think up.\n",
      "Feature engineering is really about giving your \n",
      "model a deeper context so it can learn the task \n",
      "better. You don’t necessarily want to create random \n",
      "features for the sake of it, but something that’s \n",
      "potentially relevant like number of female relatives, \n",
      "for example. (Since females were more likely \n",
      "to survive the sinking of the Titanic, could it be \n",
      "possible that if a person had more female relatives, \n",
      "they were less likely to survive as preference was \n",
      "given to their female relatives instead?)Chapter 1  Gettin G Started: data analy SiS6The next step after feature engineering is data \n",
      "processing, which is a step involving all preparations \n",
      "made to process the data to be passed into the model. \n",
      "In the context of the snake species image data, this \n",
      "could involve normalizing all the values to be between \n",
      "0 and 1 as well as “batching” the data into groups.\n",
      "This step also usually creates several subsets of \n",
      "your initial data: a training data set , a testing data \n",
      "set, and a validation data set . We will go into more \n",
      "detail on the purpose of each of these data sets \n",
      "later. For now, a training data set  contains the data \n",
      "you want the model to learn from, the testing data \n",
      "set contains data you want to evaluate the model’s \n",
      "performance on, and the validation data set  is \n",
      "used to either select a model or help tune a model’s \n",
      "hyperparameters to draw out a better performance.\n",
      " 5. Build the model:  Now that the data processing \n",
      "is done, this step is all about selecting the proper \n",
      "architecture and building the model. For the snake \n",
      "species image data, a good choice would be to use a \n",
      "convolutional neural network (CNN) because they \n",
      "work very well for any tasks involving images. From \n",
      "there, it is up to you to define the specific architecture \n",
      "of the model with respect to its layer composition.\n",
      " 6. Training, evaluating, and validating:  When you’re \n",
      "training your CNN model, you’re usually passing in \n",
      "batches of data until the entire data makes a full pass \n",
      "through the model. From the results of this “forward \n",
      "pass, ” calculations are made that tell the model how to \n",
      "adjust the weights as they are made going backwards \n",
      "across the network in what’s called the “backward Chapter 1  Gettin G Started: data analy SiS7pass. ” The training process is essentially where the \n",
      "model learns how to perform the task and gets better \n",
      "at it the more examples it sees.\n",
      "After the training process, either the evaluation step \n",
      "or the validation step can come next. As long as the \n",
      "testing set and validation set come from different \n",
      "distributions (the validation set can be derived from \n",
      "the training set, while the testing set can be derived \n",
      "from the original data), the model is technically seeing \n",
      "new data in the evaluation and validation processes. \n",
      "The model will never learn anything from the \n",
      "evaluation data, so you can test your model anytime.\n",
      "Model evaluation is where the model’s performance \n",
      "metrics such as accuracy, precision, recall, and so on are \n",
      "evaluated on a data set that it has never seen before. We \n",
      "will go into more detail on the evaluation step once it \n",
      "becomes more relevant in the next chapter, Chapter 2.\n",
      "Depending on the context, the exact purpose of \n",
      "validation can differ, along with the question of \n",
      "whether or not evaluation should be performed first \n",
      "after training. Let’s define several sample scenarios \n",
      "where you would use validation:\n",
      "• Selecting a model architecture:  Of several \n",
      "model types or architectures, you use k-fold \n",
      "cross-validation, for example, to quickly train and \n",
      "evaluate each of the models on some data partition \n",
      "of the validation set to get an idea of how they are \n",
      "performing. This way, you can get a good idea of \n",
      "which model is performing best, allowing you to pick \n",
      "a model and continue with the rest of the process.Chapter 1  Gettin G Started: data analy SiS8• Selecting the best model:  Of several trained \n",
      "models, you can use something like k-fold cross-  \n",
      "validation to quickly evaluate each model on the \n",
      "validation data to allow you to get an idea of which \n",
      "ones are performing best.\n",
      "• Tuning hyperparameters:  Quickly train a model \n",
      "and test it with different hyperparameter setups to \n",
      "get an idea of which configurations work better. You \n",
      "can start with a broad range of hyperparameters. \n",
      "From there, you can use the results to narrow \n",
      "the range of hyperparameters until you get to a \n",
      "configuration where you are satisfied. Models \n",
      "in deep learning, for example, can have many \n",
      "hyperparameters, so using validation to tune those \n",
      "hyperparameters can work well in deep learning \n",
      "settings. Just beware of diminishing returns. After a \n",
      "certain precision with the hyperparameter setting, \n",
      "you’re not going to see that big of a performance \n",
      "boost in the model.\n",
      "• Indication of high variance:  This validation data \n",
      "is slightly different from the other three examples. \n",
      "In the case of neural networks, this data is derived \n",
      "from a small split of the training data. After one full \n",
      "pass of the training data, the model evaluates on \n",
      "this validation data to calculate metrics such as loss \n",
      "and accuracy.\n",
      "If your training accuracy is high and training loss \n",
      "is low, but the validation accuracy is low and the \n",
      "validation loss is high, that’s an indication that \n",
      "your model suffers from high variance . What Chapter 1  Gettin G Started: data analy SiS9this means is that your model has not learned to \n",
      "generalize what it is “learning” to new data, as the \n",
      "validation data in this case is comprised of data it \n",
      "has never seen before. In other words, your model \n",
      "is overfitting . The model just isn’t recreating the \n",
      "kind of performance it gets on the training data on \n",
      "new data that it hasn’t seen before.\n",
      "If your model has poor training accuracy and high \n",
      "training loss, then your model suffers from high \n",
      "bias , meaning it isn’t learning how to perform the \n",
      "task correctly on the training data at all.\n",
      "This little validation split during the training \n",
      "process can give you an early indication of when \n",
      "overfitting is occurring.\n",
      " 7. Predicting:  Once the model has been trained, \n",
      "evaluated, and validated, it is then ready to make \n",
      "predictions. In the context of the snake species \n",
      "detector, this step involves passing in visual images \n",
      "of the snake in question to get some prediction back. \n",
      "For example, if the model is supposed to detect \n",
      "the snake, draw a box around it, and label it (in an \n",
      "object detection task), it will do so and display the \n",
      "results in real time in the application.\n",
      "If it just classifies the snake in the picture, the user \n",
      "simply sends their photo of a snake to the model \n",
      "(via the application) to get a species classification \n",
      "prediction along with perhaps a probability \n",
      "confidence score.\n",
      "Hopefully now you have a better idea of what goes on when creating \n",
      "machine learning solutions.Chapter 1  Gettin G Started: data analy SiS10With all that in mind, let’s get started on the example, where you will \n",
      "use the credit card data set to build simple anomaly detection models \n",
      "using the data.\n",
      " Credit Card Data Set\n",
      "Before you perform any data analysis, you need to first collect your data. \n",
      "Once again, the data set can be found at the following link:  www.kaggle.\n",
      "com/mlg-ulb/creditcardfraud .\n",
      "Following the link, you should see something like the following in \n",
      "Figure  1-1.\n",
      "From here, you want to download the data set by clicking the \n",
      "Download (144 MB) button next to New Notebook. It should take you to \n",
      "a sign-in page if you’re not already signed in, but you should be able to \n",
      "download the data set after that.\n",
      "Figure 1-1.  Kaggle website page on the credit card dataChapter 1  Gettin G Started: data analy SiS11Once the zip file finishes downloading, simply extract it somewhere \n",
      "to reveal the credit card data set. Now let’s open up Jupyter and explore \n",
      "this data set. Before you start this step, let’s go over the exact packages and \n",
      "their versions:\n",
      "• Python 3.6.5\n",
      "• numpy 1.18.3\n",
      "• pandas 0.24.2\n",
      "• matplotlib 3.2.1\n",
      "To check your package versions, you can run a command like\n",
      "pip show package_name\n",
      "Alternatively, you can run the following code to display the version in \n",
      "the notebook itself:\n",
      "import module_name\n",
      "print(module_name.__version__)\n",
      "In this case, module_name  is the name of the package you’re importing, \n",
      "such as numpy.\n",
      " Loading the Data Set\n",
      "Let’s begin! First, open a new notebook and import all of the dependencies \n",
      "and set global parameters for this notebook:\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from pylab import rcParams\n",
      "rcParams['figure.figsize'] = 14, 8Chapter 1  Gettin G Started: data analy SiS12Refer to Figure  1-2.\n",
      "Now that you have imported the necessary libraries, you can load the \n",
      "data set. In this case, the data  folder exists in the same directory as the \n",
      "notebook file and contains the creditcard.csv  file. Here is the code:\n",
      "data_path = \"data/creditcard.csv\"\n",
      "df = pd.read_csv(data_path)\n",
      "Refer to Figure  1-3.\n",
      "Figure 1-2.  Jupyter notebook cell with some import statements \n",
      "as well as a global parameter definition for the size of all \n",
      "matplotlib plotsChapter 1  Gettin G Started: data analy SiS13Now that the data frame has been loaded, let’s take a look at its contents:\n",
      "df.head()\n",
      "Refer to Figure  1-4.\n",
      "Figure 1-3.  Defining the data path to the credit card data set .csv file, \n",
      "reading its contents, and creating a pandas data frame object\n",
      "Figure 1-4.  Calling the head() function on the data frame to display \n",
      "the first five rows of the data frameChapter 1  Gettin G Started: data analy SiS14If you are not familiar with the df.head(n)  function, it essentially \n",
      "prints the first n rows of the data frame. If you did not pass any arguments, \n",
      "like in the figure above, then the function defaults to a value of five, \n",
      "printing the first five rows of the data frame.\n",
      "Feel free to play around with that function as well as use the scroll bar \n",
      "to explore the rest of the features.\n",
      "Now, let’s look at some basic statistical values relating to the values in \n",
      "this data frame:\n",
      "df.describe()\n",
      "Refer to Figure  1-5.\n",
      "Feel free to scroll right and look at the various statistics for the rest of \n",
      "the columns. As you can see in Figure  1-5, the function generates statistical \n",
      "summaries for data in each of the columns in the data frame.\n",
      "The main takeaway here is that there are a huge number of data points. \n",
      "In fact, you can check the shape of the data frame by simply calling\n",
      "df.shape\n",
      "Refer to Figure  1-6.\n",
      "Figure 1-5.  Calling the describe() function on the data frame to get \n",
      "statistical summaries of the data in each columnChapter 1  Gettin G Started: data analy SiS15There are 284,807 rows and 31 columns in this data frame. That’s a lot \n",
      "of entries! Not only that, but if you look at Figure  1-5, you’ll see that the \n",
      "values can get really large for the column Time . In fact, keep scrolling right, \n",
      "and you’ll see that values can get very large for the column Amount  as well. \n",
      "Refer to Figure  1-7.\n",
      "As you can see, there are at least two columns with very large values. \n",
      "What this tells you is that later on, when building the various data sets for \n",
      "the model training process, you definitely need to scale down the data. \n",
      "Otherwise, such large data values can potentially mess up the training \n",
      "process.\n",
      "Figure 1-6.  Calling the shape() function on the data frame to get an \n",
      "output in the format (number_of_rows, number_of_columns)\n",
      "Figure 1-7.  Scrolling right in the output of the describe function \n",
      "reveals that the maximum data value in the column Amount is also \n",
      "very large, just like the maximum data value in the column TimeChapter 1  Gettin G Started: data analy SiS16 Normal Data and Fraudulent Data\n",
      "Since there are only two classes, normal  and fraud , let’s split up the data \n",
      "frame by class and continue with the data analysis. In the context of \n",
      "anomaly detection, the fraud  class is also the anomaly  class, hence why \n",
      "we chose to name the data frame representing fraudulent transaction \n",
      "data anomalies  and interchangeably refer to this class as either fraud  or \n",
      "anomaly .\n",
      "Here is the code:\n",
      "anomalies = df[df.Class == 1]\n",
      "normal = df[df.Class == 0]\n",
      "After that, run the following in a separate cell:\n",
      "print(f\"Anomalies: {anomalies.shape}\")\n",
      "print(f\"Normal: {normal.shape}\")\n",
      "Refer to Figure  1-8.\n",
      "From here, you can see that the data is overwhelmingly biased towards \n",
      "normal data, and that anomalies only comprise a vast minority of data \n",
      "points in the overall data set. What this tells you is that you will have to \n",
      "craft the training, evaluation, and validation sets more carefully so each of \n",
      "these sets will have a good representation of anomaly data.\n",
      "Figure 1-8.  Defining data frames for fraudulent/anomalous data \n",
      "and for normal data and printing their shapesChapter 1  Gettin G Started: data analy SiS17In fact, let’s look at this disparity in a graphical manner just to see how \n",
      "large the difference is:\n",
      "class_counts = pd.value_counts(df['Class'], sort = True)\n",
      "class_counts.plot(kind = 'bar', rot=0)\n",
      "plt.title(\"Class Distribution\")\n",
      "plt.xticks( range(2), [\"Normal\", \"Anomaly\"])\n",
      "plt.xlabel(\"Label\")\n",
      "plt.ylabel(\"Counts\")\n",
      "Refer to Figure  1-9.\n",
      "The graph visually shows the immense difference between the number \n",
      "of data values of the two classes.\n",
      "Figure 1-9.  A graph visually demonstrating the difference in counts \n",
      "for normal data and anomalous dataChapter 1  Gettin G Started: data analy SiS18So now you can begin analyzing some of the characteristics of data \n",
      "points in each class. First of all, the columns in this data set are Time , \n",
      "values V1 through V28, Amount , and Class .\n",
      "So, do anomalous data values comprise transactions with excessive \n",
      "amounts? Let’s look at some statistical summary values for Amount :\n",
      "anomalies.Amount.describe()\n",
      "Refer to Figure  1-10  for the output.\n",
      "It seems like the data is skewed right, and that anomalous transactions \n",
      "comprise values that are not very high. In fact, most of the transactions are less \n",
      "than $100, so it’s not like fraudulent transactions are high-value transactions.\n",
      "normal.Amount.describe()\n",
      "Refer to Figure  1-11  for the output.\n",
      "Figure 1-10.  Output of the describe() function on the data frame for \n",
      "fradulent values for the column Amount\n",
      "Figure 1-11.  Output of the describe() function on the data frame for \n",
      "normal values for the column AmountChapter 1  Gettin G Started: data analy SiS19If you look at the normal data, it’s even more skewed right than the \n",
      "anomalies. Most of the transactions are below $100, and some of the \n",
      "amounts can get very high to values like $25,000.\n",
      " Plotting\n",
      "Let’s now turn to a graphical approach to help visually illustrate this better. \n",
      "First, you define some functions to help plot the various columns of the \n",
      "data to make it much easier to visualize the various relationships:\n",
      "def plot_histogram(df, bins, column, log_scale= False):\n",
      "    bins = 100\n",
      "    anomalies = df[df.Class == 1]\n",
      "    normal = df[df.Class == 0]\n",
      "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex= True)\n",
      "    fig.suptitle(f'Counts of {column} by Class')\n",
      "    ax1.hist(anomalies[column], bins = bins, color=\"red\")\n",
      "    ax1.set_title('Anomaly')\n",
      "    ax2.hist(normal[column], bins = bins, color=\"orange\")\n",
      "    ax2.set_title('Normal')\n",
      "    plt.xlabel(f'{column}')\n",
      "    plt.ylabel('Count')\n",
      "    if log_scale:\n",
      "        plt.yscale('log')\n",
      "    plt.xlim((np.min(df[column]), np.max(df[column])))\n",
      "    plt.show()Chapter 1  Gettin G Started: data analy SiS20def plot_scatter(df, x_col, y_col, sharey = False):\n",
      "    anomalies = df[df.Class == 1]\n",
      "    normal = df[df.Class == 0]\n",
      "     fig, (ax1, ax2) = plt.subplots(2, 1, sharex= True, \n",
      "sharey=sharey)\n",
      "    fig.suptitle(f'{y_col} over {x_col} by Class')\n",
      "     ax1.scatter(anomalies[x_col], anomalies[y_col], color='red')\n",
      "    ax1.set_title('Anomaly')\n",
      "    ax2.scatter(normal[x_col], normal[y_col], color='orange')\n",
      "    ax2.set_title('Normal')\n",
      "    plt.xlabel(x_col)\n",
      "    plt.ylabel(y_col)\n",
      "    plt.show()\n",
      "Refer to Figure  1-12  to see the code in cells.\n",
      "Figure 1-12.  Each of the plotter functions in their own Jupyter cellsChapter 1  Gettin G Started: data analy SiS21Now, let’s start by plotting values for Amount  by Class  for the entire \n",
      "data frame:\n",
      "plt.scatter(df.Amount, df.Class)\n",
      "plt.title(\"Transaction Amounts by Class\")\n",
      "plt.ylabel(\"Class\")\n",
      "plt.yticks( range(2), [\"Normal\", \"Anomaly\"])\n",
      "plt.xlabel(\"Transaction Amounts ($)\")\n",
      "plt.show()\n",
      "Refer to Figure  1-13 .\n",
      "It seems like there are some massive outliers in the normal data set, as \n",
      "suspected. However, the graph isn’t very informative in telling you about \n",
      "Figure 1-13.  A scatterplot of data values in the data frame \n",
      "encompassing all the data values. The plotted columns are Amount \n",
      "on the x-axis and Class on the y-axisChapter 1  Gettin G Started: data analy SiS22value counts, so let’s use the plotting functions defined earlier to draw \n",
      "graphs that provide more context:\n",
      "bins = 100\n",
      "plot_histogram(df, bins, \"Amount\", log_scale= True)\n",
      "Refer to Figure  1-14 .\n",
      "From this, you can definitely notice a right skew as well as the massive \n",
      "outliers present in the normal data. Since you can’t really see much of the \n",
      "anomalies, let’s create another plot:\n",
      "plt.hist(anomalies.Amount, bins = bins, color=\"red\")\n",
      "plt.show()\n",
      "Figure 1-14.  A histogram of counts for data values organized into \n",
      "intervals in the column Amount in the data frame. The number of \n",
      "bins is 100, meaning the interval of each bar in the histogram is the \n",
      "range of the data in the column Amount divided by the number of binsChapter 1  Gettin G Started: data analy SiS23Refer to Figure  1-15 .\n",
      "The anomalies seem to be right skewed as well, but much more heavily \n",
      "so. This means that the majority of anomalous transactions actually have \n",
      "quite low transaction amounts.\n",
      "Alright, so what about time? Let’s plot another basic scatterplot:\n",
      "plt.scatter(df.Time, df.Class)\n",
      "plt.title(\"Transactions over Time by Class\")\n",
      "plt.ylabel(\"Class\")\n",
      "plt.yticks( range(2), [\"Normal\", \"Anomaly\"])\n",
      "plt.xlabel(\"Time (s)\")\n",
      "plt.show()\n",
      "Refer to Figure  1-16 .\n",
      "Figure 1-15.  A histogram of just the values in the anomaly data \n",
      "frame for the column Amount. The number of bins is also 100 here, as \n",
      "it will be for the rest of the examplesChapter 1  Gettin G Started: data analy SiS24This graph isn’t very informative, but it does tell you that fraudulent \n",
      "transactions are pretty spread out over the entire timeline. Once again, let’s \n",
      "use the plotter functions to get an idea of the counts:\n",
      "plot_scatter(df, \"Time\", \"Amount\")\n",
      "Refer to Figure  1-17 .\n",
      "Figure 1-16.  A scatterplot for values in the data frame df with data \n",
      "in the column Time on the x-axis and data in the column Class in the \n",
      "y-axisChapter 1  Gettin G Started: data analy SiS25You have a better context now, but it doesn’t seem to tell you much. \n",
      "You can see that fraudulent transactions occur throughout the entire \n",
      "timeline and that there is no specific period of time when it seems like \n",
      "higher-value transactions occur. There do seem to be two main clusters, \n",
      "but this could also be a result of the lack of data points compared to the \n",
      "normal points.\n",
      "Let’s now look at the histogram to take into account frequencies:\n",
      "plot_scatter(df, \"Time\", \"Amount\")\n",
      "Refer to Figure  1-18 .\n",
      "Figure 1-17.  Using the plot_scatter() function to plot data values for \n",
      "the columns Time on the x-axis and Amount on the y-axis in the df \n",
      "data frameChapter 1  Gettin G Started: data analy SiS26From this, you get a really good context of the amount of fraudulent/\n",
      "anomalous transactions going on over time. For the normal data, it seems \n",
      "that they occur in waves. For the anomalies, there doesn’t seem to be a \n",
      "particular peak time; they just occur throughout the entire timespan.\n",
      "It does appear that that they have defined spikes near the start of \n",
      "the first transaction, and that some of the spikes do occur where normal \n",
      "transactions are in the “trough” of the wave pattern shown. However, \n",
      "a good portion of the fraudulent transactions still occur where normal \n",
      "transactions are at a maximum.\n",
      "So what does the data for the other columns look like? Let’s look at \n",
      "some interesting plots for V1:\n",
      "plot_histogram(df, bins, \"V1\")\n",
      "Refer to Figure  1-19 .\n",
      "Figure 1-18.  Using the plot_histogram() function to plot data values \n",
      "for the column Time in the df data frameChapter 1  Gettin G Started: data analy SiS27Here, you can see a clear difference in the distribution of points for \n",
      "each class over the same V1 values. The range of values that the fraudulent \n",
      "transactions encompass extend well into the values for V1. Let’s keep \n",
      "exploring, looking at how the values for Amount  relate to V1:\n",
      "plot_scatter(df, \"Amount\", \"V1\", sharey= True)\n",
      "What the sharey  parameter does is it forces both subplots to share \n",
      "the same y-axis, meaning the plots are displayed on the same scale. You \n",
      "are specifying this so it will be easier to tell what the distribution of the \n",
      "anomalous points looks like in comparison to the normal points. Refer to \n",
      "Figure  1-20 .\n",
      "Figure 1-19.  Using the plot_histogram() function to plot the data in \n",
      "the column V1 in dfChapter 1  Gettin G Started: data analy SiS28From this graph, the fraudulent points don’t seem out of place \n",
      "compared to all of the other normal points.\n",
      "Let’s continue and look at how time relates to the values for V1:\n",
      "plot_scatter(df, \"Time\", \"V1\", sharey= True)\n",
      "Refer to Figure  1-21 .\n",
      "Figure 1-20.  Using the plot_scatter() function to plot the values in the \n",
      "columns Amount on the x-axis and V1 on the y-axis in dfChapter 1  Gettin G Started: data analy SiS29Other than a few defined spikes that stand out from where the normal \n",
      "points would have been, most of the fraudulent data in this context seems \n",
      "to blend in with the normal data.\n",
      "Doing this one at a time for all of the other values will be tedious, so \n",
      "let’s just plot them all at once using a simple script. Here is the code to plot \n",
      "all of the frequency counts for each column from V1 to V28:\n",
      "for f in range(1, 29):\n",
      "    print(f'V{f} Counts')\n",
      "    plot_histogram(df, bins, f'V{f}')\n",
      "Refer to Figure  1-22 .\n",
      "Figure 1-21.  Using the plot_scatter() function to plot the values in the \n",
      "columns Time on the x-axis and V1 on the y-axis in dfChapter 1  Gettin G Started: data analy SiS30Since the output has been minimized, hover where the bar darkens \n",
      "and click to expand the output so you can see the graphs a lot better. Refer \n",
      "to Figure  1-23 .\n",
      "Now you should see something like in Figure  1-24 .\n",
      "Figure 1-22.  A script to plot histograms using the plot_histogram() \n",
      "function for data in each column from V1 to V28 in df\n",
      "Figure 1-23.  Hovering over the bar to the left of the plots (it should \n",
      "darken and show the tooltip as shown) and clicking it to expand the \n",
      "outputChapter 1  Gettin G Started: data analy SiS31Scrolling through, you can see a lot of interesting graphs such as \n",
      "Figure  1-25  and Figure  1-26 .\n",
      "Figure 1-24.  What the expanded output should look like. All of the \n",
      "graphs should display continuously, as depicted in the figureChapter 1  Gettin G Started: data analy SiS32In this case, you can see a clear differentiation between the fraudulent \n",
      "data and the normal data that you didn’t see in the graphs earlier. And \n",
      "so, features such as V12 are certainly more important in helping give the \n",
      "model a better context.\n",
      "Figure 1-25.  A histogram of data for the column V12 in df. As you \n",
      "can see, there is a very clear deviation seen with the anomalous values \n",
      "compared to the normal values. Both plots share the same x-axis \n",
      "scale, so while the counts might be very low compared to the normal \n",
      "values, they are still spread out far more than the normal values for \n",
      "the same range of V12 column valuesChapter 1  Gettin G Started: data analy SiS33This time you can see an even bigger difference between fraudulent \n",
      "data and normal data. Once again, it’s features like V12 and V17 that hold \n",
      "the data that will help the model understand how to differentiate between \n",
      "the anomalies and the normal points.\n",
      "To minimize the output, click the same bar as earlier when you \n",
      "expanded the output. Let’s now look at how all of these data points vary \n",
      "according to time:\n",
      "for f in range(1, 29):\n",
      "    print(f'V{f} vs Time')\n",
      "    plot_scatter(df, \"Time\", f'V{f}', sharey= True)\n",
      "Once again, expand the output and explore the graphs. Refer to \n",
      "Figure  1-27  and Figure  1-28  to see some interesting results.\n",
      "Figure 1-26.  A histogram of data for the column V17 in df. Just like \n",
      "with the column V12, there is also a clear deviation seen with the \n",
      "anomalous values compared to the normal values. This indicates \n",
      "that the column V17 is more likely to help the model learn how to \n",
      "differentiate between normal and fraudulent transactions than some \n",
      "of the other columns that don’t show such a devianceChapter 1  Gettin G Started: data analy SiS34Once again, with V12 you can see a significant difference between the \n",
      "anomalies and the normal data points. A good portion of the anomalies \n",
      "remain hidden within the normal data points, but a significant amount of \n",
      "them can be differentiated from the rest.\n",
      "Figure 1-27.  The scatterplot for Time on the x-axis and V12 on the \n",
      "y-axis shows a deviation between the anomalies and the normal data \n",
      "points. Although a significant portion of the anomalies fall under the \n",
      "band of normal points, there are still a good number of anomalies \n",
      "that fall out of that range. And so you can see that against Time, the \n",
      "data for the column V12 also shows this deviation from the normal \n",
      "data pointsChapter 1  Gettin G Started: data analy SiS35The difference between the anomalies and the normal points are \n",
      "highlighted even further when looking at V17. It seems that even in \n",
      "relation to time, columns V12 and V17 hold data that best help distinguish \n",
      "fraudulent transactions from normal transactions. You can see in the \n",
      "graph that a few normal points are with the anomalous points as well, but \n",
      "hopefully the model can learn the true difference taking into account all of \n",
      "the data.\n",
      "Finally, let’s see the relationship between each of these columns and \n",
      "Amount :\n",
      "for f in range(1, 29):\n",
      "    print(f'Amount vs V{f}')\n",
      "    plot_scatter(df,   f'V{f}', \"Amount\", sharey= True)\n",
      "This time there seems to be a few more graphs more clearly showing \n",
      "the differences between the normal and fraudulent points. Refer to \n",
      "Figure  1-29 , Figure  1-30 , and Figure  1-31 .\n",
      "Figure 1-28.  The scatterplot for Time on the x-axis and V17 on the \n",
      "y-axis shows a deviation between the anomalies and the normal data \n",
      "points. As with the values for V12, you can observe another deviation \n",
      "between the normal points and the fraudulent points. In this case, the \n",
      "difference seems to be a bit more pronounced, as the anomalies seem \n",
      "to be more spread out than in Figure  1-27Chapter 1  Gettin G Started: data analy SiS36The graphs from V9 through V12 all show a clear differentiation \n",
      "between the anomalies and the normal points, even if a good portion \n",
      "of the anomalies are within the cluster of normal points. One thing to \n",
      "note is that it may not be the same anomalies that differ each time in the \n",
      "graphs, allowing the model to better learn how to differentiate between the \n",
      "anomalies and the normal points.\n",
      "Figure 1-29.  Looking at the scatterplot for Amount on the y-axis and \n",
      "V10 on the x-axis, you can see a pronounced deviation of fraudulent \n",
      "points from the normal points. For the relationship of the V columns \n",
      "against Amount, it seems that more columns show an increased \n",
      "deviation compared to the earlier plots. This difference is not so large, \n",
      "as you still see that a sizeable portion of the anomalies are within the \n",
      "normal data cluster. However, this still gives the model some context \n",
      "in how a fraudulent transaction differs from a normal transactionChapter 1  Gettin G Started: data analy SiS37You can once again see that V12 consistently differentiates between \n",
      "anomalies and normal data. However, there is still the problem of a good \n",
      "portion of the anomalies staying hidden within the normal data cluster.\n",
      "Figure 1-30.  A scatterplot for the column Amount on the y-axis and \n",
      "V12 on the x-axis. Once again, you can see a pronounced deviation of \n",
      "fraudulent points from the normal points. In this case, the majority of \n",
      "fraudulent points seem to deviate from the normal point cluster. You \n",
      "can also see that there is a band of normal points far from the main \n",
      "cluster, and that the band coincides with the anomalous data points. \n",
      "It is a possible reason to keep in mind if the model classifies points like \n",
      "these as anomaliesChapter 1  Gettin G Started: data analy SiS38You can also see that this differentiation between normal points and \n",
      "fraudulent points holds for V17 looking at transaction amounts.\n",
      "You could also look at the data for each of the V columns and plot them \n",
      "against each other, but that’s more useful to help identify precise changes \n",
      "in trends that will be more useful to know if you want to further train the \n",
      "model to improve its performance on the new data. First of all, it’s possible \n",
      "that not every feature is very significant. So, if trends do shift, it does not \n",
      "necessarily mean that the model’s performance will be downgraded.\n",
      "Thorough analysis of the data helps data scientists get a much better \n",
      "understanding of how the various data columns relate to each other and \n",
      "lets them identify if trends are shifting over time. As data is continuously \n",
      "collected over time, data biases and trends are bound to shift. So perhaps \n",
      "a year from now, it’s the column V18 that shows profound differences \n",
      "between anomalous points and normal points, and V17 now shows that \n",
      "most anomalous points are contained within the cluster of normal points.\n",
      "Figure 1-31.  A scatterplot for the column Amount on the y-axis and \n",
      "V17 on the x-axis. Just as with Figure  1-30 , you can see a deviation \n",
      "again of fraudulent points from the normal point cluster. Once again, \n",
      "the majority of fraudulent points show this deviation, but you can also \n",
      "see some normal points that coincide with these anomalous pointsChapter 1  Gettin G Started: data analy SiS39 Summary\n",
      "Data analysis is a crucial step in the process of creating a machine learning \n",
      "solution. Not only does it determine the type of model and influence the \n",
      "set of features that will be selected for the training process, but it also helps \n",
      "identify any changes in trends over time that may signify that the model \n",
      "needs to be further trained. You explored and analyzed the data in the \n",
      "credit card data set, generated many plots to get an idea of the relationship \n",
      "between the two plotted variables, and identified some features that \n",
      "distinguish between normal points and anomalies. In the next chapter, you \n",
      "will process the data to create various subsets to help train several types of \n",
      "machine learning models.Chapter 1  Gettin G Started: data analy SiS41© Sridhar Alla, Suman Kalyan Adari 2021 \n",
      "S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \n",
      "https://doi.org/10.1007/978-1-4842-6549-9_2CHAPTER 2\n",
      "Building Models\n",
      "In this chapter, we will go over how to build a simple logistic regression \n",
      "model in both scikit-learn and PySpark. We will also go over the process of \n",
      "k-fold cross validation to tune a hyperparameter in scikit-learn.\n",
      " Introduction\n",
      "In the previous chapter, you loaded the credit card data set and analyzed \n",
      "the distribution of its data. You also looked at the relationships between \n",
      "the features and got a general idea of how heavily they influence the labels.\n",
      "Now that you’ve gained a better understanding of the data set, you \n",
      "will proceed with building the models themselves. You will be using the \n",
      "same credit card data set as in the previous chapter. In this chapter, you \n",
      "will look at two frameworks: scikit-learn , and PySpark . The models you \n",
      "build in scikit-learn and in PySpark will stay relevant for the rest of the \n",
      "book, as you will be using both of them later on when you host them on \n",
      "cloud services to make predictions. You will keep it simple and construct \n",
      "logistic regression models in these two frameworks. Since the input data \n",
      "format is different for these two frameworks, you can’t just conduct the \n",
      "data processing in advance and use those train/test/validate sets for these \n",
      "two frameworks. However, it is possible to do so for scikit-learn and Keras, \n",
      "for example, depending on how the last layer is constructed in the Keras \n",
      "model.42You will be performing the validation step with the scikit-learn model \n",
      "to tune a hyperparameter. Hyperparameters  can be thought of as model-  \n",
      "related parameters that influence the training process and result.\n",
      "That being said, let’s get started with scikit-learn and build a logistic \n",
      "regression model. One thing to note is that we will provide a lot of \n",
      "commentary in the scikit-learn model that we may skip over in the PySpark \n",
      "example, so be sure to at least read through the process for scikit-learn to \n",
      "get a general idea of how train-test-validate works.\n",
      " Scikit-Learn\n",
      "Before we get started, here are the packages and their versions that you \n",
      "will need. We will provide an easy way for you to check the versions of your \n",
      "packages within the code itself.\n",
      "Here are the versions of our configuration:\n",
      "• Python 3.6.5\n",
      "• numpy 1.18.5\n",
      "• pandas 1.1.0\n",
      "• matplotlib 3.2.1\n",
      "• seaborn 0.10.1\n",
      "• sklearn 0.22.1.post1\n",
      "In the code below, you will find that some of the imports are \n",
      "unnecessary, such as importing all of sklearn when you only use a bit of \n",
      "its functionality. This is done for the purpose of displaying the version and \n",
      "such statements have a # beside them.Chapter 2  Building Models43 Data Processing\n",
      "So now, let’s begin with the import  statements:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib #\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import sklearn #\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.metrics import roc_auc_score, plot_roc_curve, \n",
      "confusion_matrix\n",
      "from sklearn.model_selection import KFold\n",
      "print(\"numpy: {}\".format(np.__version__))\n",
      "print(\"pandas: {}\".format(pd.__version__))\n",
      "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
      "print(\"seaborn: {}\".format(sns.__version__))\n",
      "print(\"sklearn: {}\".format(sklearn.__version__))\n",
      "Refer to Figure  2-1 to see the output.Chapter 2  Building Models44Now you can move on to loading the data. You will be using the same \n",
      "credit card dataset as from the previous chapter:\n",
      "data_path = \"data/creditcard.csv\"\n",
      "df = pd.read_csv(data_path)\n",
      "Refer to Figure  2-2 to see this code in a cell.\n",
      "Figure 2-1.  The output showing the printed versions of the modules \n",
      "you will need. Some modules are imported for the sake of printing the \n",
      "versions and have been marked with a # beside them to indicate that \n",
      "they are not necessary to run the code\n",
      "Figure 2-2.  Loading the data frame using pandas. The credit card \n",
      "data set is located in a folder called data , which is located in the same \n",
      "directory as the notebook fileChapter 2  Building Models45There shouldn’t be any output from loading the data frame. To see the \n",
      "data frame you just loaded, call the following to ensure it has read the data \n",
      "correctly:\n",
      "df.head()\n",
      "You should see something like in Figure  2-3.\n",
      "If you remember from the previous chapter, there is a massive \n",
      "imbalance in the distribution of data between the normal data and the \n",
      "anomalies. Because of this, you are going to take a slightly alternative \n",
      "approach in how you craft this data.\n",
      "This is where data analysis comes into play. Because you know that \n",
      "a massive disparity between the data counts in each class exists, you will \n",
      "now take care to specially craft the data sets so that it is ensured that a \n",
      "good amount of anomalies end up in each data set. If you simply select \n",
      "100,000 data points from df, split it into your training/test/validate sets \n",
      "and continue, it is entirely possible that very few or even no anomalies \n",
      "end up in one or more of those sets. At that point, you would have a lot of \n",
      "trouble in getting the model to properly learn this task.\n",
      "This is why you will be splitting up the anomalies and normal points to \n",
      "create your training/test/validate sets.\n",
      "With that in mind, let’s create data frames for the normal points and for \n",
      "the fraudulent points:\n",
      "Figure 2-3.  The output of the head() function. The data has loaded \n",
      "correctly, and you can see the first five rows of the data frameChapter 2  Building Models46normal = df[df.Class == 0].sample(frac=0.5, random_state=2020).\n",
      "reset_index(drop= True)\n",
      "anomaly = df[df.Class == 1]\n",
      "You have set the random_state  to a specific value so that the results of \n",
      "the random sampling should be the same no matter how many times you \n",
      "repeat it, helping with reproducibility. Unfortunately, given the nature of \n",
      "how models learn, you cannot expect to get the same results every time for \n",
      "something like neural networks, for example.\n",
      "In the code, you filter out the respective values by class, and sample \n",
      "50% of the entire data frame’s normal points to comprise the normal data \n",
      "in this context.\n",
      "Refer to Figure  2-4 to see this code in a cell.\n",
      "You can add some code to check the shapes as well:\n",
      "print(f\"Normal: {normal.shape}\")\n",
      "print(f\"Anomaly: {anomaly.shape}\")\n",
      "Refer to Figure  2-5 for the output.\n",
      "Figure 2-5.  Printing the shapes of the normal and anomaly data \n",
      "frames. There is a clear difference in the number of entries in the two \n",
      "data frames\n",
      "Figure 2-4.  Filtering the data frame values by class to create the \n",
      "normal and anomaly data frames. The normal data frame contains \n",
      "50% of all normal data points, randomly selected as determined by \n",
      "the seed (random_state)Chapter 2  Building Models47As you can see, there is still a big disparity between the normal points \n",
      "and the anomalies. In the case of logistic regression, the model is still \n",
      "able to learn how to distinguish between the two, but in the case of neural \n",
      "networks, for example, this disparity means the model never really learns \n",
      "how to classify anomalies. However, as you will see later in this chapter, \n",
      "you can tell the model to weigh the anomalies far more in its learning \n",
      "process compared to the normal points.\n",
      "Now you can start creating the train/test/validate split. However, scikit-  \n",
      "learn provides functionality to create train/test splits only. To get around \n",
      "that, you will create train and test sets, and then split the train set again \n",
      "into train and validate sets.\n",
      "First, you will split the data into train and test data, keeping the normal \n",
      "points and anomalies separate. To do this, you will use the train_test_\n",
      "split()  function from scikit-learn. Commonly passed parameters are\n",
      "• x: The x set you want to split up\n",
      "• y: The y set you want to split up corresponding to  \n",
      "the x set\n",
      "• test_size : The proportion of data in x and y that you \n",
      "want to randomly sample for the test set.\n",
      "And so, to split up x and y into your training and testing sets, you may \n",
      "see code like the following:\n",
      "x_train, x_test, y_train, y_test = train_test_split(x, y, test_\n",
      "size=0.2, random_state = 2020)\n",
      "Just like earlier, random_state  is setting the random seed so that every \n",
      "time you run it, the data will be split the same way.Chapter 2  Building Models48If you don’t pass in the y parameter, you simply get a split on the x \n",
      "data. And so, keeping that in mind, let’s split up your normal points and \n",
      "anomalies into training and testing sets:\n",
      "normal_train, normal_test = train_test_split(normal, test_size \n",
      "= 0.2, random_state = 2020)\n",
      "anomaly_train, anomaly_test = train_test_split(anomaly,  \n",
      "test_size = 0.2, random_state = 2020)\n",
      "There should be no output but refer to Figure  2-6 to see the code  \n",
      "in a cell.\n",
      "Now, you can create your training and validation sets by calling the \n",
      "same function on the respective training sets. You don’t want to split it \n",
      "by 20% again, though, since the training set is already 80% of the original \n",
      "data set. If you used a 20% split again, the validation set would be 16% of \n",
      "the original data, and the training set would be 64% of the original data. \n",
      "You will instead be doing a 60-20-20 split for the training, testing, and \n",
      "validation data, respectively, and so you will be using a new test_size  \n",
      "value of 0.25 to ensure these proportions hold (0.25 * 0.8 = 0.2).\n",
      "With that in mind, let’s create your training and validation splits:\n",
      "normal_train, normal_validate = train_test_split(normal_train, \n",
      "test_size = 0.25, random_state = 2020)\n",
      "anomaly_train, anomaly_validate = train_test_split(anomaly_\n",
      "train, test_size = 0.25, random_state = 2020)\n",
      "Refer to Figure  2-7 to see the code in a cell.\n",
      "Figure 2-6.  Splitting the normal and anomaly data frames into train \n",
      "and test subsets. The respective test sets comprise 20% of the original setsChapter 2  Building Models49To create your final training, testing, and validation sets, you have to \n",
      "concatenate the respective normal and anomaly data splits.\n",
      "First, you define x_train , x_test , and x_validate :\n",
      "x_train = pd.concat((normal_train, anomaly_train))\n",
      "x_test = pd.concat((normal_test, anomaly_test))\n",
      "x_validate = pd.concat((normal_validate, anomaly_validate))\n",
      "Next, you define y_train , y_test , and y_validate :\n",
      "y_train = np.array(x_train[\"Class\"])\n",
      "y_test = np.array(x_test[\"Class\"])\n",
      "y_validate = np.array(x_validate[\"Class\"])\n",
      "Finally, you have to drop the column Class  in the x sets since it would \n",
      "defeat the purpose of teaching the model how to learn what makes up a \n",
      "normal and a fraudulent transaction if you gave it the label directly:\n",
      "x_train = x_train.drop(\"Class\", axis=1)\n",
      "x_test = x_test.drop(\"Class\", axis=1)\n",
      "x_validate = x_validate.drop(\"Class\", axis=1)\n",
      "To see all this code in a cell, refer to Figure  2-8.\n",
      "Figure 2-7.  You create train and validate splits from the training \n",
      "data. You have chosen to make the validation set comprise 25% of \n",
      "the respective original training sets. As these original training sets \n",
      "themselves comprise of 80% of the original normal and anomaly data \n",
      "frames, the respective validation splits are 20% (0. 25 * 0.8) of their \n",
      "original normal and anomaly data frames. And so, the final training \n",
      "split also becomes 60% of the original, as 0.75 * 0.8 = 0.6Chapter 2  Building Models50Let’s get the shapes of the sets you just created:\n",
      "print(\"Training sets:\\nx_train: {} y_train: {}\".format(x_train.\n",
      "shape, y_train.shape))\n",
      "print(\"\\nTesting sets:\\nx_test: {} y_test: {}\".format(x_test.\n",
      "shape, y_test.shape))\n",
      "print(\"\\nValidation sets:\\nx_validate: {} y_validate:  \n",
      "{}\".format(x_validate.shape, y_validate.shape))\n",
      "Refer to Figure  2-9 to see the output.\n",
      "Looking at the data analysis, you can see that some of the values get \n",
      "really large. The fine details are beyond the scope of this book, but when \n",
      "some features have a relatively small range but others have an extremely \n",
      "large range (think of the range of V1 and Time  from the previous chapter), \n",
      "the model will have a much harder time learning.\n",
      "Figure 2-8.  Creating the respective x and y splits of the training, \n",
      "testing, and validation sets. The x sets are the combinations of the \n",
      "normal and anomaly sets for each split (train, test, validate), while \n",
      "the y sets are simply the data in the Class columns of those x sets. You \n",
      "then drop the label column from the x sets\n",
      "Figure 2-9.  Printing the output of the different sets. The three sets \n",
      "should comprise 60%, 20%, and 20% of the original union of the \n",
      "normal and anomaly setsChapter 2  Building Models51In more detail, the model will have a hard time optimizing the cost \n",
      "function and may take many more steps to converge, if it is able to do so at all.\n",
      "And so it is better to scale everything down by normalizing the data. \n",
      "You will be using scikit-learn’s StandardScaler , which normalizes all of \n",
      "the data such that the mean is 0 and the standard deviation is 1.\n",
      "Here is the code to standardize your data:\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(pd.concat((normal, anomaly)).drop(\"Class\", axis=1))\n",
      "x_train = scaler.transform(x_train)\n",
      "x_test = scaler.transform(x_test)\n",
      "x_validate = scaler.transform(x_validate)\n",
      "It is important to note that you are fitting the scaler on the entire data \n",
      "frame so that it standardizes all of your data in the same way. This is to \n",
      "ensure the best results since you don’t want to standardize x_train , x_test , \n",
      "and x_validate  in their own ways since it would create discrepancies in \n",
      "the data and would be problematic for the model. Of course, once you’ve \n",
      "deployed the model and start receiving new data, you would still standardize \n",
      "it using the scaler from the training process, but this new data could possibly \n",
      "come from a slightly different distribution than your training data. This \n",
      "would especially be the case if trends start shifting - this new standardized \n",
      "data could possibly lead to a tougher time for the model since it wouldn’t fit \n",
      "very well in the distribution that the model trained on.\n",
      "Refer to Figure  2-10  to see the code in a cell.\n",
      "Figure 2-10.  Fitting a standard scaler object on a concatenation of \n",
      "the normal and anomaly data frames. This is done so that each of \n",
      "the train, test, and validate subsets will be scaled according to the \n",
      "same standards, ensuring that there are no discrepancies between the \n",
      "scaling of the dataChapter 2  Building Models52 Model Training\n",
      "Finally, you can now define your logistic regression model:\n",
      "sk_model = LogisticRegression(random_state= None, max_iter=400, \n",
      "solver='newton-cg').fit(x_train, y_train)\n",
      "Refer to Figure  2-11  to see the code in a cell. There should not be any \n",
      "outputs after execution if it all goes well. Any errors you might see could \n",
      "involve a failure to converge. For that, changing the max_iter  parameter \n",
      "could help, and changing the solver  algorithm could help as well.\n",
      "After the training process, either the evaluation step or validation \n",
      "step can come next. As long as the testing set and the validation set come \n",
      "from different distributions (the validation set is derived from the training \n",
      "set, while the testing set is derived from the original data), the model is \n",
      "technically seeing new data in the evaluation and in the validation processes.\n",
      "The context also matters. If you are using the validation process to \n",
      "select the best model out of a set of trained models, then the validation \n",
      "process can come after the training process. You can still evaluate one or \n",
      "all of your trained models, but it could be unnecessary because in this \n",
      "context you’re trying to find the best model for the code.\n",
      "In the context where you’re trying to tune your hyperparameters for \n",
      "a model you are going to stick with, it doesn’t matter whether you do the \n",
      "evaluation first or the validation first. Doing the evaluation first, as you will \n",
      "be doing shortly, can give you a good idea of how well the model is doing \n",
      "currently before starting the validation step. The model will never learn \n",
      "from the evaluation data, so there’s no harm in evaluating the model on \n",
      "this data.\n",
      "Figure 2-11.  Defining the logistic regression model and training it on \n",
      "the training dataChapter 2  Building Models53In this example, you are looking at tuning the hyperparameter for class \n",
      "weights (how much to weight a normal sample and how much to weight a \n",
      "fraudulent sample).\n",
      "But first, let’s evaluate your model to get a deeper understanding of \n",
      "how everything works.\n",
      " Model Evaluation\n",
      "You can now look at accuracy and AUC scores. First, you find the accuracy \n",
      "using the built-in score function of the model:\n",
      "eval_acc = sk_model.score(x_test, y_test)\n",
      "Next, let’s get the list of predictions from the model to help calculate \n",
      "the AUC score. AUC is usually a better metric since it better explains \n",
      "the performance of the model. The general gist of it is that a model that \n",
      "perfectly classifies every point correctly will have an AUC score of 100%.\n",
      "The problem with accuracy in this context is that if there are 100,000 \n",
      "normal points and perhaps around 100 anomalies, the model can classify \n",
      "all of the normal points correctly and none of the anomalies and still get \n",
      "a really high accuracy above 99%. However, the AUC score would show \n",
      "a value much lower at around 0.5. An AUC of 0.5 means that the model \n",
      "knows nothing and is practically just guessing randomly, but in this case, it \n",
      "means the model only ever predicts “normal” for any point it sees. In other \n",
      "words, it hasn’t actually learned much of anything if it doesn’t know how \n",
      "to predict an anomaly.\n",
      "It’s also worth mentioning that AUC isn’t the sole metric by which one \n",
      "should base the worthiness of a model, since context matters. In this case, \n",
      "normal points far outnumber anomalies, so accuracy is a relatively poor \n",
      "metric to solely judge model performance on. AUC scores in this case \n",
      "would reflect the mode’s performance well, but it’s also possible to get \n",
      "higher AUC scores but lower accuracy scores. That just means you must Chapter 2  Building Models54look at the results carefully to understand exactly what’s happening. To \n",
      "help with this, you will look at a “confusion matrix” shortly.\n",
      "Now, let’s get the predictions and calculate the AUC score:\n",
      "preds = sk_model.predict(x_test)\n",
      "auc_score = roc_auc_score(y_test, preds)\n",
      "Finally, let’s print out the scores:\n",
      "print(f\"Auc Score: {auc_score:.3%}\")\n",
      "print(f\"Eval Accuracy: {eval_acc:.3%}\")\n",
      "Refer to Figure  2-12  to see all three of the cells above and the output \n",
      "that results.\n",
      "In this case, both the AUC score and the accuracy score are high. \n",
      "Between the two, the accuracy score is definitely inflated by the number of \n",
      "normal points that exist, but the AUC score indicates that the model does \n",
      "a pretty good job at distinguishing between the anomalies and the normal \n",
      "points.\n",
      "Scikit-learn actually provides a function that lets you see the ROC \n",
      "curve—the figure from which the AUC score (or “area under curve”) is \n",
      "derived from. Run the following:\n",
      "roc_plot = plot_roc_curve(sk_model, x_test, y_test, \n",
      "name='Scikit-learn ROC Curve')\n",
      "Refer to Figure  2-13  for the output.\n",
      "Figure 2-12.  Printing out the AUC score and the accuracy for the \n",
      "scikit-learn logistic regression modelChapter 2  Building Models55What’s basically happening is that scikit-learn takes in the model and \n",
      "the evaluation set to dynamically generate the curve as it predicts on the \n",
      "test sets. The metrics you see on the axes are derived from how correctly \n",
      "the model predicts each of the values. The “true positive rate” and the \n",
      "“false positive rate” are derived from the values on the confusion matrix \n",
      "that you will see below.\n",
      "From that graph, the AUC score is generated. You can see that it differs \n",
      "from the score that was calculated earlier, but this can be attributed to the \n",
      "two functions calculating the scores slightly differently.\n",
      "Let’s now build the confusion matrix and plot it using seaborn:\n",
      "conf_matrix = confusion_matrix(y_test, preds)\n",
      "ax = sns.heatmap(conf_matrix, annot= True,fmt='g')\n",
      "ax.invert_xaxis()\n",
      "ax.invert_yaxis()\n",
      "plt.ylabel('Actual')\n",
      "plt.xlabel('Predicted')\n",
      "Refer to Figure  2-14  for the output.\n",
      "Figure 2-13.  The ROC curve generated for the logistic regression \n",
      "model you just trained. An ROC curve starting with a true positive \n",
      "value of 1.0 at a false positive value of 0.0 is the best possible curve in \n",
      "theory. From that point, it should keep going right while maintaining \n",
      "its value as it hits 1.0 on the x-axis. This graph is quite close to that \n",
      "ideal, hence why the AUC score is so high at 0.98. The discrepancy in \n",
      "AUC score here compared to when you calculated it earlier has to do \n",
      "with how the value is actually calculatedChapter 2  Building Models56This is what a confusion matrix looks like. The y-axis consists of \n",
      "the true labels, while the x-axis consists of predicted labels. When the \n",
      "true label is “0” and the model predicts “0, ” we call this a true negative . \n",
      "“True” refers to the true label, and “negative” refers to the label the model \n",
      "predicts.\n",
      "What counts as “positive” and what counts as “negative” can differ. In \n",
      "tasks such as disease detection, if a test finds someone to have the disease, \n",
      "they are said to “test positive. ” Otherwise, they “test negative. ” Anomaly \n",
      "detection is similar. When a model thinks that a point is an anomaly, it \n",
      "flags it with the label “1. ” And so, a point is labeled “positive” if the model \n",
      "thinks it is an anomaly, and “negative” if it doesn’t.\n",
      "Figure 2-14.  The confusion matrix plot of the results of training. The \n",
      "accuracy for the normal points is very good, but the accuracy for the \n",
      "anomaly points is ok. There is still further room for improvement \n",
      "looking at these results, as you have not tuned the hyperparameters of \n",
      "the model yet, but it already does ok in detecting anomalies. The goal \n",
      "now is to keep the accuracy for the normal points as high as possible, \n",
      "or at a high enough level that’s acceptable, while raising the accuracy \n",
      "for the anomaly points as high as possible. Based on this confusion \n",
      "matrix plot, you can now see that the lower AUC score is more \n",
      "accurate at reflecting the true performance of the model. You can see \n",
      "that a non-negligible amount of anomalies were falsely classified as \n",
      "normal, hence an AUC score of 0.84 is a much better indicator of the \n",
      "model’s performance than the graph’s apparent score of 0.98Chapter 2  Building Models57You may notice that we have inverted the axes in the code. This is \n",
      "simply to get it in the format so that the top left of the matrix corresponds \n",
      "to “true positives, ” the top right of the matrix corresponds to “false \n",
      "negatives, ” the bottom left of the matrix corresponds to “false positives, ” \n",
      "and the bottom right of the matrix corresponds to “true negatives. ”\n",
      "To quickly recap these concepts:\n",
      "• True positives  are values that the model predicts as \n",
      "positive that actually are positive.\n",
      "• False negatives  are values that the model predicts as \n",
      "negative that actually are positive.\n",
      "• False positives  are values that the model predicts as \n",
      "positive that actually are negative.\n",
      "• True negatives  are values that the model predicts as \n",
      "negative that actually are negative.\n",
      "To look at how well the model identifies anomalies, look at the 1 \n",
      "row on the y-axis. The sum of this row should equal the total number \n",
      "of anomalies in the test set: 99 anomalies. The model predicted about \n",
      "68.7% of the anomalies correctly (68/(68+31)) and predicted 99.98% of the \n",
      "normal points correctly (28425/(28425 + 7)) looking at the bottom row.\n",
      "As you can see, the confusion matrix gives us a really good look at \n",
      "the true performance of the model. You now know that it does very well \n",
      "in the task of predicting normal points but does an ok job at predicting \n",
      "anomalies. That being said, the model can still predict a majority of \n",
      "anomalies correctly. And so you can see that the AUC score of 0.84 was \n",
      "much more accurate at indicating the performance of the model than the \n",
      "graph, which had an AUC of 0.98. With an AUC of 0.98, you can expect that \n",
      "there are very, very few instances of false negatives or false positives.Chapter 2  Building Models58 Model Validation\n",
      "Let’s now look at how to use the process of k-fold cross-validation to \n",
      "compare several hyperparameter values. After the validation process has \n",
      "ended, you will compare the evaluation metrics to get a better idea of what \n",
      "hyperparameter setting works best.\n",
      "The hyperparameter you want to tune is how much you want to weight \n",
      "the anomalies by compared to the normal data points. By default, both of \n",
      "them are weighted equally. Let’s define a list of weights to iterate over:\n",
      "anomaly_weights = [1, 5, 10, 15]\n",
      "Next, you define the number of folds and initialize your data fold \n",
      "generator:\n",
      "num_folds = 5\n",
      "kfold = KFold(n_splits=num_folds, shuffle= True,  \n",
      "random_state=2020)\n",
      "What this KFold()  function does is that it splits the data passed in into \n",
      "num_folds  different partitions. A single fold acts as a validation set at a \n",
      "time, while the rest of the folds are used for training. In this context, the \n",
      "“validation fold” is basically what the model will be evaluating on. It is \n",
      "called “validation” since it helps us get an idea of how the model is doing \n",
      "on data it has never seen before.\n",
      "If you have built deep learning models before, you may know that \n",
      "during the training process, you can split a small portion of the training \n",
      "set aside as a validation set. This lets you know during training if you’re \n",
      "overfitting or not, as decreasing training loss and increasing validation loss \n",
      "would indicate.\n",
      "Refer to Figure  2-15  to see the code above in cells.Chapter 2  Building Models59Now you define the validation script:\n",
      "logs = []\n",
      "for f in range(len(anomaly_weights)):\n",
      "    fold = 1\n",
      "    accuracies = []\n",
      "    auc_scores= []\n",
      "    for train, test in kfold.split(x_validate, y_validate):\n",
      "        weight = anomaly_weights[f]\n",
      "        class_weights= {\n",
      "            0:1,\n",
      "            1: weight\n",
      "        }\n",
      "        sk_model = LogisticRegression(random_state= None,\n",
      "                                      max_iter=400,\n",
      "                                      solver='newton-cg',\n",
      "                                       class_weight=class_\n",
      "weights).fit(x_\n",
      "validate[train],  \n",
      "y_validate[train])\n",
      "Figure 2-15.  Setting the different values for anomaly weights to test \n",
      "with the validation script and constructing the KFold data generator. \n",
      "In this case, you are using five folds, so the data passed in will be split \n",
      "five waysChapter 2  Building Models60        for h in range(40): print('-', end=\"\")\n",
      "        print(f\"\\nfold {fold}\\nAnomaly Weight: {weight}\")\n",
      "         eval_acc = sk_model.score(x_validate[test],  \n",
      "y_validate[test])\n",
      "        preds = sk_model.predict(x_validate[test])\n",
      "        try:\n",
      "            auc_score = roc_auc_score(y_validate[test], preds)\n",
      "        except:\n",
      "            auc_score = -1\n",
      "         print(\"AUC: {}\\neval_acc: {}\".format(auc_score,  eval_acc))\n",
      "        accuracies.append(eval_acc)\n",
      "        auc_scores.append(auc_score)\n",
      "         log = [sk_model, x_validate[test], y_validate[test], preds]\n",
      "        logs.append(log)\n",
      "        fold = fold + 1\n",
      "    print(\"\\nAverages: \")\n",
      "    print(\"Accuracy: \", np.mean(accuracies))\n",
      "    print(\"AUC: \", np.mean(auc_scores))\n",
      "    print(\"Best: \")\n",
      "    print(\"Accuracy: \", np.max(accuracies))\n",
      "    print(\"AUC: \", np.max(auc_scores))\n",
      "That’s a lot to take in at once, so be sure to refer to Figure  2-16  to make \n",
      "sure your code is formatted correctly.Chapter 2  Building Models61Before you run the script, let’s go over what the code does, as that was a \n",
      "lot of code thrown out at once.\n",
      "The first loop goes over each of the anomaly weights. You set the fold \n",
      "number here equal to 1 and define empty lists to hold values for accuracy \n",
      "and AUC scores for each run with the current weight parameter.\n",
      "The second loop goes over the five fold boundaries that the KFold()  \n",
      "object defines. You set the class_weights  dictionary and pass it into the \n",
      "model as a hyperparameter. After the training process, you evaluate as \n",
      "usual. There is a try-except block for the AUC score in the event that the \n",
      "Figure 2-16.  The validation script in a cell. The script is quite long, so \n",
      "be sure it is formatted correctly because a single space misalignment \n",
      "can cause issuesChapter 2  Building Models62fold generated only has values of one class (so really if it only has normal \n",
      "data and no anomalies). If the AUC score is -1 for any fold, then you know \n",
      "there was a problem with one of the folds.\n",
      "You do save the model, the validation data, and the predictions so that \n",
      "you can examine the confusion matrix and plot the ROC curve for any run \n",
      "you like. After the end of the five folds, the script then displays averages \n",
      "and the best scores.\n",
      "The output will be truncated when you run this, so don’t forget to \n",
      "expand it like in the previous chapter to look at all of the runs. Feel free to \n",
      "explore the output or even change the number of folds but beware of the \n",
      "results because increasing the number of folds can mean that the number \n",
      "of anomalies must be spread across even more partitions. In this specific \n",
      "context, a lower number of folds is likely to be better because you have so \n",
      "few anomaly points.\n",
      "When you sift through the output, you can see that the best results \n",
      "occur when the anomaly weight is set to 10. This setting had the highest \n",
      "average AUC score and had the best AUC score as well, resulting in an \n",
      "output like what you see in Figure  2-17 .Chapter 2  Building Models63\n",
      "Figure 2-17.  Looking at the results of the best setup in the validation \n",
      "script output. The best setup turned out to be one where the anomalies \n",
      "were weighted as 10, as it had the best average AUC score and the best \n",
      "AUC score with the other anomaly weight parameters. The true best \n",
      "weight is likely around an anomaly weight of 10, though you must \n",
      "perform another hyperparemter search with a more narrowed range \n",
      "to find the absolute best setting. You can keep narrowing the search as \n",
      "much as you’d like, but past a certain precision, you will find that you \n",
      "are getting diminishing returnsChapter 2  Building Models64Let’s examine the plots for this setup since it was the best performer of \n",
      "all of them on average.\n",
      "First, you load the correct log in the list of logs. Since the anomaly \n",
      "weight was 10, and the second fold performed the best, you want to look at \n",
      "the twelfth index in the entries in logs. (The first five correspond to indices \n",
      "0-4, and the next five are indices 5-9. With index 10, you begin the first fold \n",
      "with weight ten, so the second fold is at index 11.)\n",
      "sk_model, x_val, y_val, preds = logs[11]\n",
      "Let’s look at the ROC curve. Keep in mind that since there is so little \n",
      "data in the validation set, the AUC score may not be so accurate. Here is \n",
      "the code:\n",
      "roc_plot = plot_roc_curve(sk_model, x_val, y_val, name='Scikit-  \n",
      "learn ROC Curve')\n",
      "Refer to Figure  2-18  to see the output of the above two cells.\n",
      "Figure 2-18.  Viewing the ROC curve for a specific validation fold. \n",
      "As you can see, the ROC curve is quite optimal. A perfect ROC curve \n",
      "would start as close as possible to 1.0 on the y-axis while maintaining \n",
      "that level right as it reaches 1.0 on the x-axis. An ROC graph like that \n",
      "would mean the AUC would be as close to 1.0 as possible. In this case, \n",
      "you almost see the perfect AUC curve, and the AUC is stated to be 1.0. \n",
      "The confusion matrix in Figure  2-19  will reveal a lot more about why \n",
      "the AUC score is so lowChapter 2  Building Models65This graph looks different compared to the ROC plot you saw earlier. In \n",
      "fact, it almost seems perfect.\n",
      "Let’s look at the confusion matrix to get a better idea of how the model \n",
      "performed on this fold:\n",
      "conf_matrix = confusion_matrix(y_val, preds)\n",
      "ax = sns.heatmap(conf_matrix, annot= True,fmt='g')\n",
      "ax.invert_xaxis()\n",
      "ax.invert_yaxis()\n",
      "plt.ylabel('Actual')\n",
      "plt.xlabel('Predicted')\n",
      "The resulting confusion matrix can be seen in Figure  2-19 .\n",
      "Figure 2-19.  The confusion matrix for a specific validation fold. It \n",
      "has very good accuracy in labeling normal data points and does very \n",
      "well with anomaly points. Additionally, you can see that there are \n",
      "barely any anomalies in this validation fold if you count the entries \n",
      "in the top row: 21 anomalies to 5,685 normal points. It is no wonder, \n",
      "then, that having a higher weight on the anomaly helped the model \n",
      "factor in these anomalies in its learning process, resulting in better \n",
      "performance in anomaly detectionChapter 2  Building Models66The model did quite well on correctly classifying the anomalies, but \n",
      "the goal of validation in this case is just to help nudge the hyperparameter \n",
      "setting in the right direction. Based on the results of the validation process, \n",
      "you know that the optimal hyperparameter value lies within the values of \n",
      "10 and 15 because those two settings produced the best results.\n",
      "Of course, you can narrow the range further to include values between \n",
      "10 and 15 for the anomaly weights and repeat this process again and again, \n",
      "further reducing the range until a good, optimal value is found. After a \n",
      "certain precision, however, you will find that you are getting diminishing \n",
      "returns, and that the effort you put into hyperparameter tuning only \n",
      "produces near-negligible boosts in performance.\n",
      "With that, you now know how to train, evaluate, and validate a logistic \n",
      "regression model in scikit-learn.\n",
      " PySpark\n",
      "We have provided the versions of the modules we will be using. Installing \n",
      "PySpark can be a little complicated as it’s not a matter of doing pip \n",
      "install PySpark  depending on the version, so beware of that.\n",
      "Here are the versions of our configuration:\n",
      "• Python 3.6.5\n",
      "• PySpark 3.0.0\n",
      "• matplotlib 3.2.1\n",
      "• seaborn 0.10.1\n",
      "• sklearn 0.22.1.post1\n",
      "With that, let’s begin. Again, we will not provide commentary as \n",
      "detailed as in the scikit-learn example, so be sure to review the whole \n",
      "process in scikit-learn to get a good idea of how it will go. Additionally, we \n",
      "won’t be validating the model in PySpark in this example.Chapter 2  Building Models67 Data Processing\n",
      "Here are the import  statements:\n",
      "import pyspark #\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark import SparkConf, SparkContext\n",
      "from pyspark.sql.types import *\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "from pyspark.ml import Pipeline\n",
      "from pyspark.ml.classification import LogisticRegression as \n",
      "LogisticRegressionPySpark\n",
      "import pyspark.sql.functions as F\n",
      "import os\n",
      "import seaborn as sns\n",
      "import sklearn #\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_auc_score\n",
      "import matplotlib #\n",
      "import matplotlib.pyplot as plt\n",
      "os.environ[\"SPARK_LOCAL_IP\"]='127.0.0.1'\n",
      "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
      "spark.sparkContext._conf.getAll()\n",
      "print(\"pyspark: {}\".format(pyspark.__version__))\n",
      "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
      "print(\"seaborn: {}\".format(sns.__version__))\n",
      "print(\"sklearn: {}\".format(sklearn.__version__))\n",
      "The output should look something like in Figure  2-20 .Chapter 2  Building Models68You will notice that there is some additional code relating to PySpark \n",
      "that you have had to define. With PySpark, you must define a Spark context \n",
      "and create a Spark session. What this really means is that you are creating \n",
      "a point of connection to the Spark engine, enabling the engine to run all of \n",
      "the code relating to Spark functionality.\n",
      "Let’s now load the data set. PySpark has its own functionality for \n",
      "creating data frames, so you won’t be using pandas. Execute the following:\n",
      "data_path = 'data/creditcard.csv'\n",
      "df = spark.read.csv(data_path, header = True, inferSchema = True)\n",
      "labelColumn = \"Class\"\n",
      "columns = df.columns\n",
      "numericCols = columns\n",
      "numericCols.remove(labelColumn)\n",
      "print(numericCols)\n",
      "Figure 2-20.  Importing the necessary modules and printing their \n",
      "versions. Once again, modules imported solely for the purpose of \n",
      "displaying versions are marked with a # so you may remove them and \n",
      "the print statements if desiredChapter 2  Building Models69You should see something like Figure  2-21 .\n",
      "Printing the columns is just to ensure that the label column has been \n",
      "removed successfully.\n",
      "You can look at the data frame now just to ensure that it has been \n",
      "loaded properly. You will have to use built-in functionality to convert to a \n",
      "pandas data frame, because Spark data frames are not very clean to look at.\n",
      "Look at the following two cells and their outputs:\n",
      "df.show(2)\n",
      "Refer to Figure  2-22 .\n",
      "Figure 2-21.  Reading the credit card data set in PySpark and \n",
      "removing the Class column from the list of columns. This is done \n",
      "because you don’t want the Class column to be included in the feature \n",
      "vector, as you will see in Figure  2-22Chapter 2  Building Models70Now compare this to the following:\n",
      "df.toPandas().head()\n",
      "Refer to Figure  2-23 .\n",
      "Figure 2-22.  The output of the Spark data frame. Since there are so \n",
      "many columns in the data frame, the output is very messy and very \n",
      "difficult to read. Fortunately, there is built-in functionality to convert \n",
      "PySpark data frames into pandas data frames, making it much easier \n",
      "to view the rows in the Spark data frameChapter 2  Building Models71So whenever you want to check a Spark data frame, make sure to \n",
      "convert it to pandas if it has a lot of columns.\n",
      "The data processing procedure for PySpark is slightly different than \n",
      "in pandas. To train the model, you must pass in a vector called features . \n",
      "Take a look at the following code:\n",
      "stages = []\n",
      "assemblerInputs = numericCols\n",
      "assembler = VectorAssembler(inputCols=assemblerInputs, \n",
      "outputCol=\"features\")\n",
      "stages += [assembler]\n",
      "dfFeatures = df.select(F.col(labelColumn).alias('label'), \n",
      "*numericCols )\n",
      "This defines the inputs to the assembler so that it knows what columns \n",
      "to transform into the features  vector.\n",
      "From here, let’s add to the cell above and create the normal and \n",
      "anomaly data splits as with the scikit-learn example.\n",
      "normal = dfFeatures.filter(\"Class == 0\").\n",
      "sample(withReplacement= False, fraction=0.5, seed=2020)\n",
      "anomaly = dfFeatures.filter(\"Class == 1\")\n",
      "Figure 2-23.  Using PySpark’s built-in functionality to convert the \n",
      "spark data frame into a pandas data frame for easier viewing. As seen \n",
      "in Figure  2-22 , it is extremely hard to read the direct output of a Spark \n",
      "data frameChapter 2  Building Models72normal_train, normal_test = normal.randomSplit([0.8, 0.2],  \n",
      "seed = 2020)\n",
      "anomaly_train, anomaly_test = anomaly.randomSplit([0.8, 0.2], \n",
      "seed = 2020)\n",
      "The cell should look like Figure  2-24 .\n",
      "Just like in the scikit-learn example, you combine the respective \n",
      "normal and anomaly splits to form your training and testing sets. This \n",
      "time, however, you won’t have a validation set, so you are looking at an \n",
      "80- 20 s plit between the training and testing data.\n",
      "train = normal_train.union(anomaly_train)\n",
      "test = normal_test.union(anomaly_test)\n",
      "Refer to Figure  2-25  to see the cell.\n",
      "Figure 2-24.  Constructing the VectorAssembler that will be used later \n",
      "to create a feature vector from the input data. You also create a normal \n",
      "and anomaly data split similar to how it was done in scikit-learn, and \n",
      "split it in a similar fashion into training and testing subsets\n",
      "Figure 2-25.  Creating the training and testing sets in a similar \n",
      "manner to how you did it in scikit-learn, but with PySpark’s \n",
      "functionalityChapter 2  Building Models73Let’s finish the rest of the pipeline and create the feature  vector:\n",
      "pipeline = Pipeline(stages = stages)\n",
      "pipelineModel = pipeline.fit(dfFeatures)\n",
      "train = pipelineModel.transform(train)\n",
      "test = pipelineModel.transform(test)\n",
      "selectedCols = ['label', 'features'] + numericCols\n",
      "train = train.select(selectedCols)\n",
      "test = test.select(selectedCols)\n",
      "print(\"Training Dataset Count: \", train.count())\n",
      "print(\"Test Dataset Count: \", test.count())\n",
      "Refer to Figure  2-26  to see the output.\n",
      " Model Training\n",
      "You can now define and train the model:\n",
      "lr = LogisticRegressionPySpark(featuresCol = 'features', \n",
      "labelCol = 'label', maxIter=10)\n",
      "lrModel = lr.fit(train)\n",
      "trainingSummary = lrModel.summary\n",
      "pyspark_auc_score = trainingSummary.areaUnderROC\n",
      "Figure 2-26.  Using a pipeline to create a feature vector from the data \n",
      "frame. This feature vector is what the logistic regression model will \n",
      "train onChapter 2  Building Models74Refer to Figure  2-27  to see the above code in a cell.\n",
      " Model Evaluation\n",
      "Once the model has finished training, run the evaluation code:\n",
      "predictions = lrModel.transform(test)\n",
      "y_true = predictions.select(['label']).collect()\n",
      "y_pred = predictions.select(['prediction']).collect()\n",
      "evaluations = lrModel.evaluate(test)\n",
      "accuracy = evaluations.accuracy\n",
      "Add the following code as well to display the metrics:\n",
      "print(f\"AUC Score: {roc_auc_score(y_pred, y_true):.3%}\")\n",
      "print(f\"PySpark AUC Score: {pyspark_auc_score:.3%}\")\n",
      "print(f\"Accuracy Score: {accuracy:.3%}\")\n",
      "Refer to Figure  2-28  to see the output.\n",
      "Figure 2-27.  Defining the PySpark logistic regression model, training \n",
      "it, and finding the AUC score using the built-in function of the model\n",
      "Figure 2-28.  The output metrics. The AUC score is calculated using \n",
      "scikit-learn’s scoring algorithm, while the PySpark AUC score metric \n",
      "comes from the training summary of the PySpark model. Finally, the \n",
      "accuracy score is also outputtedChapter 2  Building Models75You can see that the AUC score and the accuracy are quite high, so let’s \n",
      "examine the graphs.\n",
      "First, let’s look at the ROC curve:\n",
      "pyspark_roc = trainingSummary.roc.toPandas()\n",
      "plt.xlabel('False Positive Rate')\n",
      "plt.ylabel('True Positive Rate')\n",
      "plt.title('PySpark ROC Curve')\n",
      "plt.plot(pyspark_roc['FPR'],pyspark_roc['TPR'])\n",
      "To see the graph, refer to Figure  2-29 .\n",
      "Figure 2-29.  The ROC curve for the PySpark logistic regression model \n",
      "you just trained. A perfect ROC curve would have the true positive \n",
      "rate starting at 1.0, where it continues right to a false positive rate \n",
      "value of 1.0. This curve is quite close to that, hence why its area (AUC) \n",
      "is said to be around 0.97997 by PySpark, keeping in mind a perfect \n",
      "AUC score is 1.00Chapter 2  Building Models76The curve looks quite optimal. Let’s now look at the confusion matrix \n",
      "to get a detailed idea of how the model performs:\n",
      "conf_matrix = confusion_matrix(y_true, y_pred)\n",
      "ax = sns.heatmap(conf_matrix, annot= True,fmt='g')\n",
      "ax.invert_xaxis()\n",
      "ax.invert_yaxis()\n",
      "plt.ylabel('Actual')\n",
      "plt.xlabel('Predicted')\n",
      "Refer to Figure  2-30  to view the confusion matrix plot.\n",
      "Figure 2-30.  The plotted confusion matrix of the PySpark logistic \n",
      "regression model you just trained. The accuracy of correctly labeled \n",
      "points for the normal data is very high and is decent for the \n",
      "anomalous dataChapter 2  Building Models77From this, you have a much more detailed account of how the model \n",
      "performed. Looking at just the anomalies, you see that the model has \n",
      "a 81.4% accuracy (70/(70+16)) in predicting anomalies. This is better \n",
      "than the model you trained in scikit-learn, though you haven’t tuned the \n",
      "hyperparameter to attain maximum performance.\n",
      "PySpark does have an option to weight your data, but this is done on \n",
      "a sample-by-sample basis. What this means is that instead of passing in a \n",
      "weight dictionary for each class, you have to create a column in the data \n",
      "frame with each anomaly being weighted a certain amount and each \n",
      "normal point being weighted as 1, for example. By default, everything \n",
      "is weighted as 1, so that means the PySpark model may have a greater \n",
      "potential in performance than the scikit-learn model.\n",
      "Moving on to the normal points, you see a really good accuracy of \n",
      "99.96% (28363/(28363+10)), so it is able to identify normal points very well.\n",
      " Summary\n",
      "With the insights you gained from data analysis, you processed the data \n",
      "into training, testing, and validation sets in scikit-learn and PySpark \n",
      "(you only did a train-test split in PySpark, but you could have split the \n",
      "training data into training and validation sets just like in scikit-learn). \n",
      "From there, you constructed logistic regression models in each framework \n",
      "and trained and evaluated on them. You looked at accuracy and AUC \n",
      "scores as metrics and looked at the ROC curve and confusion matrix to \n",
      "get a better idea of how the model performed. For the scikit-learn model, \n",
      "you performed k-fold cross-validation to help tune the hyperparameter. \n",
      "In the next chapter, you will keep your experiences with data analysis \n",
      "and model creation in mind as you learn about MLOps and how you can \n",
      "operationalize your models.Chapter 2  Building Models79© Sridhar Alla, Suman Kalyan Adari 2021 \n",
      "S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \n",
      "https://doi.org/10.1007/978-1-4842-6549-9_3CHAPTER 3\n",
      "What Is MLOps?\n",
      "In this chapter, we will cover the concepts behind the term “MLOps” and \n",
      "go over what it is, why it’s useful, and how it’s implemented.\n",
      " Introduction\n",
      "Creating machine learning solutions to various problems can be quite \n",
      "the arduous task. Let’s imagine ourselves in the shoes of a team that is \n",
      "attempting to solve a problem with machine learning. You may be familiar \n",
      "with this process if you read Chapter 1, but we will recap the entire process \n",
      "once again to establish the context. You may skip past this section if you \n",
      "are already familiar with this. The entire process may look somewhat like \n",
      "the following:\n",
      "• Colle ct and process raw data:  Raw data is rarely in \n",
      "a format that is easy to train a model on. Usually, it \n",
      "requires processing to remove aberrant data points \n",
      "such as null values and faulty data values. Other \n",
      "times, you might have to process the raw data to \n",
      "extract only the information you need among all of \n",
      "the noise.80• Analyze the data:  This step involves looking at the \n",
      "data points and understanding their characteristics. \n",
      "How is it structured? What does the distribution of the \n",
      "data points look like? Are there any identifiable trends \n",
      "or biases in the data? This step is crucial because it \n",
      "dictates how you are going to approach the problem. \n",
      "If you already have a trained model you are looking \n",
      "to update, it also tells you if there are any new trends \n",
      "in the data that your model should be updated to \n",
      "consider. If you identified any “useless features” that \n",
      "don’t really influence the output, you might drop them \n",
      "and train a new model to improve training speed while \n",
      "possibly boosting performance.\n",
      "• Process the data for training:  In this step, you could \n",
      "be scaling the data to a more appropriate range and \n",
      "perhaps removing any outliers and/or anomalies that \n",
      "could interfere with model performance. Furthermore, \n",
      "you could also be applying feature engineering to \n",
      "create new features from existing data points and \n",
      "perhaps give your model more or a better context \n",
      "during training. This is also where you create training \n",
      "and testing data sets, though optimal practice is to \n",
      "make training, testing, and validation data sets.\n",
      "• Construct, train, and test the model:  In this step, you \n",
      "are creating the model, setting hyperparameters, and \n",
      "training the model. In the case of deep learning, you \n",
      "can also select a subsection of the training set to be a \n",
      "data validation set. The purpose of this set is to have \n",
      "the model be evaluated on it at the end of every epoch \n",
      "or full forward pass of the data through the model. By \n",
      "comparing the model performance on data it’s seen Chapter 3  What Is MLOps?81many times over during training versus data it hasn’t \n",
      "seen at all (or rather, data that has no effect on weight \n",
      "adjustment), you can see if the model is truly learning \n",
      "to generalize or if it’s just overfitting.\n",
      "• Overfitting  is when a model performs significantly \n",
      "better on a training set compared to data that it has \n",
      "never seen before. As just discussed, one way to \n",
      "give an early indication of overfitting is to set aside a \n",
      "portion of the training set as validation data during the \n",
      "training phase. This can give you an early indication \n",
      "of overfitting without having to find out after the \n",
      "training process has finished, which can take anywhere \n",
      "from minutes to days depending on the depth of the \n",
      "model and the equipment used. And so, it follows that \n",
      "overfitting can also be observed when the model is \n",
      "evaluated on the testing data or validation data, and \n",
      "discrepancies in model performance can be observed \n",
      "between these sets and the training set.\n",
      "• This phenomenon of overfitting could partially result \n",
      "from the model not receiving enough data points \n",
      "during training to reflect the variety it is expected to \n",
      "see, so fixing the training set by introducing more \n",
      "variety or even increasing the number of data points \n",
      "can help. Additionally, including methods such as \n",
      "regularization or dropout into the model’s architecture \n",
      "can also help combat overfitting in the case of deep \n",
      "learning models.Chapter 3  What Is MLOps?82• An im portant thing to discuss is the purpose of the \n",
      "testing  and validation  sets. Testing sets are reserved \n",
      "for evaluating a model’s performance on data it’s never \n",
      "seen before.\n",
      "• Validation sets are reserved for helping select models, \n",
      "select model architecture, tune hyperparameters, or \n",
      "simply to give an indication of model performance on \n",
      "data it’s never seen during the training process.\n",
      "• An exam ple of validation is k-fold cross-validation, \n",
      "where it generates k random partitions of test-train \n",
      "data from validation data and can be used to train/\n",
      "evaluate the model on all of them to give an idea \n",
      "of the best performance it can attain with various \n",
      "hyperparameter settings. Of course, we can also use \n",
      "k-fold cross validation to perform the other functions \n",
      "that validation helps with. You looked at an example \n",
      "using this method of validation in Chapter 2, when you \n",
      "used it to help tune the weighting of anomalies.\n",
      "• Coupling this technique with a script that has a set of \n",
      "hyperparameters can result in an optimal model with \n",
      "proper hyperparameters. From there, the model can \n",
      "be retrained and evaluated again on the test set to get a \n",
      "final performance benchmark.\n",
      "• The specific order this is done in can differ, though. \n",
      "For example, trained models can also be evaluated \n",
      "first and then validated, compared to the other way \n",
      "around. This is because the training process is likely to \n",
      "be repeated with altered hyperparameters anyway after \n",
      "the evaluation stage reflects some form of performance Chapter 3  What Is MLOps?83discrepancy or if validation data during the training \n",
      "process reveals that possible overfitting is occurring. \n",
      "Either way, it really depends, but good practice is to at \n",
      "least incorporate both testing and validation data to \n",
      "best tune the model.\n",
      "• Validate and tune the model:  As previously \n",
      "discussed, the validation set can be another “testing” \n",
      "set that the model has never seen before, and can \n",
      "be used in any of the several ways described earlier \n",
      "and in Chapter 1. Once your model has reached an \n",
      "acceptable level of performance on the validation set \n",
      "and is retrained and evaluated again, you can look at \n",
      "deploying the model.\n",
      "• Deploy and monitor the model:  In this step, the \n",
      "model has finally left the hands of the machine \n",
      "learning/data science team. It is now the job of \n",
      "engineering and operational teams to integrate this \n",
      "model into the application and put it into service. \n",
      "Operational teams are in charge of constantly \n",
      "monitoring the performance of the model, with dips \n",
      "in performance possibly indicating that this entire \n",
      "process may need to be repeated to update the model \n",
      "to understand new trends. Operational teams are also \n",
      "responsible for reporting any bugs and unexpected \n",
      "model predictions to the data science team, feedback \n",
      "that also contributes to the start of this whole cycle as \n",
      "the model needs to be fixed.Chapter 3  What Is MLOps?84Hopefully, it’s clear just how work-intensive the entire process can \n",
      "get, especially since it will most likely need to be repeated multiple times. \n",
      "While it is possibly easier the second time around since you’re only \n",
      "updating the model on new data patterns and trends, it is still a problem \n",
      "that can take up hours of manual labor that can be better spent elsewhere. \n",
      "After all, maintenance of applications in the software development \n",
      "process is usually where most of the money and resources go, not the \n",
      "initial construction and release of the application. The same can apply \n",
      "to machine learning models, worsening the overall maintenance costs \n",
      "because the costs for deployed machine learning models are added on top \n",
      "of the costs for the software application utilizing the services of the models.\n",
      "Imagine if you could simply automate this entire process away, \n",
      "allowing you to take full advantage of high-performance machine \n",
      "learning models without all of that hassle. This is where MLOps comes in, \n",
      "something that can be thought of as the intersection between machine \n",
      "learning and DevOps practices. DevOps , or developmental operations, \n",
      "refers to a set of practices that combines the work processes of software \n",
      "developers with those of operational teams to create a common set \n",
      "of practices that functions as a hybrid of the two roles. As a result, the \n",
      "developmental cycle of software is expedited, and continuous delivery \n",
      "of software products is ensured. Total costs also go down because \n",
      "maintenance costs are reduced as a result of the increase in efficiency of \n",
      "the workflow in maintaining the software applications. Refer to Figure  3-1 \n",
      "to see a graph representing the DevOps workflow.Chapter 3  What Is MLOps?85\n",
      "Figure 3-1.  A graph depicting the workflow in a DevOps \n",
      "environment. Software development teams typically adopt the Agile \n",
      "methodology of software development, which is summarized above \n",
      "through the planning, building, and testing stages. Operational teams \n",
      "are in charge of deploying, maintaining, and collecting feedback in \n",
      "the form of bugs and user feedback and relaying this information to \n",
      "the development teams. From there, the development team enters the \n",
      "maintenance phase of the application, where they plan, build, test, \n",
      "and push the next patch/update for the application. Furthermore, \n",
      "automating the process of testing and deploying allows for continuous \n",
      "integration and delivery of software products, something we will \n",
      "expand upon later in this chapterChapter 3  What Is MLOps?86Similarly, MLOps adopts DevOps principles and applies them to \n",
      "machine learning models in place of software, uniting the developmental \n",
      "cycles followed by data scientists and machine learning engineers \n",
      "with that of operational teams to help ensure continuous delivery of \n",
      "high-performance machine learning models. The process of model \n",
      "development in what’s called the experimental stage , something we will \n",
      "look at in detail later in the chapter, can lead to impressive performances \n",
      "and can seem like very promising solutions. However, the reality is more \n",
      "that most models simply never make it past this experimental stage, \n",
      "since deploying them is a massive undertaking on its own. Unfortunately, \n",
      "maintaining models once deployed also drains resources, as every new \n",
      "update requires reintegration into the application. This means that even \n",
      "if the model is deployed, all teams have their work cut out for them. For \n",
      "these reasons, most models simply never make it past the prototype phase.\n",
      "Until the emergence of MLOps principles, deploying solutions created \n",
      "using the latest in machine learning technology served as a significant \n",
      "challenge to businesses due to the amount of resources that would be \n",
      "required. This is why MLOps is so crucial . It makes it significantly easier \n",
      "to deploy and maintain your machine learning solutions by automating \n",
      "most of the hard parts for you, massively expediting the development and \n",
      "maintenance processes. With a fully automated setup, teams can keep up \n",
      "with the latest in machine learning technology and deploy new models \n",
      "quickly. Services can maintain their high level of performance and perhaps \n",
      "even improve on this front as teams can deploy newer, more promising \n",
      "model architectures.\n",
      "Now that you have a better idea of what MLOps is about and why it is \n",
      "so important, let’s jump into the details and look at how an ideal MLOps \n",
      "implementation is set up.Chapter 3  What Is MLOps?87 MLOps Setups\n",
      "Before we look at any specific MLOps setups, let’s first establish three \n",
      "different setups representing the various stages of automation: manual \n",
      "implementation , continuous model delivery , and continuous \n",
      "integration/continuous delivery of pipelines .\n",
      "Manual implementation  refers to a setup where there are no MLOps \n",
      "principles applied and everything is manually implemented. The steps \n",
      "discussed above in the creation of a machine learning model are all \n",
      "manually performed. Software engineering teams must manually integrate \n",
      "the models into the application, and operational teams must help ensure \n",
      "all functionality is preserved along with collecting data and performance \n",
      "metrics of the model.\n",
      "Continuous model delivery  is a good middle ground between a \n",
      "manual setup and a fully automated one. Here, we see the emergence \n",
      "of pipelines  to allow for automation of the machine learning side of the \n",
      "process. Note that we will mention this term quite often in the sections \n",
      "below. If you’d like to get a better idea about what a pipeline is, refer to the \n",
      "section titled “Pipelines and Automation” further down in this chapter. \n",
      "For now, a pipeline  is an infrastructure that contains a sequence of \n",
      "components manipulating information as it passes through the pipeline. \n",
      "The function of the pipeline can slightly differ within the setups, so be \n",
      "sure to refer to the graphs and explanations to get a better idea of how the \n",
      "pipeline in the example functions.\n",
      "The main feature of this type of setup is that the deployed model \n",
      "has pipelines established to continuously train it on new data, even \n",
      "after deployment. Automation of the experimental stage, or the model \n",
      "development stage, also emerges along with modularization of code \n",
      "to allow for further automation in the subsequent steps. In this setup, \n",
      "continuous delivery  refers to expedited development and deployment of \n",
      "new machine learning models. With the barriers to rapid deployment lifted Chapter 3  What Is MLOps?88(the tediousness of manual work in the experimental stage) by automation, \n",
      "models can now be created or updated at a much faster pace.\n",
      "Continuous integration/continuous delivery of pipelines  refers to \n",
      "a setup where pipelines in the experimental stage are thoroughly tested \n",
      "in an automated process to make sure all components work as intended. \n",
      "From there, pipelines are packaged and deployed, where deployment \n",
      "teams deploy the pipeline to a test environment, handle additional testing \n",
      "to ensure both compatibility and functionality, and then deploy it to the \n",
      "production environment. In this setup, pipelines can now be created \n",
      "and deployed at a quick pace, allowing for teams to continuously create \n",
      "new pipelines built around the latest in machine learning architectures \n",
      "without any of the resource barriers associated with manual testing and \n",
      "integration.\n",
      " Manual Implementation\n",
      "Now that we’ve established three variations of setups, let’s look at the first \n",
      "of the three deployment setups of machine learning models, which has no \n",
      "MLOps principles integrated.\n",
      "In this case, there is a team of data scientists and machine learning \n",
      "engineers, who will now be referred to as the “model development team, ” \n",
      "manually performing data analysis and building, training, testing, and \n",
      "validating their models. Once their model has been finalized, they must \n",
      "create a model class and push this to a code repository. Software engineers \n",
      "extract this model class and integrate it into an existing application or \n",
      "system, and operational teams are in charge of monitoring the application, \n",
      "maintaining functionality, and providing feedback to both the software \n",
      "and model development teams.\n",
      "Everything here is manual, meaning any new trends in the data lead \n",
      "to the model development team having to update the model and repeat \n",
      "the entire process again. This is quite likely to happen considering the \n",
      "high volume of users interacting with your model every day. Combined Chapter 3  What Is MLOps?89with performance metrics and user data collection, the information will \n",
      "reveal a lot of aspects about your model as well as the user base the model \n",
      "is servicing. Chances are high that you will have to update it to maintain \n",
      "its performance on the new data. This is something to keep in mind as you \n",
      "follow through with the process on the graph.\n",
      "Refer to Figure  3-2 for a graphical representation of the setup.\n",
      "Let’s go through this step by step. We can split the flow into roughly \n",
      "two parts: the experimental stage , which involves the machine learning \n",
      "side of the entire workflow, and the deployment stage,  which handles \n",
      "integration of the model into the application and maintaining operations.\n",
      "Figure 3-2.  Graph depicting a possible deployment setup of a \n",
      "machine learning model without MLOps principles. The arrows with \n",
      "a dotted border mean that progression to the next step depends upon \n",
      "a condition in the current step. For example, in the model validation \n",
      "step, machine learning engineers must ensure that the model meets a \n",
      "minimum benchmark in performance before pushing a model class to \n",
      "the repositoryChapter 3  What Is MLOps?90Experimental Stage:\n",
      " 1. Data store:  The data store refers to wherever data \n",
      "relevant to data analysis and model development \n",
      "is stored. An example of a data store could be using \n",
      "Hadoop to store large volumes of data, which can \n",
      "be used by multiple model development teams. In \n",
      "this example, data scientists can pull raw data from \n",
      "this data store to start performing experiments and \n",
      "conducting data analysis.\n",
      " 2. Process raw data:  As previously mentioned, raw \n",
      "data must be processed in order to collect the \n",
      "relevant information. From there, it must also \n",
      "be purged of faults and corrupted data. When a \n",
      "company collects massive volumes of data every \n",
      "day, some of it is bound to be corrupted or faulty \n",
      "in some way eventually, and it’s important to get \n",
      "rid of these points because they can harm the \n",
      "data analysis and model development processes. \n",
      "For example, one null value entry can completely \n",
      "destroy the training process of a neural network \n",
      "used for a regression (value prediction) task.\n",
      " 3. Data analysis:  This step involves analyzing all \n",
      "aspects of the data. The general gist of it was \n",
      "discussed earlier, but in the context of updating \n",
      "the model, data scientists want to see if there are \n",
      "any new trends or variety in data that they think \n",
      "the model should be updated on. Since the initial \n",
      "training process can be thought of as a small \n",
      "representation of the real-world setting, there is a \n",
      "fair chance that the model will need to be updated Chapter 3  What Is MLOps?91soon after the initial deployment. This does depend \n",
      "on how many characteristics of the true user base \n",
      "the original training set captured however, but user \n",
      "bases change over time, and so must the models. By \n",
      "“user base, ” we refer to the actual customers using \n",
      "the prediction services of the model.\n",
      " 4. Model building stage:  This stage is more or less the \n",
      "same as what we discussed earlier. The second time \n",
      "around, when updating the model, it could turn out \n",
      "that slight adjustments to the model layers may be \n",
      "needed. In some of the worst cases, the current model \n",
      "architecture being used cannot achieve a high enough \n",
      "performance even with new data or architectural \n",
      "tweaks. An entirely new model may have to be built, \n",
      "trained, and validated. If there are no such issues, \n",
      "then the model would just be further trained, tested, \n",
      "validated, and pushed to the code repository upon \n",
      "meeting some performance criteria.\n",
      "• An important thing to note about this experimental \n",
      "stage is that it is quite popular for experiments \n",
      "to be conducted using Jupyter notebook. When \n",
      "model development teams reach a target level \n",
      "of performance, they must work on building \n",
      "a workable model that can be called by other \n",
      "code. For example, this can be done by creating \n",
      "a model class with various functions that provide \n",
      "functionality such as load_weights , predict , and \n",
      "perhaps even evaluate  to allow for easier gathering \n",
      "of performance metrics. Since the true label can’t be \n",
      "known in real-time settings, evaluation metrics can \n",
      "simply be something like a root-mean-squared error.Chapter 3  What Is MLOps?92Deployment Stage:\n",
      " 5. Model deployment:  In this case, this step is where \n",
      "software engineers must manually integrate \n",
      "the model into the system/application they are \n",
      "developing. Whenever the model development \n",
      "team finishes with their experiments, builds \n",
      "a workable model, and pushes it to the code \n",
      "repository, the engineering team must manually \n",
      "integrate it again. Although the process may not be \n",
      "that bad the second time around, there is still the \n",
      "issue of fixing any potential bugs that may arise from \n",
      "the new model. Additionally, engineering teams \n",
      "must also handle testing of not only the model once \n",
      "it is integrated into the application, but also of the \n",
      "rest of the application.\n",
      " 6. Model services:  This step is where the model is \n",
      "finally deployed and is interacting with the user \n",
      "base in real time. This is also where the operational \n",
      "team steps in to help maintain the functionality of \n",
      "the software. For example, if there are any issues \n",
      "with some aspect of the model functionality, the \n",
      "operational team must record the bug and forward it \n",
      "to the model development team.\n",
      " 7. Data collection:  The operational team can also \n",
      "collect raw data and performance metrics. This \n",
      "data is crucial for the company to operate since \n",
      "that is how it makes its decisions. For example, the \n",
      "company might want to know what service is most \n",
      "popular with the user base, or how well the machine \n",
      "learning models are performing so far. This job can Chapter 3  What Is MLOps?93be performed by the application as well, storing all \n",
      "the relevant data in some specific data store related \n",
      "to the application.\n",
      " 8. Data forwarded to data store:  This step is where \n",
      "the operational team sends the data to the data \n",
      "store. Because there could be massive volumes of \n",
      "data collected, it’s fair to assume some degree of \n",
      "automation on behalf of the operational team on \n",
      "this end. Additionally, the application itself could \n",
      "also be in charge of forwarding data it collects to the \n",
      "relevant data store.\n",
      " Reflection on the Setup\n",
      "Right away, you can notice some problems that may arise from such an \n",
      "implementation. The first thing to realize is that the entire experimental \n",
      "stage is manual, meaning data scientists and machine learning engineers \n",
      "must repeat those steps every time. When models are constantly exposed \n",
      "to new data that is more than likely not captured in the original training \n",
      "set, models must frequently be retrained so that they are always up to date \n",
      "with current trends in user data. Unfortunately, when the entire process of \n",
      "analyzing new trends, training, testing, and validating data is manual, this \n",
      "may require significant resources over time, which may become unfeasible \n",
      "for a company without the resources to spare. Additionally, trends in data \n",
      "can change over time. For example, perhaps the age group with the largest \n",
      "number of users logging into the site is comprised of people in their early \n",
      "twenties. A year later, perhaps the dominant age group is now teenagers. \n",
      "What was normal back then isn’t normal now, and this could lead to losses \n",
      "in ad revenues, for example, if that’s the service (targeted advertising) the \n",
      "model in this case provides.Chapter 3  What Is MLOps?94Another issue is that tools such as Jupyter notebook are very popular \n",
      "for prototyping and experimenting machine learning and deep learning \n",
      "models. Even if the experiments aren’t carried out on notebooks, it’s likely \n",
      "that work must be done in order to push the model to the source repo. \n",
      "For example, constructing a model class with some important functions \n",
      "such as load_weights , predict , and evaluate  would be ideal for a model \n",
      "class. Some external code may call upon load_weights()  to set the model \n",
      "weights from different training instances (so if the model has been further \n",
      "trained and updated, simply call this function to get the new model). The \n",
      "function predict()  would then be called to make predictions based on \n",
      "some input data and provide the services the application requires, and \n",
      "the function evaluate()  would be useful in keeping performance metrics. \n",
      "Live data will almost never have truth labels on it (unless the user provides \n",
      "instant feedback, like Google’s captchas where you select the correct \n",
      "images), so a score metric like a root-mean-squared error can be useful \n",
      "when keeping track of performance.\n",
      "Once the model class is completed and pushed, software engineering \n",
      "teams must integrate the model class into the overall application/system. \n",
      "This could prove difficult the first time around, but once the integration \n",
      "has been completed, updates to the model can be as simple as loading new \n",
      "weights. Unfortunately, model architectures are likely to change, so the \n",
      "software teams must reintegrate new model classes into the application.\n",
      "Furthermore, deep learning is a complicated and rapidly evolving \n",
      "field. Models that were cutting-edge several years ago can be far surpassed \n",
      "by the current state-of-the-art models, so it’s important to keep updating \n",
      "your model architectures and to make full use of the new developments in \n",
      "the field. This means teams must continuously repeat the model-building \n",
      "process in order to keep up with developments in the field.\n",
      "Hopefully it is more clear that this implementation is quite flawed in \n",
      "how much work is required to not only create and deploy the model in the \n",
      "first place, but also to continuously maintain it and keep it up to par.Chapter 3  What Is MLOps?95Alright, so how would we go about improving it? Where does this \n",
      "MLOps come into play? To answer these questions, let’s look at the second \n",
      "setup of the three defined earlier.\n",
      " Continuous Model Delivery\n",
      "This setup contains pipelines  for automatic training of the deployed \n",
      "model as well as for speeding up the experimental process. Refer to \n",
      "Figure  3-3 for a graphical representation of this setup.\n",
      "This is a lot to take in at once, so let’s break it down and follow it \n",
      "according to the numbers on the graph.\n",
      " 1. Feature store:  This is a data storage bin that \n",
      "takes the place of the data store in the previous \n",
      "example. The reason for this is that all data can now \n",
      "be standardized to a common definition that all \n",
      "Figure 3-3.  Graph depicting a possible deployment setup of a \n",
      "machine learning model with automation via pipelines Chapter 3  What Is MLOps?96processes can use in this instance. For example, the \n",
      "processes in the experimental stage will be using the \n",
      "same input data as the deployed training pipeline \n",
      "because all of the data is held to the same definition. \n",
      "What is meant by common definition  is that raw \n",
      "data is cleansed and processed in a procedural \n",
      "way that applies to all relevant raw data. These \n",
      "processed features are then held in the feature store \n",
      "for pipelines to draw from, and it is ensured that \n",
      "every pipeline uses features processed according to \n",
      "this standard. This way, any perceived differences \n",
      "in trends between two different pipelines won’t be \n",
      "attributed to deviances in processing procedures.\n",
      "Presume for an instance that you are trying to provide \n",
      "an object detection service that detects and identifies \n",
      "various animals in a national park. All video feed \n",
      "from the trail cameras (a video can be thought of as \n",
      "a sequence of frames) can be stored as raw data, but \n",
      "it can be possible that different trail cameras have \n",
      "different resolutions. Instead of repeating the same \n",
      "data processing procedure, you can simply apply the \n",
      "same procedure (normalizing, scaling, and batching \n",
      "the frames, for example) to the raw videos and store \n",
      "the features that you know all pipelines will use.\n",
      " 2. Data analysis:  In this step, data analysis is still \n",
      "performed to give data scientists and machine \n",
      "learning engineers an idea of what the data looks \n",
      "like, how it’s distributed, and so on, just like in the \n",
      "manual setup. Similarly, this step can determine \n",
      "whether or not to proceed with construction of a \n",
      "new model or just update the current model.Chapter 3  What Is MLOps?97 3. Automated model building and analysis:  In this \n",
      "step, data scientists and machine learning engineers \n",
      "can select a model, set any specific hyperparameters, \n",
      "and let the pipeline automate the entire process. \n",
      "The pipeline will automatically process the data \n",
      "according to the specifications of this model (take \n",
      "the case where the features are 331x331x3 images \n",
      "but this particular model only accepts images that \n",
      "are 224x224x3), build the model, train it, evaluate \n",
      "it, and validate it. During validation, the pipeline \n",
      "may automatically tune the hyperparameters \n",
      "as well optimize performance. It is possible that \n",
      "manual intervention may be required in some \n",
      "cases (debugging, for example, when the model is \n",
      "particularly large and complex, or if the model has a \n",
      "novel architecture), but automation should otherwise \n",
      "take care of producing an optimal model. Once this \n",
      "occurs, modularized code is automatically created so \n",
      "that this pipeline can be easily deployed.\n",
      "Everything in this stage is set up so that the \n",
      "experimental stage goes very smoothly, requiring \n",
      "only that the model is built. Depending on the level of \n",
      "automation implemented, perhaps all that is required \n",
      "is that the model architecture is selected with some \n",
      "hyperparameters specified, and the automation takes \n",
      "care of the rest. Either way, the development process \n",
      "in the experimental stage is sped up massively. With \n",
      "this stage going faster, more experiments can be \n",
      "performed too, leading to possible boosts in overall \n",
      "efficiency as productivity is increased and optimal \n",
      "solutions can be found quicker.Chapter 3  What Is MLOps?98 4. Modularized code:  The experimental stage is set \n",
      "up so that the pipeline and its components are \n",
      "modularized. In this specific context, the data \n",
      "scientist/machine learning engineer defines and \n",
      "builds some model, and the data is standardized to \n",
      "some definition. Basically, the pipeline should be \n",
      "able to accept any constructed model and perform \n",
      "the corresponding steps given some data without \n",
      "hardcoding anything. (Meaning there isn’t any code \n",
      "that will only work for a specific model and specific \n",
      "data. The code works with generalized cases of \n",
      "models and data.)\n",
      "This is modularization , when the whole system \n",
      "is divided into individual components that each \n",
      "have their own function, and these components \n",
      "can be switched out depending on variable inputs. \n",
      "Thanks to the modularized code, when the pipeline \n",
      "is deployed, it will be able to accept any new feature \n",
      "data as needed in order to update the deployed \n",
      "model. Furthermore, this structure also lets it \n",
      "swap out models as needed, so there’s no need to \n",
      "construct the entire pipeline for every new model \n",
      "architecture.\n",
      "Think of it this way: the pipeline is a puzzle piece, \n",
      "and the models along with their feature data are \n",
      "various puzzle pieces that can all fit within the \n",
      "pipeline. They all have their own “image” on the \n",
      "piece and the other sides can have variable shapes, \n",
      "but what is important is that they fit with the \n",
      "pipeline and can easily be swapped out for others.Chapter 3  What Is MLOps?99 5. Deploy pipeline:  In this step, the pipeline is \n",
      "manually deployed and is retrieved from the \n",
      "source code. Thanks to its modularization, the \n",
      "pipeline setup is able to operate independently \n",
      "and automatically train the deployed model on \n",
      "any new data if needed, and the application is \n",
      "built around the code structure of the pipeline \n",
      "so all components will work with each other \n",
      "correspondingly. The engineering team has to build \n",
      "parts of the application around the pipeline and its \n",
      "modularized components the first time around, but \n",
      "after that, the pipelines should work seamlessly with \n",
      "the applications so as long as the structure remains \n",
      "the same. Models are simply swapped, unlike before \n",
      "when the model had to be manually integrated into \n",
      "the application. This time, the pipeline must be \n",
      "integrated into the application, and the models are \n",
      "simply swapped out.\n",
      "However, it is important to mention that pipeline \n",
      "structures can change depending on the model. The \n",
      "main takeaway here is that pipelines should be able \n",
      "to handle many more models before having to be \n",
      "restructured compared to the setup before where \n",
      "“swapping” models meant you only loaded updated \n",
      "weights. Now, if several architectures all have \n",
      "common training, testing, and validation code, they \n",
      "can all be used under the same pipeline.\n",
      " 6. Automated training pipeline:  This pipeline \n",
      "contains the model that provides its services and \n",
      "is set up to automatically fetch new features upon \n",
      "activation of the trigger. The conditions for trigger Chapter 3  What Is MLOps?100activation will be discussed in item 10. When the \n",
      "pipeline finishes updating a trained model, the \n",
      "model is saved to a model registry, a type of storage \n",
      "unit that holds trained models for ease of access.\n",
      " 7. Model registry:  This is a storage unit that \n",
      "specifically holds model classes and/or weights. The \n",
      "purpose of this unit is to hold trained models for \n",
      "easy retrieval by an application, for example, and it \n",
      "is a good component to add to an automation setup. \n",
      "Without the model registry, the model classes and \n",
      "weights would just be saved to whatever source code \n",
      "repository is established, but this way, we make the \n",
      "process simpler by providing a centralized area of \n",
      "storage for these models. It also serves to bridge the \n",
      "gap between model development teams, software \n",
      "development teams, and operational teams since it \n",
      "is accessible by everyone, which is ultimately what \n",
      "we want in an ideal automation setup.\n",
      "This registry along with the automated training \n",
      "pipeline assures continuous delivery of model \n",
      "services  since models can frequently be updated, \n",
      "pushed to this registry, and deployed without having \n",
      "to go through the entire experimental stage.\n",
      " 8. Model services:  Here the application pulls the \n",
      "latest, best performing model from the model \n",
      "registry and makes use of its prediction services. \n",
      "This action then goes on to provide the desired \n",
      "functionality in the application.Chapter 3  What Is MLOps?101 9. Performance and user data collection:  New \n",
      "data is collected as usual along with performance \n",
      "metrics related to the model. This data goes to \n",
      "the feature store, where the new data is processed \n",
      "and standardized so that it can be used in both the \n",
      "experimental stage and the deployment stage and \n",
      "there are no discrepancies between the data used by \n",
      "either stage. Performance data is stored so that data \n",
      "scientists can tell how the model is performing once \n",
      "deployed. Based on that data, important decisions \n",
      "such as whether or not to build a new model with a \n",
      "new architecture can be made.\n",
      " 10. Training pipeline trigger:  This trigger, upon \n",
      "activation, initiates the automated training pipeline \n",
      "for the deployed model and allows for feature \n",
      "retrieval by the pipeline from the feature store. The \n",
      "trigger can have any of the following conditions, \n",
      "although it is not limited to them:\n",
      "• Manual trigger:  Perhaps the model is to be trained \n",
      "only if the process is manually initiated. For \n",
      "example, data science teams can choose to start \n",
      "this process after reviewing performance and data \n",
      "and concluding that the deployed model needs to \n",
      "train on fresh batches of data.\n",
      "• Scheduled training:  Perhaps the model is set to \n",
      "train on a specific schedule. This can be a certain \n",
      "time on the weekend, every night during hours of \n",
      "lowest traffic, every month, and so on.Chapter 3  What Is MLOps?102• Performance issues:  Perhaps performance data \n",
      "indicates that the model’s performance has dipped \n",
      "below a certain benchmark. This can automatically \n",
      "activate the training process to attempt to get the \n",
      "performance back up to par. If this is not possible \n",
      "or is taking too many resources, data scientists and \n",
      "machine learning engineers can choose to build \n",
      "and deploy a new model.\n",
      "• Changes in  data patterns:  Perhaps changes in \n",
      "the trends of the data have been noticed while \n",
      "creating the features in the feature store. Of course, \n",
      "the feature store isn’t the only possible place that \n",
      "can analyze data and identify any new trends \n",
      "or changes in the data. There can be a separate \n",
      "process/program dedicated to this task, which can \n",
      "decide whether or not to activate the trigger.\n",
      "This would also be a good condition to begin the \n",
      "training process, since the new trends in the data \n",
      "are likely to lead to performance degradation. \n",
      "Instead of waiting for the performance hit to \n",
      "activate the trigger, the model can begin training \n",
      "on new data immediately upon sufficient \n",
      "detection of such changes in the data, allowing \n",
      "for the company to minimize any potential losses \n",
      "from such a scenario.\n",
      " Reflection on the Setup\n",
      "This implementation fixes many of the issues from the previous setup. \n",
      "Thanks to the integration of pipelines in the experimental stage, the \n",
      "previous problem of having the entire stage be composed of manual Chapter 3  What Is MLOps?103processes is no longer a concern. The pipeline automates the whole \n",
      "process of training, evaluating, and validating a model. The model \n",
      "development team now only needs to build the model and reuse any \n",
      "common training, evaluation, and validation procedures that are still \n",
      "applicable to this model. At the end of the model development pipeline, \n",
      "relevant model metrics are collected and displayed to the operator. These \n",
      "metrics can help the model development team to prototype quickly and \n",
      "arrive at optimal solutions even faster than they would have without the \n",
      "automation since they can run multiple pipelines on different models and \n",
      "compare all of them at once.\n",
      "Automated model creation pipelines in the experimental stage \n",
      "allow for teams to respond faster to any significant changes in the data \n",
      "or any issues with the deployed model that need to be resolved. Unlike \n",
      "before, where the only model swapping was the result of loading updated \n",
      "weights for the same model, these pipelines are structured to allow for \n",
      "various models with different architectures as long as they all use the \n",
      "same training, evaluation, and validation procedures. Thanks to the \n",
      "modularized code, the pipeline can simply swap out model classes and \n",
      "their respective weights once deployed. The modularization allows for \n",
      "easier deployment of the pipeline and lets models be swapped out easily to \n",
      "allow for further training of any model during deployment. Should a model \n",
      "require special attention from the model development team, it can simply \n",
      "be trained further by the team and swapped back in once it is ready. Now \n",
      "teams can respond much more quickly by being able to swap models in \n",
      "and out in such a manner.\n",
      "The pipelines also make it much easier for software engineering \n",
      "teams and operational teams to deploy the pipelines and models. Because \n",
      "everything is modularized, teams do not have to work on integrating \n",
      "new model classes into the application every time. Everyone benefits, \n",
      "and model development teams do not have to be as hesitant about \n",
      "implementing new architectures so as long as the new model still uses the \n",
      "same training, evaluation, and validation code as in the existing pipeline.Chapter 3  What Is MLOps?104While this setup solves most of the issues that plagued the original \n",
      "setup, there are still some important problems that remain. Firstly, there \n",
      "are no mechanisms in place to test and debug the pipelines, so this must \n",
      "all be done manually before it is pushed to a source repository. This can \n",
      "become a problem when you’re trying to push many iterations of pipelines, \n",
      "such as when you’re building different models with architectures that \n",
      "differ in how they must be trained, tested, and validated. Perhaps the latest \n",
      "models are showing a vast improvement over the old state-of-the art, and \n",
      "your team wants to implement these new solutions as soon as possible. In \n",
      "situations like this, teams will frequently need to debug and test pipelines \n",
      "before pushing them to source code for deployment. In this case, there is \n",
      "still some automation left to be done to avoid manual work.\n",
      "Pipelines are also manually deployed, so if the structure in the code \n",
      "changes, the engineering teams must rebuild parts of the application to \n",
      "work with the new pipeline and its modularized code. Modularization \n",
      "works smoothly when all components know what to expect from \n",
      "each other, but if the code of one of the components changes so that \n",
      "it isn’t compatible anymore, either the application must be rebuilt to \n",
      "accommodate the new changes or the component must be rewritten to \n",
      "work with the original pipeline. Unfortunately, new model architectures \n",
      "may require that part of the pipeline itself be rewritten, so it is likely \n",
      "that the application itself must be worked on to accommodate the new \n",
      "pipeline.\n",
      "Hopefully you begin to see the vast improvements that automation \n",
      "has made in this setup, but also the issues that remain to be solved. The \n",
      "automation has solved the issue of building and creating new models, but \n",
      "the problem of building and creating new pipelines still remains.\n",
      "To find an answer to that problem, let’s take a look at the last of the \n",
      "three setups defined earlier.Chapter 3  What Is MLOps?105 Continuous Integration/Continuous Delivery \n",
      "of Pipelines\n",
      "In this setup, we will be introducing a system to thoroughly test pipeline \n",
      "components before they are packaged and ready to deploy. This will ensure \n",
      "continuous integration of pipeline code  along with continuous delivery of \n",
      "pipelines , crucial elements of the automation process that the previous setup \n",
      "was missing. Refer to Figure  3-4 for a graphical representation of such a setup.\n",
      "Though this is mostly the same setup, we will go through it again step \n",
      "by step with an emphasis on the newly introduced elements.\n",
      " 1. Feature store:  The feature store contains \n",
      "standardized data processed into features. Features \n",
      "can be pulled by data scientists for offline data \n",
      "analysis. Upon activation of the trigger, features can \n",
      "also be sent to the automated training pipeline to \n",
      "further train the deployed model.\n",
      "Figure 3-4.  Graph depicting added testing systems and a package \n",
      "store to the automation setup in Figure  3-2Chapter 3  What Is MLOps?106 2. Data analysis:  This step is performed by data \n",
      "scientists on features pulled from the feature store. \n",
      "The results from the analysis can determine whether \n",
      "or not to build a new model or adjust the architecture \n",
      "of an existing model and retrain it from there.\n",
      " 3. Automated model building and analysis:  This \n",
      "step is performed by the model development team. \n",
      "Models can be built by the team and passed into the \n",
      "pipeline, assuming that they are compatible with the \n",
      "training, testing, and validation code, and the entire \n",
      "process is automatically conducted with a model \n",
      "analysis report generated at the end. In the case \n",
      "where the team wants to implement some of the \n",
      "latest machine learning architectures, models will \n",
      "have to be created from scratch with integration into \n",
      "pipelines in mind to maintain modularity. Parts of \n",
      "the pipeline code may have to change as well, which \n",
      "is acceptable because the new components of this \n",
      "setup can handle this automatically.\n",
      " 4. Modularized code:  Once the model reaches a \n",
      "minimum level of performance in the validation \n",
      "step, the pipeline, its components, and the model \n",
      "are all ready to be modularized and stored in a \n",
      "source repository.\n",
      " 5. Source repository:  The source repository holds \n",
      "all of the packaged pipeline and model code for \n",
      "different pipelines and different models. Teams can \n",
      "create multiples at once for different purposes and \n",
      "store them all here. In the old setup, pipelines and \n",
      "models would be pulled from here and manually Chapter 3  What Is MLOps?107integrated and deployed by software engineering \n",
      "teams. In this setup, the modularized code must \n",
      "now be tested to make sure all of the components \n",
      "will work correctly.\n",
      " 6. Testing:  This step is crucial in achieving continuous \n",
      "integration , or a result of automation where \n",
      "new components and elements are continuously \n",
      "designed, built, and deployed in the new \n",
      "environment.\n",
      "Pipelines and their components, including the \n",
      "model, must be thoroughly tested to ensure that \n",
      "all outputs are correct. Furthermore, the pipelines \n",
      "themselves must be tested so that they are \n",
      "guaranteed to work with the application and how it \n",
      "is designed. There shouldn’t be bugs in the pipeline, \n",
      "for example, that would break its compatibility with \n",
      "the application. The application is programmed to \n",
      "expect a specific behavior from the pipeline, and the \n",
      "pipeline must behave correspondingly.\n",
      "If you are familiar with software development, the \n",
      "testing of pipeline components and the models is \n",
      "similar to the automated testing that developers \n",
      "write to check various parts of an application’s \n",
      "functionality. A simple example is automated testing \n",
      "to ensure data of various types are successfully \n",
      "received by the server and are added to the correct \n",
      "databases.Chapter 3  What Is MLOps?108With pipelines and machine learning models, some \n",
      "examples of testing include:\n",
      "• Does the validation testing procedure lead to \n",
      "correct tuning of the hyperparameters?\n",
      "• Does each pipeline component work correctly? \n",
      "Does it output the expected element? For example, \n",
      "after model evaluation, does it correctly begin the \n",
      "validation step? (Alternatively, if model evaluation \n",
      "goes after model validation, does the evaluation \n",
      "step correctly initiate?)\n",
      "• Is the data processing performed correctly? Are \n",
      "there any issues with the data post-processing \n",
      "that would lead to poor model performance? \n",
      "Avoiding this outcome is for the best since it would \n",
      "waste resources having to fix the data processing \n",
      "component. If the business relies on rapid pipeline \n",
      "deployment, then avoiding this type of scenario is \n",
      "even more crucial.\n",
      "• Does the data processing component correctly \n",
      "perform data scaling? Does it correctly perform \n",
      "feature engineering? Does it correctly transform \n",
      "images?\n",
      "• Does the model analysis work correctly? You \n",
      "want to make sure that you’re basing decisions \n",
      "on accurate data. If the model truly performs well \n",
      "but faults in the model analysis component of the \n",
      "pipeline lead the data scientist/machine learning \n",
      "engineer to believe the model isn’t performing that \n",
      "well, then it could lead to issues where pipeline \n",
      "deployment is slowed down. Likewise, you don’t Chapter 3  What Is MLOps?109want the model analysis to be displaying the wrong \n",
      "information, even if it mistakenly displays precision \n",
      "for accuracy.\n",
      "The more thorough the automated testing, the \n",
      "better the guarantee that the pipeline will operate \n",
      "within the application without issues. (This doesn’t \n",
      "necessarily include model performance as that has \n",
      "to do more with the model architecture, how the \n",
      "model is developed, and what it is capable of.)\n",
      "Once the pipeline passes all the tests, it is then \n",
      "automatically packaged and sent to a package store. \n",
      "Continuous integration of pipelines is now achieved \n",
      "since teams can build modularized and tested \n",
      "pipelines much more quickly and have them ready \n",
      "for deployment.\n",
      " 7. Package store:  The package store is a containment \n",
      "unit that holds various packaged pipelines. It is \n",
      "optional but included in this setup so that there \n",
      "is a centralized area where all teams can access \n",
      "packaged pipelines that are ready for deployment. \n",
      "Model development teams push to this package \n",
      "store, and software engineers and operational \n",
      "teams can retrieve a packaged pipeline and deploy \n",
      "it. In this way, it is similar to the model registry in \n",
      "that both help achieve continuous delivery . The \n",
      "package store helps achieve continuous delivery of \n",
      "pipelines just as the model registry helps achieve \n",
      "continuous delivery of models and model services.\n",
      "Thanks to automated testing providing continuous \n",
      "integration of pipelines and continuous delivery of Chapter 3  What Is MLOps?110pipelines via the package store, pipelines can also be \n",
      "deployed rapidly by operational teams and software \n",
      "engineers. With this, businesses can easily keep \n",
      "up with the latest trends and advances in machine \n",
      "learning architectures, allowing for better and better \n",
      "performance and more involved services.\n",
      " 8. Deploy pipeline:  Pipelines can be retrieved \n",
      "from the package store and deployed in this step. \n",
      "Software engineering and operational teams must \n",
      "ensure that the pipeline will integrate without \n",
      "incident into the application. Because of that, \n",
      "there can be more testing on the part of software \n",
      "engineering teams to ensure proper integration of \n",
      "the pipeline. For example, one test can be to ensure \n",
      "the dependencies of the pipeline are considered \n",
      "in the application (if, for example, TensorFlow has \n",
      "updated and contains new functionality the pipeline \n",
      "now uses, the application should update its version \n",
      "of TensorFlow as well).\n",
      "Teams usually want to deploy the pipelines into \n",
      "a test environment where it will be subjected \n",
      "to further automated testing to ensure full \n",
      "compatibility with the application. This can be \n",
      "done automatically, where the pipelines go from \n",
      "the package store into the test environment, or \n",
      "manually, where teams decide to deploy the \n",
      "pipeline into the test environment. After the \n",
      "pipeline passes all the tests, teams can choose to \n",
      "manually deploy the pipeline into the production \n",
      "environment or have it automatically done.Chapter 3  What Is MLOps?111Either way, pipeline creation and deployment is a \n",
      "much faster process now especially since teams do \n",
      "not have to manually test the pipelines and they do \n",
      "not have to build or modify the application to work \n",
      "with the pipeline every time.\n",
      " 9. Automated training pipeline:  The automated \n",
      "training pipeline, once deployed, exists to further \n",
      "train models upon activation of the trigger. This \n",
      "helps keep models as up to date as possible on new \n",
      "trends in data and maintain high performance for \n",
      "longer. Upon validation of the model, models are \n",
      "sent to the model registry where they are held until \n",
      "they are needed for services.\n",
      " 10. Model registry:  The model registry holds trained \n",
      "models until they are needed for their services. \n",
      "Once again, continuous delivery of model services \n",
      "is achieved as the automated training pipeline \n",
      "continuously provides the model registry with high-\n",
      "performance machine learning models to be used to \n",
      "perform various services.\n",
      " 11. Model services:  The best models are pulled from \n",
      "the model registry to perform various services for \n",
      "the application.\n",
      " 12. Performance and user data collection:  Model \n",
      "performance data and user data is collected to be \n",
      "sent to model development teams and the feature \n",
      "store, respectively. Teams can use the model \n",
      "performance metrics along with the results from \n",
      "the data analysis to help decide their next course of \n",
      "action.Chapter 3  What Is MLOps?112 13. Training pipeline trigger:  This step involves some \n",
      "condition being met (refer to the previous setup, \n",
      "continuous model delivery ) to initiate the training \n",
      "process of the deployed pipeline and feed it with \n",
      "new feature data pulled from the feature store.\n",
      " Reflection on the Setup\n",
      "The main issue of the previous setup that this one fixes is that of pipeline \n",
      "deployment. Previously, pipelines had to be manually tested by machine \n",
      "learning teams and operational teams to ensure that the pipeline and \n",
      "its components worked, and that the pipeline and its components \n",
      "were compatible with the application. However, in this setup, testing is \n",
      "automated, allowing for teams to much more easily build and deploy \n",
      "pipelines than before. The biggest advantage to this is that businesses can \n",
      "now keep up with significant changes in the data requiring the creation \n",
      "of new models and new pipelines, and can also capitalize on the latest \n",
      "machine learning trends and architectures all thanks to rapid pipeline \n",
      "creation and deployment combined with continuous delivery of model \n",
      "services from the previous setup.\n",
      "The important thing to understand from all these examples is that \n",
      "automation is the way to go. Machine learning technology has progressed \n",
      "incredibly far within the last decade alone, but finally, the infrastructure to \n",
      "allow you to capitalize on these advancements is catching up.\n",
      "Hopefully, after seeing the three possible MLOps setups, you \n",
      "understand more about MLOps and how implementations of MLOps \n",
      "principles might look. You might have noticed that pipelines have been \n",
      "mentioned quite often throughout the descriptions of the setups, and you \n",
      "might be wondering, “What are pipelines, and why are they so crucial for \n",
      "automation?”\n",
      "To answer that question, let’s take a look at what a “pipeline” really is.Chapter 3  What Is MLOps?113 Pipelines and Automation\n",
      "Pipelines are an important part of automation setups employing DevOps \n",
      "principles. One way to think about a pipeline  is that it is a specific, often \n",
      "sequential procedure that dictates the flow of information as it passes \n",
      "through the pipeline. To see an example of a testing pipeline in a software \n",
      "development setting, refer to Figure  3-5.\n",
      "In the MLOps setups above, you’ve seen pipelines for automating the \n",
      "process of training a deployed model and for building, testing, and packing \n",
      "pipelines as well as for testing integration of packaged pipelines before \n",
      "deploying them to the production environment.\n",
      "So, what does all that really mean? To get a better idea of what exactly \n",
      "goes on in a pipeline, let’s follow the flow of data through a pipeline in the \n",
      "experimental stage. Even if you understand how pipelines work, it may \n",
      "be worth following the example anyway as we now look at this pipeline \n",
      "through the context of using MLOps APIs.\n",
      "Figure 3-5.  A tes ting pipeline in a software development setting. \n",
      "The pipeline for testing packaged model pipelines in the optimal \n",
      "setup above is similar in that individual components must be tested, \n",
      "components must be tested in groups, and in the case where pipelines \n",
      "are deployed to a test environment first where further tests are \n",
      "performed before they are deployed to the production environmentChapter 3  What Is MLOps?114 Journey Through a Pipeline\n",
      "We will be looking at the model development pipeline in the experimental \n",
      "stage. Before we begin, it is important to mention that we will be \n",
      "referencing API calls in this pipeline. This is because some APIs can be \n",
      "called while executing scripts or even Jupyter cells at key points in the \n",
      "model’s development, giving MLOps monitoring software information on \n",
      "model training, model evaluation, and model validation. At the end of the \n",
      "pipeline, the MLOps software would also ready the model for deployment \n",
      "via functionality provided by the API.\n",
      "You will read more about this API in the next chapter, Chapter 4, but \n",
      "for now, you may assume that the API will take care of automation as you \n",
      "follow along through the example.\n",
      " Model Selection\n",
      "As seen in Figure  3-4, the experimental pipeline begins with the selection \n",
      "of a model. This is up to the operator, who must now choose and build a \n",
      "model. Some APIs allow you to call their functionality while building the \n",
      "model to connect with MLOps software as the rest of the process goes \n",
      "on. This software then keeps track of all relevant metrics related to the \n",
      "model’s development along with the model itself in order to initiate the \n",
      "deployment process.\n",
      "In this case, the operator has chosen to use a logistic regression. Refer \n",
      "to Figure  3-6.Chapter 3  What Is MLOps?115 Data Preprocessing\n",
      "With the model now selected and built, and with feature data supplied by \n",
      "the feature store, the process can now move forward to the next stage in \n",
      "the pipeline: data preprocessing. Refer to Figure  3-7.\n",
      "Figure 3-6.  A graphical representation of a pipeline where the \n",
      "operator has selected a logistic regression model. The rest of the steps \n",
      "have been hidden for now and will appear as we gradually move \n",
      "through the pipelineChapter 3  What Is MLOps?116The data preprocessing can be done manually or automatically. In \n",
      "this case, the data preprocessing only involves normalization and resizing \n",
      "of image feature data, so the operator can implement this manually. \n",
      "Depending on the level of automation, the operator can also call some \n",
      "function that takes in data and automatically processes it depending on \n",
      "the type of data and any other parameters provided.\n",
      "Either way, the end of the processing stage will result in the data being \n",
      "broken up into subsets. In this example, the operator chose to create a \n",
      "training set, a testing set, and a validation set. Now, the operator can begin \n",
      "the training process.\n",
      " Training Process\n",
      "Depending on the framework being used, the operator can further split \n",
      "up the training data into a training set and a data validation set and use \n",
      "both in the training process. The data validation set exists totally separate \n",
      "Figure 3-7.  The operator has chosen to normalize and resize the \n",
      "image data. The process creates a training set, a testing set, and a \n",
      "validation setChapter 3  What Is MLOps?117from the training set (although it is derived from it) since the model never \n",
      "sees it during training. Its purpose is to periodically evaluate the model’s \n",
      "performance on a data set that it has never seen before. Refer to Figure  3-8.\n",
      "In the context of deep learning, for example, the model can evaluate \n",
      "on the validation set at the end of each epoch, generating some metric \n",
      "data for the operator to see. Based on this, the operator can judge how \n",
      "the model is doing and whether or not it could be overfitting and adjust \n",
      "hyperparameters or model structure if needed.\n",
      "The API can also be told what script to run in order to initiate this \n",
      "entire pipeline process. The script can contain the training, evaluation, \n",
      "and validation code all at once so the API can run this entire pipeline when \n",
      "needed.\n",
      "Once the training process is done, the process moves to the evaluation \n",
      "stage.\n",
      "Figure 3-8.  The model training process beginsChapter 3  What Is MLOps?118 Model Evaluation\n",
      "In the evaluation stage, the model’s performance is measured on a test \n",
      "data set that it has never seen. This performance will indicate to the \n",
      "operator whether or not the model is overfitting, especially if it performed \n",
      "extremely well in training but has trouble replicating those results in this \n",
      "stage. That is part of why the training data can be split to include some \n",
      "validation data, as it can be an early indicator of overfitting. This can be \n",
      "crucial especially if the model takes a significant amount of time to run. \n",
      "You would rather know earlier, partway through training, if the model \n",
      "is overfitting, rather than after it ran all night and is evaluated the next \n",
      "morning. Refer to Figure  3-9.\n",
      "Figure 3-9.  Training results are stored in a common area (for \n",
      "example, the API could be called to monitor these results) for the \n",
      "metrics of the current model. Model evaluation begins on the trained \n",
      "model using the testing setChapter 3  What Is MLOps?119Another thing to note again is that the validation stage could come \n",
      "before the evaluation stage, but in this case, the trained model will be \n",
      "evaluated first on a test data set before the validation stage begins. This \n",
      "is just to get a sense of how the model does on the testing set before \n",
      "hyperparameter tuning begins. Of course, hyperparameter tuning via the \n",
      "validation step could be performed first before the final model evaluation, \n",
      "but in some frameworks, model evaluation would come first. An example \n",
      "of this is a validation process like scikit-learn’s cross-validation. Of course, \n",
      "you can evaluate the tuned model on the test set once again to get a final \n",
      "performance evaluation.\n",
      "Once the evaluation finishes, metrics are stored by the API or by some \n",
      "other mechanism that the team has implemented, and the process moves \n",
      "on to the validation stage.\n",
      " Model Validation\n",
      "In this stage, the model begins the validation process, which attempts \n",
      "to seek the best hyperparameters. You could combine the use of a script \n",
      "to iterate through various configurations of hyperparameter values \n",
      "and utilize k-fold cross-validation, for example, to help decide the best \n",
      "hyperparameters. Refer to Figure  3-10 .Chapter 3  What Is MLOps?120In any case, the point of a validation set is to help tune the model’s \n",
      "hyperparameters. The team could even automate this process entirely \n",
      "if they tend to train a lot of models of the same few types, saving \n",
      "time and resources in the long run by automating the validation and \n",
      "hyperparameter tuning process for that set of models.\n",
      "Finally, once the model achieves a good level of performance and \n",
      "finishes the validation stage, the validation results are stored, and all \n",
      "relevant data is displayed as a summary to the operator. Again, depending \n",
      "on the level of automation, perhaps the model is retrained and evaluated \n",
      "on the best hyperparameter setup discovered in the validation stage. The \n",
      "API simply needs to be told what metrics to track and it will automatically \n",
      "do so.\n",
      "Figure 3-10.  Evaluation metrics are stored along with the training \n",
      "metrics by the API, and the validation process beginsChapter 3  What Is MLOps?121 Model Summary\n",
      "At this point, the operator can compare the outcome of this experiment \n",
      "with that of other models, using the metrics as baselines for comparison. \n",
      "The API can track the relevant metrics for different model runs and can \n",
      "compare them all at the same time. Should the operator decide to move \n",
      "forward with this particular model, the API and the MLOps software \n",
      "can allow for deployment on a simple click of a button. Usually, the \n",
      "deployment is to a staging environment first, where the functionality can \n",
      "be tested further before moving directly into the production environment. \n",
      "Everything is configurable, and the API can adapt to the needs of the \n",
      "business and its workflow. If the developers want to deploy straight to \n",
      "production, sure, though that could potentially be unwise considering the \n",
      "case of failure. Refer to Figure  3-11 .\n",
      "Figure 3-11.  Validation is complete, and all metrics are displayed to \n",
      "the operatorChapter 3  What Is MLOps?122After the model passes the tests in the staging environment, it can \n",
      "then be deployed to the production environment, where it can be further \n",
      "monitored by the software.\n",
      "Hopefully now you have a better understanding of what a pipeline \n",
      "really is. The pipelines for models and pipeline integration testing are \n",
      "similar, except they are assisted by MLOps software and APIs such as \n",
      "Databricks and MLFlow, for example. Let’s now look at how you can go \n",
      "about using those APIs and software to help you implement MLOps.\n",
      " How to Implement MLOps\n",
      "MLOps sounds great. It helps you deploy machine learning models rapidly \n",
      "and helps maintain them once they’re deployed. However, the biggest \n",
      "problem now seems to be the question of how to get there. The level of \n",
      "automation described in the setups requires significant work from both the \n",
      "“ML ” and “Ops” sides of the workflow to achieve it. It almost seems better \n",
      "in the short run to build and deploy the models manually rather than \n",
      "devote resources to setting up the entire infrastructure, but this is simply \n",
      "unsustainable in the long run.\n",
      "Also, Jupyter is great for performing experiments, so is there a way to \n",
      "track them as well? This sort of functionality would be extremely useful \n",
      "especially when teams are implementing advanced machine learning \n",
      "architectures from scratch, as it would let them compare the new models \n",
      "across all of the relevant metrics with deployed models or current \n",
      "architectures. Tasks like these are more convenient to do in a notebook \n",
      "and having to convert everything to a proper model file is simply further \n",
      "work.\n",
      "The takeaway here is that accounting for these factors and more would \n",
      "require significant resources to plan, develop, and test. For smaller-scale \n",
      "businesses, this is an undertaking that’s possibly beyond their reach. So, \n",
      "what now?Chapter 3  What Is MLOps?123The good news is that are a great assortment of tools available to use \n",
      "now that essentially implement all of the automation for you, such as \n",
      "the API we looked at in the pipeline example earlier. Several examples of \n",
      "such tools that we will explore in later chapters are MLFlow, Databricks , \n",
      "AWS SageMaker , Microsoft Azure , Google Cloud , and Datarobots . With \n",
      "these tools, implementing MLOps principles into your workflow will be \n",
      "significantly easier.\n",
      "In the case of MLFlow, integrating it into code is extremely simple. \n",
      "You only have to write a couple lines of code to track all of the metrics \n",
      "you need. The functionality of the API we looked at earlier in the pipeline \n",
      "example is all provided by MLFlow. Furthermore, MLFlow also saves the \n",
      "model for you, allowing for model serving functionality where given some \n",
      "data, the model returns its predictions.\n",
      "MLFlow also integrates into Databricks, AWS SageMaker, Microsoft \n",
      "Azure, and can be deployed to Google Cloud as well, all of which are \n",
      "tools that help manage your MLOps setup and serve as platforms to \n",
      "deploy your models on. While the cloud platforms do provide some \n",
      "MLOps functionality, with the extent of this varying for each platform, the \n",
      "advantage of using MLFlow is that it lets you have the freedom of choice \n",
      "when it comes to one platform to commit to. Furthermore, it gives you a \n",
      "greater degree of freedom, as you can perform all the experiments locally \n",
      "and offline, and you can support models from many different frameworks. \n",
      "MLFlow also provides functionality to help you modularize any custom-  \n",
      "built models or models made from other frameworks not explicitly \n",
      "supported.\n",
      "And so, to really answer the question of how to implement MLOps, \n",
      "you will get familiar with MLFlow and explore each of those tools. The goal \n",
      "is to take the model we built in Chapter 2 all the way to deployment and \n",
      "beyond.Chapter 3  What Is MLOps?124 Summary\n",
      "MLOps is a set of principles and practices adopted from DevOps and \n",
      "applied to machine learning. You explored three different types of MLOps \n",
      "setups with varying degrees of automation: manual implementation , \n",
      "continuous model delivery , and continuous integration/continuous \n",
      "delivery of pipelines . You identified that the manual implementation was \n",
      "riddled with issues regarding scalability and efficiency and you explored \n",
      "a setup that ensured continuous model delivery. Although this setup \n",
      "fixed many of the issues found in the manual setup, there were still some \n",
      "problems with pipeline integration testing to be solved. The final setup \n",
      "solved this issue too and ensured continuous integration and delivery of \n",
      "pipelines, completing the total automation setup.\n",
      "You also looked into what a pipeline really is so that you can \n",
      "understand why they are so crucial to the automation setup. Finally, you \n",
      "learned about some tools that can help you implement MLOps into your \n",
      "workspace, avoiding the trouble of implementing all the automation from \n",
      "scratch. In the next chapter, you will look at MLFlow, an excellent API that \n",
      "lets you implement your own MLOps setups and is compatible with many \n",
      "platforms and frameworks.Chapter 3  What Is MLOps?125© Sridhar Alla, Suman Kalyan Adari 2021 \n",
      "S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \n",
      "https://doi.org/10.1007/978-1-4842-6549-9_4CHAPTER 4\n",
      "Introduction \n",
      "to MLFlow\n",
      "In this chapter, we will cover what MLFlow is, what it does, and how \n",
      "you can implement MLOps setups into your existing projects. More \n",
      "specifically, we will cover how you can integrate MLFlow with scikit-learn, \n",
      "TensorFlow 2.0+/Keras, PyTorch, and PySpark. We will go over experiment \n",
      "creation; metric, parameter, and artifact logging; model logging; and how \n",
      "you can deploy models on a local server and query them for predictions.\n",
      " Introduction\n",
      "In the previous chapter, we went over what an optimal MLOps setup \n",
      "looks like. However, the level of automation presented would require \n",
      "an immense amount of resources dedicated to the project. Fortunately, \n",
      "there are APIs that do this for you, such as MLFlow . MLFlow is an API that \n",
      "allows you to integrate MLOps principles into your projects with minimal \n",
      "changes made to existing code. With just a couple lines of code here and \n",
      "there, you can track all of the details relevant to the project that you want. \n",
      "Furthermore, you can even save the model for future use in deployment, \n",
      "for example, and you can compare all of the metrics between individual \n",
      "models to help you select the best model.126The great thing about MLFlow is that it abstracts everything for you. \n",
      "It packages and modularizes the models for you so that when you deploy \n",
      "the model and want to make predictions, all you need to do is simply pass \n",
      "in the input data in a certain format. All of the modularization that we \n",
      "discussed in the previous chapter with the pipelines is taken care of by \n",
      "MLFlow. MLFlow also allows you to create a wrapper around your model \n",
      "if your model prediction code needs to be different. We will look at this \n",
      "functionality in detail in the next chapter, when you deploy your models to \n",
      "Amazon SageMaker. Even with custom code, MLFlow will modularize it so \n",
      "that it will still work the same way as any other model once it is deployed \n",
      "and ready to make predictions.\n",
      "In detail, we will go over the following in this chapter:\n",
      "• Creating experiments:  Experiments in MLFlow \n",
      "essentially allow you to group your models and any \n",
      "relevant metrics. For example, you can compare \n",
      "models that you’ve built in TensorFlow and in PyTorch \n",
      "and name this experiment something like  \n",
      "pytorch_tensorflow . In the context of anomaly \n",
      "detection, you can create an experiment called  \n",
      "model_prototyping  and group all of the models that \n",
      "you want to test by running the training pipelines after \n",
      "setting model_prototyping  as the experiment name.\n",
      "As you’ll see shortly, grouping model training \n",
      "sessions by experiment can really help organize \n",
      "your workspace because you’ll get a clear idea of the \n",
      "context behind trained models.\n",
      "• Model and metric logging:  MLFlow allows you to \n",
      "save a model in a modularized form and log all of the \n",
      "metrics related to the model run . A model run can be \n",
      "thought of as the model training, testing, and validation Chapter 4  Introdu CtIon to MLF Low127pipeline from the previous chapter. In MLFlow, you \n",
      "can mark the start and the end of each run and decide \n",
      "which metrics you want to save. Additionally, you can \n",
      "save graphs, so you can also view plots like confusion \n",
      "matrices and ROC curves. A model run  is basically the \n",
      "instance in which MLFlow executes the code that you \n",
      "tell it to, so if you want, you can only train the model \n",
      "and leave it at that.\n",
      "It is possible for you to train, evaluate, and even \n",
      "validate your model, logging all of the metrics for \n",
      "each respective step in the whole process. MLFlow \n",
      "gives you a lot of flexibility in how you define \n",
      "a model run. You can end the run after simply \n",
      "training it, or you can end the run after training \n",
      "and evaluating it. If you wish, you can even set up \n",
      "an entire validation script to log the entire process \n",
      "for you, allowing you to much more easily compare \n",
      "different hyperparameter setups all at once in \n",
      "MLFlow. We will explore how to perform model \n",
      "validation with MLFlow shortly when we revisit the \n",
      "scikit-learn experiment from Chapter 2.\n",
      "• Comp aring model metrics:  MLFlow also allows you \n",
      "to compare different models and their metrics all at \n",
      "once. And so, when performing validation to help \n",
      "tune a model’s hyperparameters, you can compare all \n",
      "of the selected metrics together in MLFlow using its \n",
      "user interface. In the previous chapter, you printed out \n",
      "everything, making the cell output possibly very large \n",
      "if the script is quite involved in its hyperparameter \n",
      "setups.Chapter 4  Introdu CtIon to MLF Low128• Model Registry:  MLFlow also adds functionality to \n",
      "allow you to implement a model registry, allowing you \n",
      "to define what stage a particular model is in. Databricks \n",
      "integrates quite well with MLFlow, providing built-  \n",
      "in model registry functionality. You will explore how \n",
      "to use the MLFlow Model Registry when you look at \n",
      "Databricks in Appendix.\n",
      "• Local deployment:  MLFlow also allows you to \n",
      "deploy on a local server, allowing you to test model \n",
      "inference . Model inference is basically the prediction \n",
      "process of a model. Data is sent to the model in one of \n",
      "several standardized formats, and MLFlow returns the \n",
      "predictions made by the model.\n",
      "Such a setup can easily be converted to work on \n",
      "a hosted server as well. As you will see in the next \n",
      "several chapters, MLFlow also allows you to deploy \n",
      "your models on popular cloud services such as \n",
      "Amazon SageMaker, Microsoft Azure, Google Cloud, \n",
      "and Databricks. The process at its core remains \n",
      "similar to how you will perform local model serving. \n",
      "The only difference comes with where you host the \n",
      "model and the particular procedure for querying it.\n",
      "With that being said, let’s get started by revisiting the scikit-learn \n",
      "logistic regression model and integrating MLFlow into it.Chapter 4  Introdu CtIon to MLF Low129 MLFlow with Scikit-Learn\n",
      "Before we begin, here are the versions of Python and the packages that \n",
      "were used:\n",
      "• Python : 3.6.5\n",
      "• numpy : 1.18.5\n",
      "• scikit-learn : 0.22.2.post1\n",
      "• pandas : 1.1.0\n",
      "• Matplotlib : 3.2.1\n",
      "• Seaborn : 0.10.1\n",
      "• MLFlow : 1.10.0\n",
      "You don’t need the exact versions of the packages we used, but in case \n",
      "some functionality is removed, renamed, or just changed in the newer \n",
      "versions and the code runs into an error, you have the exact version of the \n",
      "module you can try running the code with.\n",
      "MLFlow in particular is updated quite frequently, so you are more \n",
      "likely to run into issues running code with something like MLFlow \n",
      "compared to a package like numpy.\n",
      "With that being said, let’s dive into the first example. In this case, let’s \n",
      "revisit the scikit-learn code from the previous chapter and add MLFlow \n",
      "integration to it.\n",
      " Data Processing\n",
      "First, you begin with all of the imports:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib #Chapter 4  Introdu CtIon to MLF Low130import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import sklearn #\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.metrics import roc_auc_score, plot_roc_curve, \n",
      "confusion_matrix\n",
      "from sklearn.model_selection import KFold\n",
      "import mlflow\n",
      "import mlflow.sklearn\n",
      "print(\"Numpy: {}\".format(np.__version__))\n",
      "print(\"Pandas: {}\".format(pd.__version__))\n",
      "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
      "print(\"seaborn: {}\".format(sns.__version__))\n",
      "print(\"Scikit-Learn: {}\".format(sklearn.__version__))\n",
      "print(\"MLFlow: {}\".format(mlflow.__version__))\n",
      "The output should look something like Figure  4-1.\n",
      "Figure 4-1.  The output of importing the necessary modules and \n",
      "printing out their versionsChapter 4  Introdu CtIon to MLF Low131Now you can move on to loading the data:\n",
      "data_path = \"data/creditcard.csv\"\n",
      "df = pd.read_csv(data_path)\n",
      "df = df.drop(\"Time\", axis=1)\n",
      "Refer to Figure  4-2 to see the code in a cell.\n",
      "Note that you are once again dropping the column Time.\n",
      "You can now check to see if the data loaded in correctly:\n",
      "df.head()\n",
      "Refer to Figure  4-3 to see the head() function.\n",
      "Figure 4-2.  Loading the data set and dropping the column named \n",
      "Time because it adds very large data values that ultimately don’t have \n",
      "much of a correlation with the column Class. Model performance is \n",
      "boosted slightly simply by dropping the extraneous information\n",
      "Figure 4-3.  Verifying that the data was loaded correctly by using \n",
      "the head() function. As you can see, the columns and the data have \n",
      "loaded in correctlyChapter 4  Introdu CtIon to MLF Low132Again, you are dropping the column Time  from the data frame this \n",
      "time. This is because this column was found to add data that isn’t very \n",
      "helpful in finding an anomaly and only adds extra complexity to the data.\n",
      "In the case of deep learning models, your model might eventually learn \n",
      "that the Time  data does not correlate very well with the Class  labels and \n",
      "may place less importance on nodes processing that data. Eventually, it \n",
      "might even ignore the Time  data. However, you can speed up the learning \n",
      "process by cutting out these types of features from your training sets. This \n",
      "is because you’re sparing the models the time and resources needed to \n",
      "figure that out.\n",
      "Moving on, you will split the normal points and the anomalies:\n",
      "normal = df[df.Class == 0].sample(frac=0.5,  \n",
      "random_state=2020).reset_index(drop= True)\n",
      "anomaly = df[df.Class == 1]\n",
      "Let’s print out their respective shapes:\n",
      "print(f\"Normal: {normal.shape}\")\n",
      "print(f\"Anomaly: {anomaly.shape}\")\n",
      "Refer to Figure  4-4 to see the above two cells in Jupyter along with their \n",
      "outputs.\n",
      "Figure 4-4.  Randomly sampling 50% of all the normal data points \n",
      "in the data frame and picking out all of the anomalies from the data \n",
      "frame as separate data frames. Then, you print the shapes of both \n",
      "data sets. As you can see, the normal points massively outnumber the \n",
      "anomaly pointsChapter 4  Introdu CtIon to MLF Low133You are going to split the normal and anomaly sets into train-test-  \n",
      "validate subsets. Run the following two code blocks:\n",
      "normal_train, normal_test = train_test_split(normal,  \n",
      "test_size = 0.2, random_state = 2020)\n",
      "anomaly_train, anomaly_test = train_test_split \n",
      "(anomaly, test_size = 0.2, random_state = 2020)\n",
      "normal_train, normal_validate = train_test_split(normal_train, \n",
      "test_size = 0.25, random_state = 2020)\n",
      "anomaly_train, anomaly_validate = train_test_split \n",
      "(anomaly_train, test_size = 0.25, random_state = 2020)\n",
      "Refer to Figure  4-5 to see both code blocks in their respective cells.\n",
      "Now, you can process these sets and create the x-y splits:\n",
      "x_train = pd.concat((normal_train, anomaly_train))\n",
      "x_test = pd.concat((normal_test, anomaly_test))\n",
      "x_validate = pd.concat((normal_validate, anomaly_validate))\n",
      "y_train = np.array(x_train[\"Class\"])\n",
      "y_test = np.array(x_test[\"Class\"])\n",
      "y_validate = np.array(x_validate[\"Class\"])\n",
      "Figure 4-5.  Partitioning the normal and anomaly data frames \n",
      "separately into train, test, and validation splits. Initially, 20% of \n",
      "the normal and anomaly points are used as the test split. From \n",
      "the remaining 80% of data, 25% of that train split is used as the \n",
      "validation split, meaning the validation split is 20% of the original \n",
      "data. This leaves the final training split at 60% of the original data. In \n",
      "the end, the train-test-validate split has a 60-20-20 ratio, respectivelyChapter 4  Introdu CtIon to MLF Low134x_train = x_train.drop(\"Class\", axis=1)\n",
      "x_test = x_test.drop(\"Class\", axis=1)\n",
      "x_validate = x_validate.drop(\"Class\", axis=1)\n",
      "Refer to Figure  4-6 to see the above code block in a cell.\n",
      "You can print out the shapes of these sets:\n",
      "print(\"Training sets:\\nx_train: {} \\ny_train:  \n",
      "{}\".format(x_train.shape, y_train.shape))\n",
      "print(\"\\nTesting sets:\\nx_test: {} \\ny_test:  \n",
      "{}\".format(x_test.shape, y_test.shape))\n",
      "print(\"\\nValidation sets:\\nx_validate: {} \\ny_validate: {}\".\n",
      "format(x_validate.shape, y_validate.shape))\n",
      "Refer to Figure  4-7 to see the output shapes.\n",
      "Figure 4-6.  Creating the respective x and y splits of the training, \n",
      "testing, and validation sets by concatenating the respective normal \n",
      "and anomaly sets. You drop Class from the x-sets because it would be \n",
      "cheating otherwise to give it the label directly. You are trying to get the \n",
      "model to learn the labels by reading the x-data, not learn how to read \n",
      "the Class column in the x-dataChapter 4  Introdu CtIon to MLF Low135Finally, you scale your data using scikit-learn’s standard scaler:\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(pd.concat((normal, anomaly)).drop(\"Class\", axis=1))\n",
      "x_train = scaler.transform(x_train)\n",
      "x_test = scaler.transform(x_test)\n",
      "x_validate = scaler.transform(x_validate)\n",
      "Refer to Figure  4-8.\n",
      "Figure 4-7.  Printing out the shapes of the training, testing, and \n",
      "validation sets\n",
      "Figure 4-8.  Fitting the scaler on the superset of normal and anomaly \n",
      "points after dropping Class to scale the x-setsChapter 4  Introdu CtIon to MLF Low136 Training and Evaluating with MLFlow\n",
      "All that is left now is to train and evaluate your model. We will showcase \n",
      "validation with MLFlow functionality in a bit, but first let’s define the \n",
      "train and test functions to organize the code. This is also where you start \n",
      "integrating MLFlow into your code. Here is the train  function:\n",
      "def train(sk_model, x_train, y_train):\n",
      "    sk_model = sk_model.fit(x_train, y_train)\n",
      "    train_acc = sk_model.score(x_train, y_train)\n",
      "    mlflow.log_metric(\"train_acc\", train_acc)\n",
      "    print(f\"Train Accuracy: {train_acc:.3%}\")\n",
      "Refer to Figure  4-9 to see this code in a cell.\n",
      "You may have noticed the first of the new code with this line:\n",
      "mlflow.log_metric(\"train_acc\", train_acc)\n",
      "You create a new metric here specifically for the training accuracy so \n",
      "that you can keep track of this metric. Furthermore, you are telling MLFlow \n",
      "to log this metric, so that MLFlow will keep track of this value in each run. \n",
      "When you log multiple runs, you can compare this metric across each \n",
      "of those runs so that you can pick a model with the best AUC score for \n",
      "example.\n",
      "Figure 4-9.  Defining the train function to better organize the code. \n",
      "Additionally, you are defining a training accuracy metric that will be \n",
      "logged by MLFlowChapter 4  Introdu CtIon to MLF Low137Let’s now move on to the evaluate  function:\n",
      "def evaluate(sk_model, x_test, y_test):\n",
      "    eval_acc = sk_model.score(x_test, y_test)\n",
      "    preds = sk_model.predict(x_test)\n",
      "    auc_score = roc_auc_score(y_test, preds)\n",
      "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
      "    mlflow.log_metric(\"auc_score\", auc_score)\n",
      "    print(f\"Auc Score: {auc_score:.3%}\")\n",
      "    print(f\"Eval Accuracy: {eval_acc:.3%}\")\n",
      "     roc_plot = plot_roc_curve(sk_model, x_test, y_test, \n",
      "name='Scikit-learn ROC Curve')\n",
      "    plt.savefig(\"sklearn_roc_plot.png\")\n",
      "    plt.show()\n",
      "    plt.clf()\n",
      "    conf_matrix = confusion_matrix(y_test, preds)\n",
      "    ax = sns.heatmap(conf_matrix, annot= True,fmt='g')\n",
      "    ax.invert_xaxis()\n",
      "    ax.invert_yaxis()\n",
      "    plt.ylabel('Actual')\n",
      "    plt.xlabel('Predicted')\n",
      "    plt.title(\"Confusion Matrix\")\n",
      "    plt.savefig(\"sklearn_conf_matrix.png\")\n",
      "    mlflow.log_artifact(\"sklearn_roc_plot.png\")\n",
      "    mlflow.log_artifact(\"sklearn_conf_matrix.png\")\n",
      "Refer to Figure  4-10  to see the above code in a cell.Chapter 4  Introdu CtIon to MLF Low138Once again, you have told MLFlow to log two more metrics: the AUC \n",
      "score and the accuracy on the test set. You do so with these lines of code:\n",
      "mlflow.log_metric(\"eval_acc\", eval_acc)\n",
      "mlflow.log_metric(\"auc_score\", auc_score)\n",
      "Furthermore, you can also tell MLFlow to save the plots generated by \n",
      "matplotlib and by seaborn. With this, you can look at each of the graphs for \n",
      "each training run and do so in a highly organized manner. You must first \n",
      "save these plots, which you do in the same directory. Then, you must tell \n",
      "MLFlow to grab the artifacts to log them like so:\n",
      "mlflow.log_artifact(\"sklearn_roc_plot.png\")\n",
      "mlflow.log_artifact(\"sklearn_conf_matrix.png\")\n",
      "Make sure that they have the same names as the graphs you saved.\n",
      "Figure 4-10.  A function to calculate the evaluation metrics for the \n",
      "AUC score and accuracy. Plots for the confusion matrix and the ROC \n",
      "curve are generated, and both the metrics and the graphs are logged \n",
      "to MLFlowChapter 4  Introdu CtIon to MLF Low139 Logging and Viewing MLFlow Runs\n",
      "Finally, let’s run the code that actually sets the experiment name, starts the \n",
      "MLFlow run, and executes all this code:\n",
      "sk_model = LogisticRegression(random_state= None,  \n",
      "max_iter=400, solver='newton-cg')\n",
      "mlflow.set_experiment(\"scikit_learn_experiment\")\n",
      "with mlflow.start_run():\n",
      "    train(sk_model, x_train, y_train)\n",
      "    evaluate(sk_model, x_test, y_test)\n",
      "    mlflow.sklearn.log_model(sk_model, \"log_reg_model\")\n",
      "     print(\"Model run: \", mlflow.active_run().info.run_uuid)\n",
      "mlflow.end_run()\n",
      "Notice the new lines of MLFlow code. We will go through them one by \n",
      "one.\n",
      "First, let’s begin with what appears to set the experiment name:\n",
      "mlflow.set_experiment(\"scikit_learn_experiment\")\n",
      "What this does is that it puts the run under whatever experiment name \n",
      "you pass in as a parameter. If that name does not exist, MLFlow will create \n",
      "a new one under that name and put the run there.\n",
      "with mlflow.start_run():\n",
      "     ...\n",
      "     ...\n",
      "This line of code allows you to chunk all of your code under the context \n",
      "of one MLFlow run. This ensures that there are no discrepancies between \n",
      "where your metrics are being logged, and that it doesn’t create two \n",
      "different runs when you mean it to log everything for the same run.\n",
      "mlflow.sklearn.log_model(sk_model, \"log_reg_model\")Chapter 4  Introdu CtIon to MLF Low140This line of code is the general convention to use when you’re logging a \n",
      "model. The parameters, in order, are the model you’re saving and then the \n",
      "name you’re setting for the model when saving. In this case, you are saving \n",
      "your logistic regression model with the name log_reg_model  in this run.\n",
      "As you will see later, most other frameworks follow the same style \n",
      "when saving the model. There are a couple exceptions, but we will cover \n",
      "this when the time comes. In this case, you are calling mlflow.sklearn , \n",
      "but if you wanted to log a PySpark model, you would do mlflow.spark .\n",
      "Basically, the framework the model was built in must match the \n",
      "framework module of MLFlow when logging the model. It is possible \n",
      "to create a custom “model” in MLFlow and log this as well, something \n",
      "that is covered in the documentation. You can use this custom model to \n",
      "then specify how you want the prediction function to work. If you’d like \n",
      "to process the data some more before making predictions, for example, \n",
      "MLFlow allows you to specify this extra functionality through the use of the \n",
      "MLFlow PyFunc module. Refer to the documentation, which you can find \n",
      "here: www.mlflow.org/docs/latest/models.html#model-customization .\n",
      "print(\"Model run: \", mlflow.active_run().info.run_uuid)\n",
      "This line of code essentially gets the current run that the model and \n",
      "metrics are being logged to and prints it out. This makes it handy if you \n",
      "want to retrieve the run directly from the notebook itself instead of going to \n",
      "the UI to do so.\n",
      "mlflow.end_run()\n",
      "Finally, this tells MLFlow to end the current run. In cases where there \n",
      "is an error in the MLFlow start run code block, and the run does not \n",
      "terminate, do this to forcibly end the current run. Basically, it is there to \n",
      "ensure that MLFlow stops the run after you executed all the code relevant \n",
      "to the current run.\n",
      "Moving on, refer to Figure  4-11  to see the full output of the code.Chapter 4  Introdu CtIon to MLF Low141You can see that MLFlow automatically generates a new experiment \n",
      "if it does not already exist, so you can create a new experiment directly \n",
      "from the code. You can also see that the rest of the code basically outputs \n",
      "as usual, except it also prints the run ID of the current MLFlow run just as \n",
      "you specified. You will use this later when you select the specific model \n",
      "that you want to serve. What you will do next is open up the UI MLFlow \n",
      "provides where you can actually look at all the experiments and model \n",
      "runs. Finally, you also log the model itself as an artifact with MLFlow. \n",
      "MLFlow will modularize this code so that it will work with the code \n",
      "provided by MLFlow to support implementations of a variety of MLOps \n",
      "principles.\n",
      "Figure 4-11.  The output of running the MLFlow experiment. Under \n",
      "an MLFlow run context, you are training the model, outputting the \n",
      "graphs from the evaluation function, and logging all the metrics \n",
      "including the model to this runChapter 4  Introdu CtIon to MLF Low142The following was done on Windows 10 , but it should be the same \n",
      "on MacOS or Linux. First, open command prompt/powershell/terminal. \n",
      "Then, you must go into the directory that contains this notebook file. List \n",
      "the contents of the directory (or view this in file explorer/within Jupyter \n",
      "itself) and you will notice a new directory named mlruns .\n",
      "If you installed all of your packages in Conda, make sure you’ve \n",
      "activated the Conda environment before running this.\n",
      "What you want to do now is to make sure your command prompt, \n",
      "powershell, or terminal is in the same directory that contains mlruns , and \n",
      "type the following:\n",
      "mlflow ui -p 1234\n",
      "The command mlflow ui  hosts the MLFlow UI locally on the default \n",
      "port of 5000. However, the options -p 1234  tell it that you want to host it \n",
      "specifically on the port 1234.\n",
      "If it all goes well, and it can take several seconds, you should see \n",
      "something like Figure  4-12 .\n",
      "Figure 4-12.  Making sure that the current directory contains the \n",
      "folder mlruns and calling the command to start the UI. If successful, \n",
      "it should state “Serving on http:// … :1234. ” We have docker on our \n",
      "system, hence why yours might say localhost instead of kubernetes.\n",
      "docker.internalChapter 4  Introdu CtIon to MLF Low143Now, open a browser and type in http://localhost:1234  or \n",
      "http://127.0.0.1:1234 . Both should take you to the same MLFlow UI. If \n",
      "you used a different port, it should generally look like this:\n",
      "http://localhost:PORT_NUMBER  or http://127.0.0.1:PORT_NUMBER , \n",
      "where you replace PORT_NUMBER  with the one you used. If you did not \n",
      "specify a port parameter, then the default port used by MLFlow is 5000.\n",
      "Regardless, if it works correctly, you should see something like \n",
      "Figure  4-13  once you visit that URL.\n",
      "Notice that there is now an experiment titled scikit_learn_experiment . \n",
      "Click it, and you should see something like Figure  4-14 .\n",
      "Figure 4-13.  Your MLFlow UI should look something like this. To \n",
      "the left are the experiments. Notice that there is an experiment titled \n",
      "Default and one titled scitkit_learn_experiment, which is the one you \n",
      "just createdChapter 4  Introdu CtIon to MLF Low144You should see something like Figure  4-15 .\n",
      "Figure 4-14.  This is what your experiment, scikit_learn_experiment, \n",
      "should look like once you click it. Notice that there is one run here, \n",
      "which is what was just createdYou can see the run that just completed, along with the metrics you \n",
      "logged. Click it so that you can explore it. The run that was just completed \n",
      "should have a green check mark beside the time stamp when it finished if \n",
      "everything went well, which you can see is the case in Figure  4-14 .Chapter 4  Introdu CtIon to MLF Low145\n",
      "Figure 4-16.  The logged artifacts of this run. Notice that the graphs \n",
      "appear to be logged as well as the model itself, which was named \n",
      "log_reg_model when you were logging it in the code\n",
      "Figure 4-15.  This is the run that was just completed. Notice that the \n",
      "metrics you logged show up here\n",
      "You should now see the details of this run much more clearly. Here, \n",
      "you can see all of the parameters and metrics that were logged. Keep \n",
      "scrolling down and you should be able to see all of the logged artifacts. \n",
      "Refer to Figure  4-16 .Chapter 4  Introdu CtIon to MLF Low146Here, you can see the model that has been logged, along with the two \n",
      "graphs that you logged as artifacts. Click the graphs and you should see \n",
      "something like Figure  4-17 .\n",
      "Amazing, right? Everything is extremely organized, and you don’t \n",
      "have to worry about creating multiple folders for everything and staying \n",
      "organized. Simply tell MLFlow what to do and it will log all the information \n",
      "relevant to this run that you need. You can log your deep learning model’s \n",
      "hyperparameters for learning rate, number of epochs, specific optimizer \n",
      "parameters like beta1 and beta2 for the Adam optimizer, and so on.\n",
      "You can even log graphs, as you can see in Figure  4-17 , along with the \n",
      "models themselves. With MLFlow, you can stay highly organized with \n",
      "your experiments even if you don’t necessarily need the deployment \n",
      "capabilities to the cloud services.\n",
      "Let’s now try logging a few more runs. Rerun the cell in Figure  4-11  \n",
      "a couple times to completion and go back to the MLFlow UI. Make sure \n",
      "you have selected the experiment named scikit_learn_experiment . You \n",
      "should see something like Figure  4-18 .\n",
      "Figure 4-17.  Inspecting the graph of the confusion matrix that you \n",
      "saved. Feel free to click the other graph as well, which is of the ROC \n",
      "plotChapter 4  Introdu CtIon to MLF Low147Let’s compare the metrics you’ve logged for these runs. Select at least \n",
      "two runs, and ensure your UI looks somewhat like Figure  4-19 . We selected \n",
      "three runs.\n",
      "Figure 4-19.  This is what your UI should look like after selecting \n",
      "several runs. Make sure to select at least two so that there is something \n",
      "to compare. Also notice that the button named Compare turns solid\n",
      "Figure 4-18.  Revisiting your experiment after logging some runs in. \n",
      "The runs are logged in ascending order by timestamp, so the latest \n",
      "runs are on topChapter 4  Introdu CtIon to MLF Low148After clicking Compare, you should see something like Figure  4-20 .\n",
      "Here, you can directly compare the relevant parameters and metrics \n",
      "between the runs you have chosen. You have the option of viewing a scatter \n",
      "plot, a contour plot, or a parallel coordinates plot. Feel free to play around \n",
      "with the metrics and with the plots. You can even save these plots if you wish.\n",
      "Note that since these runs have the exact same metrics, there will only \n",
      "appear to be one point plotted.\n",
      " Loading a Logged Model\n",
      "Next, let’s briefly look at how you can load the models logged by MLFlow. \n",
      "Go back to the experiment and click a run. Note the run ID at the top and \n",
      "copy it. Then, go back to the notebook, and run the following. Note that \n",
      "there is a placeholder for the run ID:\n",
      "loaded_model =  mlflow.sklearn.load_model  \n",
      "(\"runs:/YOUR_RUNID_HERE/log_reg_model\")\n",
      "Figure 4-20.  The UI after selecting three runs to compare. As you can \n",
      "see, you can look at all of the metrics at once. There is also a graphing \n",
      "tool that lets you compare these values graphically, though you won’t \n",
      "see proper graphs as every value is the same across the runsChapter 4  Introdu CtIon to MLF Low149To better understand what this path is, let’s split it up into three \n",
      "sections: the format ( runs:/) , the run ID ( YOUR_RUNID_HERE ), and the \n",
      "model name that you used when you logged it ( log_reg_model) .\n",
      "In our case, our run ID was 3862eb3bd89b43e8ace610c521d974e6, \n",
      "so our cell looks like Figure  4-21 . Ensure your code looks somewhat like \n",
      "Figure  4-21 , with the only difference being the run ID that you chose since \n",
      "it will be different from ours.\n",
      "This is now the same model that you had when MLFlow logged it in the \n",
      "first place. With this, you can call something like .score()  and see that it’s \n",
      "the same as during training:\n",
      "loaded_model.score(x_test, y_test)\n",
      "This outputs the accuracy as the model is evaluated on the test set. If \n",
      "this truly is the same model, then the accuracy should match what was \n",
      "output earlier during the evaluation portion of the model run.\n",
      "Refer to Figure  4-22  to see the output.\n",
      "As you can see, this value matches the evaluation accuracy from \n",
      "Figure  4-11 .\n",
      "Figure 4-22.  This is the evaluation accuracy of the loaded model \n",
      "after evaluation on the test sets. If you compare this with Figure  4- 11,  \n",
      "you can see that the numbers more or less match, disregarding \n",
      "rounding\n",
      "Figure 4-21.  The code to load a model that we logged using the \n",
      "specific run ID we logged it in along with the model’s name we used \n",
      "when we logged itChapter 4  Introdu CtIon to MLF Low150Now you know how to load a model from a specific MLFlow run.\n",
      "With that, you’ve seen some of the functionality that MLFlow provides \n",
      "and how it can help in keeping your prototyping experiments much \n",
      "more organized. As you will see shortly, this entire pipeline that you just \n",
      "explored is pretty much all you need to recreate the train, test, validate \n",
      "pipeline that you saw earlier. Before you move on to looking at how you \n",
      "can use MLFlow with other frameworks, let’s go over how you can use \n",
      "MLFlow functionality to vastly improve the model validation process.\n",
      " Model Validation (Parameter Tuning) \n",
      "with MLFlow\n",
      " Parameter Tuning – Broad Search\n",
      "Just like in Chapter 2, you will use a script to help with model validation \n",
      "with respect to hyperparameter tuning. The tuning script will largely \n",
      "remain the same, except for a few modifications where MLFlow code has \n",
      "been added in.\n",
      "Run the following code to set the range of anomaly weights and to set \n",
      "the number of folds:\n",
      "anomaly_weights = [1, 5, 10, 15]\n",
      "num_folds = 5\n",
      "kfold = KFold(n_splits=num_folds, shuffle= True,  \n",
      "random_state=2020)\n",
      "The code should look like Figure  4-23 .\n",
      "Figure 4-23.  The code to determine the list of anomaly weights to \n",
      "perform validation over, to determine the number of folds, and to \n",
      "initialize the KFolds generator based on the number of foldsChapter 4  Introdu CtIon to MLF Low151Now, paste the following. This is the first half of the entire function:\n",
      "mlflow.set_experiment(\"sklearn_creditcard_broad_search\")\n",
      "logs = []\n",
      "for f in range(len(anomaly_weights)):\n",
      "    fold = 1\n",
      "    accuracies = []\n",
      "    auc_scores= []\n",
      "    for train, test in kfold.split(x_validate, y_validate):\n",
      "        with mlflow.start_run():\n",
      "            weight = anomaly_weights[f]\n",
      "            mlflow.log_param(\"anomaly_weight\", weight)\n",
      "            class_weights= {\n",
      "                0: 1,\n",
      "                1: weight\n",
      "            }\n",
      "            sk_model = LogisticRegression(random_state= None,\n",
      "                                    max_iter=400,\n",
      "                                    solver='newton-cg',\n",
      "                                     class_weight=class_\n",
      "weights).fit \n",
      "(x_validate[train],  \n",
      "y_validate[train])\n",
      "            for h in range(40): print('-', end=\"\")\n",
      "            print(f\"\\nfold {fold}\\nAnomaly Weight: {weight}\")\n",
      "             train_acc = sk_model.score(x_validate[train],  \n",
      "y_validate[train])\n",
      "            mlflow.log_metric(\"train_acc\", train_acc)\n",
      "             eval_acc = sk_model.score(x_validate[test],  \n",
      "y_validate[test])\n",
      "            preds = sk_model.predict(x_validate[test])\n",
      "            mlflow.log_metric(\"eval_acc\", eval_acc)Chapter 4  Introdu CtIon to MLF Low152Here is some more of the code. Make sure this all aligns with the code \n",
      "from above.\n",
      "            try:\n",
      "                auc_score = roc_auc_score(y_validate[test], preds)\n",
      "            except:\n",
      "                auc_score = -1\n",
      "            mlflow.log_metric(\"auc_score\", auc_score)\n",
      "             print(\"AUC: {}\\neval_acc: {}\".format(auc_score, \n",
      "eval_acc))\n",
      "            accuracies.append(eval_acc)\n",
      "            auc_scores.append(auc_score)\n",
      "             log = [sk_model, x_validate[test],  \n",
      "y_validate[test], preds]\n",
      "            logs.append(log)\n",
      "             mlflow.sklearn.log_model(sk_model,   \n",
      "f\"anom_weight_{weight}_fold_{fold}\")\n",
      "            fold = fold + 1\n",
      "            mlflow.end_run()\n",
      "    print(\"\\nAverages: \")\n",
      "    print(\"Accuracy: \", np.mean(accuracies))\n",
      "    print(\"AUC: \", np.mean(auc_scores))\n",
      "    print(\"Best: \")\n",
      "    print(\"Accuracy: \", np.max(accuracies))\n",
      "    print(\"AUC: \", np.max(auc_scores))\n",
      "First, let’s look at what that giant chunk of code looks like in a cell. \n",
      "Ensure your code and alignment matches Figure  4-24 .Chapter 4  Introdu CtIon to MLF Low153Now, let’s run this script. It should log the parameter for the anomaly \n",
      "weight and all of the metrics that you specified for every fold generated. \n",
      "When the script finishes, go to your MLFlow UI and switch the experiment \n",
      "to sklearn_creditcard_broad_search  to see all the runs you just logged. \n",
      "You should see something like in Figure  4-25 .\n",
      "Figure 4-24.  The entire validation script from Chapter 2 with some \n",
      "MLFlow code additions to log everything during the validation \n",
      "processChapter 4  Introdu CtIon to MLF Low154Let’s try sorting this by the AUC score to find the best parameters for \n",
      "the AUC. In the metrics  column, click auc_score .\n",
      "The action should result in something that looks like Figure  4-26 .\n",
      "Figure 4-25.  The output you should see after the validation \n",
      "experiment has finished. Make sure you select the experiment titled \n",
      "sklearn_creditcard_broad_searchChapter 4  Introdu CtIon to MLF Low155You want to sort the columns in descending order, so click it again to \n",
      "see something that looks like Figure  4-27 .\n",
      "Figure 4-26.  The values are all sorted by auc_score in descending \n",
      "order. We’ve highlighted this column so that you can more easily spot \n",
      "the difference between this figure and Figure  4-25 . As you can see, \n",
      "the AUC scores are in ascending order. You want to see the best AUC \n",
      "scores, so you must sort in descending orderChapter 4  Introdu CtIon to MLF Low156Perhaps you don’t really care about anything but the absolute best \n",
      "scores. Say that you are targeting AUC scores that are at least 0.90. How \n",
      "would you go about filtering everything? Well, the UI provides a search bar \n",
      "that performs a search based on the SQL WHERE clause. So, to filter your \n",
      "output, type the following and click Search:\n",
      "metrics.\"auc_score\" >= 0.90\n",
      "You should see something like Figure  4-28 . If you have copied and \n",
      "pasted the line of code, be sure to delete it and put in the quotation marks \n",
      "again if you encounter any errors about the quotation marks.\n",
      "Figure 4-27.  The values are now sorted by AUC score in descending \n",
      "order. Now you can see the runs that produced the best AUC scores \n",
      "along with the specific anomaly weight it had in that runChapter 4  Introdu CtIon to MLF Low157Notice that we put \"auc_score\"  in quotation marks. This is for cases \n",
      "where the metric name that you’ve logged contains characters like a dash \n",
      "where it might not recognize the name if you typed it out like so:\n",
      "metrics.auc-score\n",
      "The proper convention for a metric logged as \"auc-score\"  would be to \n",
      "filter it like so:\n",
      "metrics.\"auc-score\" >= 0.90\n",
      "Now let’s say that of these scores, you want to see the scores for \n",
      "anomaly weights of 5 only. It doesn’t appear that there are any results with \n",
      "the anomaly weight of 1, so we will start with 5. For that, let’s type and \n",
      "search the following:\n",
      "metrics.\"auc_score\" >= 0.90 AND parameters.anomaly_weight = \"5\"\n",
      "You should see something like Figure  4-29 .\n",
      "Figure 4-28.  The results of filtering all of the AUC scores to be above \n",
      "0.90. As you can see, only a handful of runs produced AUC scores that \n",
      "are at least 0.90Chapter 4  Introdu CtIon to MLF Low158You put the 5 in quotation marks because the parameters seem to be \n",
      "logged as string values, whereas the metrics are logged as floats.\n",
      "From this output, it seems that only two of the five folds with the \n",
      "anomaly weight set to 5 had an AUC score above 0.90. Let’s quickly search \n",
      "over the other parameters and check how many folds had an AUC score \n",
      "above 0.90 as well.\n",
      "For filtering the anomaly weight by 10, refer to Figure  4-30 .\n",
      "So, three of the five folds with the anomaly weight set to 10 had an AUC \n",
      "score above 0.90.\n",
      "Let’s check 15 now. Refer to Figure  4-31 .\n",
      "Figure 4-29.  Filtering the runs to have only runs with the anomaly \n",
      "weight set to 5 and to have an AUC score above 0.90\n",
      "Figure 4-30.  Three runs for an anomaly weight of 10 also met your \n",
      "criteria for minimum AUC scoreChapter 4  Introdu CtIon to MLF Low159You see similar results with 15.\n",
      "What if you tighten the AUC score requirement to be a minimum of \n",
      "0.95? Let’s check the runs for a minimum AUC of 0.95 and with an anomaly \n",
      "weight of 5. Refer to Figure  4-32 .\n",
      "So, it seems that only one fold reached an AUC score above 0.95 when \n",
      "the anomaly weight was 5.\n",
      "What do the results look like for an anomaly weight of 10? Refer to \n",
      "Figure  4-33 .\n",
      "Figure 4-31.  You can see that with an anomaly weight of 15, there \n",
      "seems to be two folds that had an AUC score above 0.95\n",
      "Figure 4-32.  This time, you see that only one of the folds for the runs \n",
      "with anomaly weight set to 5 has an AUC score above 0.95Chapter 4  Introdu CtIon to MLF Low160Let’s check the runs with an anomaly weight of 15. Refer to Figure  4-34 .\n",
      "It seems that for an anomaly weight of 15, only one run has achieved \n",
      "an AUC score above 0.95. It seems that you can’t look at how you can \n",
      "narrow the scope without looking at the rest of the AUC scores.\n",
      "It appears to be the case that the best AUC scores seem to be between 5 \n",
      "and 15.\n",
      "Alright, so what if the higher anomaly weights were more consistent \n",
      "in their AUC scores, and the smaller anomaly weight runs achieving the \n",
      "highest AUC scores were just flukes? To see how each anomaly weight \n",
      "Figure 4-33.  With an anomaly weight of 10, only one run has an \n",
      "AUC score above 0.95\n",
      "Figure 4-34.  With an anomaly weight of 15, only one run has \n",
      "achieved an AUC score above 0.95. From these results, you cannot \n",
      "really infer which weight setting is the best, so you have to narrow the \n",
      "scope of your hyperparameter search. As far as you know, you could \n",
      "have missed the best setting, and it could be somewhere in between 5 \n",
      "and 10 or 10 and 15Chapter 4  Introdu CtIon to MLF Low161setting did, first remove the query statement, and click Search again. Next, \n",
      "make sure that the AUC scores are in descending order. Once you’re done, \n",
      "refer to Figure  4-35  and verify that your output looks similar.\n",
      "Using the following code, let’s filter over all of the values for anomaly \n",
      "weights and check what the AUC scores look like, replacing 1 with 5, 10, \n",
      "and 15.\n",
      "parameters.anomaly_weight = \"1\"\n",
      "Refer to Figure  4-36  to see the results of filtering by an anomaly weight \n",
      "of 1.\n",
      "Figure 4-35.  Ordering the runs by descending AUC scoreChapter 4  Introdu CtIon to MLF Low162None of the scores have gone above 0.9, so you can automatically rule \n",
      "out this anomaly weight setting. If you go back to your script, you can see \n",
      "that the average AUC was around 0.8437.\n",
      "Let’s look at the runs with an anomaly weight of 5. Refer to Figure  4-37 .\n",
      "The scores have improved noticeably. If you go back to the original \n",
      "script’s output, you can see that the average AUC score is now 0.9116.\n",
      "The rest of the anomaly weights all achieved the highest AUC score of \n",
      "around 0.975, so the average AUC is a better metric to help you narrow the \n",
      "range.\n",
      "Figure 4-37.  Looking at the AUC scores of the runs with anomaly \n",
      "weight of 5 in descending order. You can see a noticeable increase in \n",
      "the average AUC score when compared to an anomaly weight of 1\n",
      "Figure 4-36.  Looking at the AUC scores of the runs with an anomaly \n",
      "weight of 1 in descending orderChapter 4  Introdu CtIon to MLF Low163Let’s now look at the runs with an anomaly weight of 10. Refer to \n",
      "Figure  4-38 .\n",
      "These scores seem even better than the ones for an anomaly weight of \n",
      "5. This time, the average AUC score is around 0.9215.\n",
      "Finally, let’s look at the scores for an anomaly weight of 15. Refer to \n",
      "Figure  4-39  to see the results of filtering by an anomaly weight of 15.\n",
      "Figure 4-38.  Looking at the AUC scores of the runs with an anomaly \n",
      "weight of 10 in descending order. These scores seem even better\n",
      "Figure 4-39.  Looking at the AUC scores of the runs with an anomaly \n",
      "weight of 15 in descending order. The scores are very similar, but \n",
      "the average is ever so slightly worse, so the true range seems to be \n",
      "somewhere in between 10 and 15Chapter 4  Introdu CtIon to MLF Low164The scores are very similar to each other, and indeed, the average AUC \n",
      "score is now 0.9212.\n",
      "Based on these results, you can see that there seems to be an increase \n",
      "from 5 to 10, but a slight decrease from 10 to 15. From this data, the \n",
      "ideal range seems to be somewhere in between 10 and 15, but again, the \n",
      "decrease in average AUC from 10 to 15 is essentially negligible. And so, \n",
      "what if it’s potentially beyond 15, and you started out with the wrong range \n",
      "to search over?\n",
      "From the results of this validation experiment, it seems that you \n",
      "haven’t found a definite range of values that you know for sure you can \n",
      "focus on. And so, you must expand your range even more just to see if you \n",
      "can get better results with higher anomaly weights.\n",
      "Looking at the distribution of data and how heavily the normal points \n",
      "outnumber the anomalies, you can use your intuition to help guide your \n",
      "hyperparameter search and expand the range far more.\n",
      "Now that you know this, let’s try expanding the range far more.\n",
      " Parameter Tuning – Guided Search\n",
      "The best overall performances were achieved by anomaly weights 10 and \n",
      "15, but it seems to be on an upward trend the higher up you go with the \n",
      "anomaly weight.\n",
      "Now that you know this, let’s try another validation run with a broader \n",
      "range of anomaly weights to try.\n",
      "Go back to the cell (or copy-paste it into a new cell) in Figure  4-23  and \n",
      "change the anomaly weights so that they look like the following:\n",
      "anomaly_weights = [10, 50, 100, 150, 200]\n",
      "You should see something like Figure  4-40 .Chapter 4  Introdu CtIon to MLF Low165The validation script itself should be the same, so if you simply \n",
      "replaced the anomaly weights in the original cell, don’t run the validation \n",
      "script yet!  Let’s create a new experiment so that you don’t clutter the \n",
      "original tuning experiment with these new runs.\n",
      "Modify the following line in the old validation script so that it goes \n",
      "from\n",
      "mlflow.set_experiment(\" sklearn_creditcard_broad_search\")\n",
      "to\n",
      "mlflow.set_experiment(\"sklearn_creditcard_guided_search\")\n",
      "You should see something like Figure  4-41 .\n",
      "Figure 4-40.  Setting a narrow range of values to search over during \n",
      "the second validation run\n",
      "Figure 4-41.  Setting a new experiment called sklearn_creditcard_\n",
      "guided_search so that the results of this second validation experiment \n",
      "are stored separately\n",
      "Now you can run this code. Once it finishes, go back to the UI, refresh \n",
      "it, and select the new experiment named sklearn_creditcard_guided_\n",
      "search . You should see something like Figure  4-42 .Chapter 4  Introdu CtIon to MLF Low166The whole point of broadening the range of anomaly weights that \n",
      "you are performing the tuning experiment on is to help you understand \n",
      "where the best hyperparameter range may lie. You did not know this \n",
      "initially, so you picked a range that was too small to help you discover the \n",
      "best value. Now that you do know, you have expanded your search range \n",
      "considerably.\n",
      "From the results of this experiment, you can hopefully narrow your \n",
      "range a lot more and repeat the experiment with a massively reduced \n",
      "range and arrive at the correct hyperparameter setting.\n",
      "You will now filter out each of the values by each unique anomaly \n",
      "weight (10, 50, 100, 150, and 200) to get an idea of how the runs with that \n",
      "setting performed.\n",
      "Make sure you’re sorting AUC scores in descending order, type the \n",
      "following query, and search:\n",
      "parameters.anomaly_weight = \"10\"\n",
      "You should see something like Figure 4-43 .\n",
      "Figure 4-42.  The results of the second validation experimentChapter 4  Introdu CtIon to MLF Low167The average AUC score as displayed by the validation script is around \n",
      "0.9215. Of course, this is the same result as from earlier.\n",
      "Let’s see how the scores look for an anomaly weight of 50. Refer to \n",
      "Figure  4-44 .\n",
      "There appears to be a minute difference in the range of AUC scores \n",
      "already. Looking at the script, you can see that the average AUC is around \n",
      "0.9248, so there appears to be a small increase in the AUC score.\n",
      "Let’s keep going and check the results for the anomaly weight of 100. \n",
      "Refer to Figure  4-45 .\n",
      "Figure 4-43.  Filtering the runs by anomaly weight of 10 and setting \n",
      "the AUC score to display in descending order\n",
      "Figure 4-44.  Filtering the runs by an anomaly weight of 50 and \n",
      "setting the AUC score to display in descending order. It seems there’s a \n",
      "slight difference in valuesChapter 4  Introdu CtIon to MLF Low168The average this time appears to be 0.9327. Despite the massive \n",
      "increase in weight, the average AUC score did not go up that high. \n",
      "However, notice that the first result with an AUC score of 0.995 has \n",
      "appeared. The best AUC score up until the anomaly weight of 50 was 0.975, \n",
      "but this anomaly weight setting has broken past that.\n",
      "Let’s keep going and see if it increases with an anomaly weight setting \n",
      "of 150. Refer to Figure  4-46A .\n",
      "Figure 4-45.  Filtering the runs by an anomaly weight of 100 and \n",
      "setting the AUC score to display in descending order\n",
      "Figure 4-46A.  Filtering the runs by an anomaly weight of 150 and \n",
      "setting the AUC score to display in descending orderChapter 4  Introdu CtIon to MLF Low169The AUC scores overall seem to be a bit higher. Indeed, the average \n",
      "AUC score is now 0.9365, so there was an increase. Finally, let’s check the \n",
      "AUC scores for an anomaly weight setting of 200. Refer to Figure  4-46B .\n",
      "The new average AUC now is 0.9396, so this anomaly weight setting \n",
      "seems even better.\n",
      "In fact, you still weren’t able to come to a conclusion about an optimal \n",
      "range, since the AUC scores keep increasing as you set higher anomaly \n",
      "weights.\n",
      "So, from this, you know that the best hyperparameter setting is \n",
      "somewhere above 200. You simply shift the range of the scope to start at \n",
      "200 and search over a slightly different area, and once you have found a \n",
      "good range of values to search over (eventually the AUC scores will start \n",
      "trending down as you increase the anomaly weight), you can narrow the \n",
      "focus and start searching again.\n",
      "After a certain amount of precision with the parameter value, you start \n",
      "to see diminishing returns where the added effort only produces negligible \n",
      "improvements in performance, but you will likely encounter this as you \n",
      "start getting deeper into the decimal values.\n",
      "Figure 4-46B.  Filtering the runs by an anomaly weight of 200 and \n",
      "setting the AUC score to display in descending orderChapter 4  Introdu CtIon to MLF Low170Hopefully now you understand more about how you can integrate \n",
      "MLFlow into the model training, testing, and validation pipeline using \n",
      "scikit-learn. You also looked at how to use the UI for basic comparisons, \n",
      "along with how you might perform hyperparameter tuning more easily \n",
      "using MLFlow.\n",
      "A quick note to make is that if you’d like to perform more complicated \n",
      "searches over multiple metrics or parameters, MLFlow provides \n",
      "functionality through the API to let you do so via SQL searches within the \n",
      "code, letting you order by multiple columns, for example.\n",
      "MLFlow also provides support for logging metrics, parameters, \n",
      "artifacts, and even models for other frameworks in their documentation. \n",
      "We will now take a look at how to integrate MLFlow with TensorFlow 2.0+/\n",
      "Keras, PyTorch, and PySpark.\n",
      " MLFlow and Other Frameworks\n",
      " MLFlow with TensorFlow 2.0 (Keras)\n",
      "MLFlow provides easy integration with TensorFlow 2.0+ (any version of \n",
      "TensorFlow 2.0 and above). To see how, let’s go over a very basic example \n",
      "of a handwritten digit classifier model on the MNIST dataset. We will be \n",
      "using the built-in Keras module to keep things simple for demonstration \n",
      "purposes. MLFlow supports TensorFlow 1.12 at a minimum, so this code \n",
      "should run as long as you have at least TensorFlow 1.12.\n",
      "We will assume a basic level of familiarity with TensorFlow 2, so \n",
      "we won’t go into much depth about what the functions, model layers, \n",
      "optimizers, and loss functions mean.\n",
      "Before we begin, here are the versions of TensorFlow, CUDA, and \n",
      "CuDNN that we used. Keep in mind that we ran this using the GPU version Chapter 4  Introdu CtIon to MLF Low171of TensorFlow (the package is called tensorflow-gpu), although you should \n",
      "be able to run this without a GPU at the cost of it taking longer:\n",
      "• TensorFlow  (GPU version) – 2.3.0\n",
      "• CUDA  – 10.1\n",
      "• CuDNN  – v7.6.5.32 for CUDA 10.1\n",
      "• Sklearn  – 0.22.2.post1\n",
      "• MLFlow –  1.10.0\n",
      " Data Processing\n",
      "Here is the code to import the necessary modules and print out their \n",
      "versions:\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
      "from tensorflow.keras.datasets import mnist\n",
      "import numpy as np\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import sklearn\n",
      "from sklearn.metrics import roc_auc_score\n",
      "import mlflow\n",
      "import mlflow.tensorflow\n",
      "print(\"TensorFlow: {}\".format(tf.__version__))\n",
      "print(\"Scikit-Learn: {}\".format(sklearn.__version__))\n",
      "print(\"Numpy: {}\".format(np.__version__))\n",
      "print(\"MLFlow: {}\".format(mlflow.__version__))\n",
      "print(\"Matplotlib: {}\".format(matplotlib.__version__))Chapter 4  Introdu CtIon to MLF Low172You should see something like Figure  4-47 .\n",
      "Let’s now load the data:\n",
      "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
      "Keras, and by extension TensorFlow, provides the MNIST handwritten \n",
      "digit dataset for you, so all you need to do to load the data is call the \n",
      "function, like in Figure  4-48 .\n",
      "Refer to Figure  4-48  to see the code in a cell.\n",
      "You can even see what one of these images looks like. Run the \n",
      "following:\n",
      "plt.imshow(x_train[0], cmap='gray'), print(\"Class: \", y_train[0])\n",
      "You should see something like Figure  4-49 .\n",
      "Figure 4-47.  Importing the necessary modules and printing their \n",
      "versions\n",
      "Figure 4-48.  Defining x_train, y_train, x_test, and y_testChapter 4  Introdu CtIon to MLF Low173Also notice that you printed out the class label associated with this \n",
      "specific image. The labels are all integers between 0 and 9, each associated \n",
      "with an image that shows a handwritten digit from 0 to 9.\n",
      "Since 2D convolutional layers in TensorFlow/Keras expect four \n",
      "dimensions in the format of (m, h, w, c) where m stands for the number of \n",
      "samples in the dataset, h and w stand for the height and width, respectively, \n",
      "and c stands for the number of channels (three if it’s an RGB color image \n",
      "for example), you need to reshape your data so that it conforms to these \n",
      "specifications. Your images are all black and white, so they technically have \n",
      "a channel of one. And so, you must reshape them like so:\n",
      "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], \n",
      "x_train.shape[2], 1)\n",
      "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1],  \n",
      "x_test.shape[2], 1)\n",
      "y_train = tf.keras.utils.to_categorical(y_train)\n",
      "y_test = tf.keras.utils.to_categorical(y_test)\n",
      "Refer to Figure  4-50  to see that code in a cell.\n",
      "Figure 4-49.  Looking at what one of the data samples looks like \n",
      "using matplotlib. You also printed out the class label associated with \n",
      "this sample, which was 5Chapter 4  Introdu CtIon to MLF Low174You converted the y sets by calling a function called  \n",
      "to_categorical().  This converts each sample from an integer value of \n",
      "say 2 or 4 corresponding to the digit represented by the x samples into a \n",
      "one-hot encoded vector.\n",
      "Samples in this format are now 0 vectors with a num_classes  number \n",
      "of digits. In other words, these vectors all have a length matching the total \n",
      "number of classes. Whatever value the label was is now the index of the \n",
      "value 1. And so, if the label is 1, the value at the index of 1 in this vector will \n",
      "be one, and everything else is a 0.\n",
      "This may be a little confusing, so refer to Figure  4-51  to see what the \n",
      "one-hot encoded label looks like for a digit representing 5.\n",
      "Figure 4-50.  Reshaping the data to include one channel, conforming \n",
      "with the specifications of the convolutional layers. Additionally, the y \n",
      "sets are being converted to one-hot encoded formats\n",
      "Figure 4-51.  The new output of the one-hot encoded label \n",
      "representing a value of 5. Notice that the value at index 5 is now 1\n",
      "As you can see, the index of the 1 is 5, corresponding to the first  \n",
      "x_train  example you looked at earlier, which was the digit 5.\n",
      "Now, let’s print out the shapes:\n",
      "print(\"Shapes\")\n",
      "print(\"x_train: {}\\ny_train: {}\".format(x_train.shape,  \n",
      "y_train.shape))\n",
      "print(\"x_test: {}\\ny_test: {}\".format(x_test.shape,  \n",
      "y_test.shape))\n",
      "You should now see something like Figure  4-52 .Chapter 4  Introdu CtIon to MLF Low175 MLFlow Run – Training and Evaluating\n",
      "Let’s move on to the creation of your model. You will be using the \n",
      "Sequential  method of model creation. The model will be quite simple, \n",
      "consisting of a couple 2D convolutional layers that feed into three dense \n",
      "layers. Run the following:\n",
      "model = Sequential()\n",
      "model.add(Conv2D(filters=16, kernel_size=3, strides=2, \n",
      "padding='same', input_shape=(28, 28, 1), activation=\"relu\"))\n",
      "model.add(Conv2D(filters=8, kernel_size=3, strides=2, \n",
      "padding='same', input_shape=(28, 28, 1), activation=\"relu\"))\n",
      "model.add(Flatten())\n",
      "model.add(Dense(30, activation=\"relu\"))\n",
      "model.add(Dense(20, activation=\"relu\"))\n",
      "model.add(Dense(10, activation=\"softmax\"))\n",
      "model.summary()\n",
      "You should see something like Figure  4-53 .\n",
      "Figure 4-52.  Printing the output shapes of the processed dataChapter 4  Introdu CtIon to MLF Low176Let’s now compile your model using the Adam optimizer and \n",
      "categorical cross-entropy for your loss. For your metric, you will only be \n",
      "using accuracy. Run the following:\n",
      "model.compile(optimizer=\"Adam\",  \n",
      "loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
      "You should see something like Figure  4-54 .\n",
      "Figure 4-53.  Creating the model and outputting a summary of the \n",
      "model’s architecture\n",
      "Figure 4-54.  Compiling your model, setting the optimizer to Adam \n",
      "optimizer, setting the loss to categorical cross-entropy, and setting the \n",
      "metric to be accuracy\n",
      "Now you get to the part where you tell MLFlow to log this run. You \n",
      "want all of the metrics to be logged to the same run, so you must tell \n",
      "MLFlow specifically to run a block of code in the context of the same run. \n",
      "To do so, you once again block your code using the following line:\n",
      "with mlflow.start_run():Chapter 4  Introdu CtIon to MLF Low177With that, run the following to set the experiment name, train the \n",
      "model, get the evaluation metrics you need, and log them all to MLFlow:\n",
      "mlflow.set_experiment(\"TF_Keras_MNIST\")\n",
      "with mlflow.start_run():\n",
      "    mlflow.tensorflow.autolog()\n",
      "    model.fit(x=x_train, y=y_train, batch_size=256, epochs=10)\n",
      "    preds = model.predict(x_test)\n",
      "    preds = np.round(preds)\n",
      "    eval_acc = model.evaluate(x_test, y_test)[1]\n",
      "    auc_score = roc_auc_score(y_test, preds)\n",
      "    print(\"eval_acc: \", eval_acc)\n",
      "    print(\"auc_score: \", auc_score)\n",
      "    mlflow.tensorflow.mlflow.log_metric(\"eval_acc\", eval_acc)\n",
      "    mlflow.tensorflow.mlflow.log_metric(\"auc_score\", auc_score)\n",
      "mlflow.end_run()\n",
      "Refer to Figure  4-55  to see the output. Ignore the warning messages. \n",
      "They don’t hinder the training process or the performance of the model.Chapter 4  Introdu CtIon to MLF Low178Another new line of code is the following:\n",
      "mlflow.keras.autolog()\n",
      "This basically tells MLFlow to log all the parameters and metrics \n",
      "associated with the particular TensorFlow/Keras model. As you will see \n",
      "shortly, MLFlow will log the hyperparameters, model metrics listed in \n",
      "the compile()  function, and even the model itself once the training has \n",
      "finished.\n",
      "Figure 4-55.  Output of the MLFlow run and the training process. You \n",
      "can also see that the metrics you calculated have been updatedChapter 4  Introdu CtIon to MLF Low179 MLFlow UI – Checking Your Run\n",
      "With that, let’s now open the MLFlow UI and check your run in MLFlow. \n",
      "Make sure your terminal or command prompt is in the same directory \n",
      "where the mlruns are stored. Usually, MLFlow saves all these runs in the \n",
      "same directory of the Jupyter notebook.\n",
      "Now that you’ve opened the UI, you should see something like \n",
      "Figure  4-56 .\n",
      "Click the tab called TF_Keras_MNIST to see the results of the \n",
      "experiment you just logged. You should see something like Figure  4-57 .\n",
      "Figure 4-56.  The MLFlow UI after running the TensorFlow \n",
      "experiment. Notice that there is a new experiment titled TF_Keras_\n",
      "MNISTChapter 4  Introdu CtIon to MLF Low180As you can see, your run was just successfully logged. Next, click it to \n",
      "explore all of the parameters, metrics, and artifacts that MLFlow logged.\n",
      "You should see something like Figure  4-58 .\n",
      "Figure 4-57.  Opening the experiment titled TF_Keras_MNIST. You \n",
      "can see that it successfully logged a run\n",
      "Figure 4-58.  Looking at the specific run logged in the experiment. \n",
      "As you can see, all the parameters and metrics were logged, even the \n",
      "one you specified. It also shows you the duration and the status of the \n",
      "run, so now you know how long it took to train the model as well as \n",
      "whether or not it completedChapter 4  Introdu CtIon to MLF Low181MLFlow saved all of the hyperparameters used when creating the \n",
      "model. This could be very useful for hyperparameter tuning on a validation \n",
      "set, for example, where you are trying to tune many hyperparameters \n",
      "at once. For example, you can definitely tune batch_size , epochs , or \n",
      "something related to the Adam optimizer like opt_learning_rate ,  \n",
      "opt_beta_1 , or opt_beta_2 .\n",
      "As you can see in Figure  4-58 , MLFlow saved the model metrics for \n",
      "accuracy and loss as calculated during the training process. In addition, \n",
      "MLFlow also saved the metrics that you defined.\n",
      "Scroll down to artifacts and click model and then data. You should see \n",
      "something like Figure  4-59 .\n",
      "Here, you can see that MLFlow also saved the model after the training \n",
      "process finished. In fact, let’s briefly look at how you can load this model. \n",
      "Make sure you go to the top and copy the run ID before doing this.\n",
      " Loading an MLFlow Model\n",
      "With the run ID copied, head on over to the notebook and create a new \n",
      "cell. Run the following code, but replace the run ID with yours:\n",
      "loaded_model =  \n",
      "mlflow.keras.load_model(\"runs:/YOUR_RUN_ID/model\")\n",
      "Figure 4-59.  Upon closer inspection of the artifacts, it seems MLFlow \n",
      "has also logged the model itselfChapter 4  Introdu CtIon to MLF Low182Your code should look similar to Figure  4-60 . Our run was \n",
      "ba423a8f28d24b67b8f703ca6be43fc2 , so that’s what we replaced  \n",
      "YOUR_RUN_ID  with.\n",
      "You’ll notice that we did mlflow.keras  instead of mlflow.tensorflow . \n",
      "This is because this model is technically a Keras model, and so it conforms \n",
      "to the specific load_model()  code in the mlflow.keras  module.\n",
      "Run the following code to quickly calculate the same evaluation \n",
      "metrics that you logged earlier:\n",
      "eval_loss, eval_acc = loaded_model.evaluate(x_test, y_test)\n",
      "preds = loaded_model.predict(x_test)\n",
      "preds = np.round(preds)\n",
      "eval_auc = roc_auc_score(y_test, preds)\n",
      "print(\"Eval Loss:\", eval_loss)\n",
      "print(\"Eval Acc:\", eval_acc)\n",
      "print(\"Eval AUC:\", eval_auc)\n",
      "This just ensures that the model is the same and demonstrates that \n",
      "you can use the model to make predictions. Refer to Figure  4-61  to see the \n",
      "output.\n",
      "Figure 4-60.  Loading a logged model using a specific run. Notice that \n",
      "we are doing mlflow.keras. This is because the model is technically a \n",
      "Keras modelChapter 4  Introdu CtIon to MLF Low183As you can see, this output matches the values from the output of \n",
      "the run earlier. Additionally, this model is also functional and can make \n",
      "predictions.\n",
      "And with that, you now know how to integrate MLFlow into your \n",
      "TensorFlow 2.0+ experiments. Again, MLFlow supports TensorFlow 1.12+, \n",
      "which also contains the Keras submodule. This means that you should be \n",
      "able to follow the same convention to log tf.keras module code as long as \n",
      "you have TensorFlow 1.12+.\n",
      "In practice, you are likely to have functions to build and compile the \n",
      "model, functions to train the model, and functions to evaluate and perhaps \n",
      "even validate the model. Just be sure to call all of them in the block with \n",
      "mlflow.start_run():  so that MLFlow knows all of this is happening \n",
      "within the same run.\n",
      "Next, let’s look at how to integrate MLFlow with PyTorch.\n",
      " MLFlow with PyTorch\n",
      "MLFlow also provides integration with PyTorch. While the process isn’t as \n",
      "easy as with Keras or TensorFlow, integrating MLFlow into your existing \n",
      "PyTorch code is quite simple. To see how to do so, we will be exploring a \n",
      "simple convolutional neural network applied to the MNIST dataset once \n",
      "again.\n",
      "Figure 4-61.  The output of the code block printing out the loss, \n",
      "accuracy, and AUC score when the model was evaluated on the test \n",
      "set. These three values match the corresponding values from the \n",
      "output of the run earlierChapter 4  Introdu CtIon to MLF Low184Before we begin, here are the versions of the modules we are using, \n",
      "including CUDA and CuDNN:\n",
      "• Torch  - 1.6.0\n",
      "• Torchvision  – 0.7.0\n",
      "• CUDA  – 10.1\n",
      "• CuDNN  – v7.6.5.32 for CUDA 10.1\n",
      "• Sklearn  – 0.22.2.post1\n",
      "• MLFlow –  1.10.0\n",
      "• numpy –  1.18.5\n",
      " Data Processing\n",
      "Let’s get started. Here’s the code to import the necessary modules, print \n",
      "out their versions, and set the device that PyTorch will use:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils import data\n",
      "import torchvision\n",
      "import torchvision.datasets\n",
      "import sklearn\n",
      "from sklearn.metrics import roc_auc_score, accuracy_score\n",
      "import numpy as np\n",
      "import mlflow\n",
      "import mlflow.pytorch\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() \n",
      "else \"cpu\")Chapter 4  Introdu CtIon to MLF Low185print(\"PyTorch: {}\".format(torch.__version__))\n",
      "print(\"torchvision: {}\".format(torchvision.__version__))\n",
      "print(\"sklearn: {}\".format(sklearn.__version__))\n",
      "print(\"MLFlow: {}\".format(mlflow.__version__))\n",
      "print(\"Numpy: {}\".format(np.__version__))\n",
      "print(\"Device: \", device)\n",
      "Refer to Figure  4-62  to see the output.\n",
      "The line of code\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() \n",
      "else \"cpu\")\n",
      "tells PyTorch which device to run the code on. If there is a GPU that CUDA \n",
      "can connect to, it will use that instead. Otherwise, it will run everything \n",
      "on the CPU. In our case, we have CUDA set up with our GPU, so Torch \n",
      "displays “cuda:0”  as seen in Figure  4-62 .\n",
      "Figure 4-62.  Importing the necessary modules and printing the \n",
      "versions of the modulesChapter 4  Introdu CtIon to MLF Low186Next, you will define some basic hyperparameters:\n",
      "batch_size = 256\n",
      "num_classes = 10\n",
      "learning_rate = 0.001\n",
      "Refer to Figure  4-63  to see them in a cell.\n",
      "Next, you will load in the MNIST dataset. Like Keras and TensorFlow, \n",
      "PyTorch also provides example datasets. In this case, you are loading \n",
      "MNIST:\n",
      "train_set = torchvision.datasets.MNIST(root='./data', \n",
      "train= True, download= True, transform= None)\n",
      "test_set = torchvision.datasets.MNIST(root='./data', \n",
      "train= False, download= True, transform= None)\n",
      "Refer to Figure  4-64  to see this code in a cell.\n",
      "You will now define your x_train , y_train , x_test , and y_test  \n",
      "datasets:\n",
      "x_train, y_train = train_set.data, train_set.targets\n",
      "x_test, y_test = test_set.data, test_set.targets\n",
      "Refer to Figure  4-65 .\n",
      "Figure 4-63.  Setting the hyperparameters relevant to the training of \n",
      "the model\n",
      "Figure 4-64.  Defining the training and testing sets by loading the \n",
      "data from PyTorchChapter 4  Introdu CtIon to MLF Low187In PyTorch, you want the data to be channels first. In other words, \n",
      "the format of the data should be (m, c, h, w) , where m stands for the \n",
      "number of samples, c stands for the number of channels, h stands for the \n",
      "height of the samples, and w stands for the width of the samples.\n",
      "Notice that this is the “opposite” format of how Keras and TensorFlow \n",
      "do it by default, which is channels last. In Keras and TensorFlow, you can \n",
      "also do channels first, but you must specify that you are doing it this way.\n",
      "Let’s reshape your x-sets:\n",
      "x_train, y_train = train_set.data, train_set.targets\n",
      "x_test, y_test = test_set.data, test_set.targets\n",
      "Refer to Figure  4-66  to see this code in a cell.\n",
      "Figure 4-65.  Creating your x_train, y_train, x_test, and y_test data \n",
      "sets from the training and testing sets\n",
      "Figure 4-66.  Reshaping the x-sets so the data is encoded in a \n",
      "channels-first format\n",
      "Figure 4-67.  The output of the first sample in the y_train set. Note \n",
      "that the numbers are not in a one-hot encoded formatBefore you print out all the shapes, note that your y-sets are not in a \n",
      "one-hot encoded format. Run the following:\n",
      "y_train[0]\n",
      "Refer to Figure  4-67 .Chapter 4  Introdu CtIon to MLF Low188Notice that this outputs a number, not a vector. You must convert \n",
      "your y-sets into a one-hot encoded format. However, there isn’t a handy \n",
      "function like keras.utils.to_categorical()  you can just call, so you will \n",
      "define one:\n",
      "def to_one_hot(num_classes, labels):\n",
      "    one_hot = torch.zeros(([labels.shape[0], num_classes]))\n",
      "    for f in range(len(labels)):\n",
      "        one_hot[f][labels[f]] = 1\n",
      "    return one_hot\n",
      "That being said, you can always call keras.utils.to_categorical() : \n",
      "and type-cast the resulting output to a PyTorch tensor.\n",
      "Refer to Figure  4-68  to see this in a cell.\n",
      "Now let’s convert your y-sets to be in a one-hot encoded format:\n",
      "y_train = to_one_hot(num_classes, y_train)\n",
      "y_test = to_one_hot(num_classes, y_test)\n",
      "Refer to Figure  4-69  to see this code in a cell.\n",
      "Figure 4-68.  A custom function that converts the input called \n",
      "“labels, ” given the number of classes, into a one-hot encoded format \n",
      "and returns it\n",
      "Figure 4-69.  Converting your y-sets into a one-hot encoded format \n",
      "using your custom functionChapter 4  Introdu CtIon to MLF Low189Let’s check what y_train  looks like now:\n",
      "y_train[0]\n",
      "Refer to Figure  4-70 .\n",
      "As you can see, it is now in a one-hot encoded format. Now you can \n",
      "proceed to checking the shapes of your data sets:\n",
      "print(\"Shapes\")\n",
      "print(\"x_train: {}\\ny_train: {}\".format(x_train.shape,  \n",
      "y_train.shape))\n",
      "print(\"x_test: {}\\ny_test: {}\".format(x_test.shape,  \n",
      "y_test.shape))\n",
      "You should see something like Figure  4-71 .\n",
      "Figure 4-70.  Checking the output of the first sample in y_train, you \n",
      "now see that the tensor has been converted into a one-hot encoded \n",
      "format\n",
      "Figure 4-71.  Printing the shapes of your training and testing sets. As \n",
      "you can see, the x-sets are in a channels-first format, and the y-sets are \n",
      "in a one-hot encoded formatChapter 4  Introdu CtIon to MLF Low190 MLFlow Run – Training and Evaluating\n",
      "Now, let’s define your model. A popular convention in PyTorch is to define \n",
      "the model as a class since it allows you to much more easily use the GPU \n",
      "while training. Instead of passing in every layer to the GPU, you can just \n",
      "send in the model object directly.\n",
      "Run the following code to define your model:\n",
      "class model(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(model, self).__init__()\n",
      "        # IN 1x28x28 OUT 16x14x14\n",
      "         self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, \n",
      "kernel_size=3, stride=2, padding=1, dilation=1)\n",
      "        # IN 16x14x14 OUT 32x6x6\n",
      "         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, \n",
      "kernel_size=3, stride=2, padding=0, dilation=1)\n",
      "        # IN 32x6x6 OUT 64x2x2\n",
      "         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, \n",
      "kernel_size=3, stride=2, padding=0, dilation=1)\n",
      "        # IN 64x2x2 OUT 256\n",
      "        self.flat1 = nn.Flatten()\n",
      "         self.dense1 = nn.Linear(in_features=256,  \n",
      "out_features=128)\n",
      "         self.dense2 = nn.Linear(in_features=128,  \n",
      "out_features=64)\n",
      "         self.dense3 = nn.Linear(in_features=64,  \n",
      "out_features=10)\n",
      "    def forward(self, x):\n",
      "        x = self.conv1(x)\n",
      "        x = nn.ReLU()(x)Chapter 4  Introdu CtIon to MLF Low191        x = self.conv2(x)\n",
      "        x = nn.ReLU()(x)\n",
      "        x = self.conv3(x)\n",
      "        x = nn.ReLU()(x)\n",
      "        x = self.flat1(x)\n",
      "        x = self.dense1(x)\n",
      "        x = nn.ReLU()(x)\n",
      "        x = self.dense2(x)\n",
      "        x = nn.ReLU()(x)\n",
      "        x = self.dense3(x)\n",
      "        x = nn.Softmax()(x)\n",
      "        return x\n",
      "Refer to Figure  4-72 .\n",
      "Figure 4-72.  Defining the model’s architecture as a classChapter 4  Introdu CtIon to MLF Low192Next, let’s send your model to the device, define and initialize an \n",
      "instance of Adam optimizer with the learning rate you set earlier, and set \n",
      "your loss function:\n",
      "model = model().to(device)\n",
      "optimizer = torch.optim.Adam(model.parameters(),  \n",
      "lr=learning_rate)\n",
      "criterion = nn.BCELoss()\n",
      "Refer to Figure  4-73 .\n",
      "Next, you will define a data loader using functionality provided by \n",
      "PyTorch to take care of batching your data set:\n",
      "dataset = data.TensorDataset(x_train,y_train)\n",
      "train_loader = data.DataLoader(dataset, batch_size=batch_size)\n",
      "Refer to Figure  4-74 .\n",
      "Figure 4-73.  Sending the model object to the device, defining your \n",
      "optimizer, and initializing the loss function\n",
      "Figure 4-74.  Creating a data loader object out of your data set. With \n",
      "this functionality, PyTorch will batch your data set for you, allowing \n",
      "you to pass in a minibatch at a time in your training loop. This \n",
      "essentially is what the TensorFlow 2/Keras .fit()  function does, but \n",
      "it’s all abstracted for youChapter 4  Introdu CtIon to MLF Low193As you can see, this is much simpler than having to make an intricate \n",
      "loop to batch and pass in data yourself.\n",
      "Finally, let’s define the training loop:\n",
      "num_epochs = 5\n",
      "for f in range(num_epochs):\n",
      "    for batch_num, minibatch in enumerate(train_loader):\n",
      "        minibatch_x, minibatch_y = minibatch[0], minibatch[1]\n",
      "         output = model.forward(torch.Tensor  \n",
      "(minibatch_x.float()).cuda())\n",
      "         loss = criterion(output, torch.Tensor  \n",
      "(minibatch_y.float()).cuda())\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        print(f\"Epoch {f} Batch_Num {batch_num} Loss {loss}\")\n",
      "This can take at least a couple minutes depending on your GPU, and \n",
      "even longer if you’re using a CPU. Feel free to lower the number of epochs \n",
      "if you’d like to decrease total training time.\n",
      "You should see an output like Figure  4-75 .Chapter 4  Introdu CtIon to MLF Low194Now, let’s start an MLFlow run, calculate the metrics you want, and log \n",
      "everything:\n",
      "mlflow.set_experiment(\"PyTorch_MNIST\")\n",
      "with mlflow.start_run():\n",
      "    preds = model.forward(torch.Tensor(x_test.float()).cuda())\n",
      "    preds = np.round(preds.detach().cpu().numpy())\n",
      "    eval_acc = accuracy_score(y_test, preds)\n",
      "    auc_score = roc_auc_score(y_test, preds)\n",
      "    mlflow.log_param(\"batch_size\", batch_size)\n",
      "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
      "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
      "Figure 4-75.  Output of your training loop. Feel free to reduce the \n",
      "number of epochs to save on training time, but this could potentially \n",
      "hinder the model’s performanceChapter 4  Introdu CtIon to MLF Low195    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
      "    mlflow.log_metric(\"auc_score\", auc_score)\n",
      "    print(\"eval_acc: \", eval_acc)\n",
      "    print(\"auc_score: \", auc_score)\n",
      "     mlflow.pytorch.log_model(model, \"PyTorch_MNIST\")\n",
      "mlflow.end_run()\n",
      "As y ou can see, MLFlow integration is still quite easy with PyTorch. \n",
      "Refer to Figure  4-76  to see the output.\n",
      "Figure 4-76.  Setting the experiment, and logging the parameters, \n",
      "metrics, and the model to the MLFlow runChapter 4  Introdu CtIon to MLF Low196 MLFlow UI – Checking Your Run\n",
      "Let’s open up the UI. Refer to Figure 4-77 .\n",
      "As you can see, there is a new experiment titled PyTorch_MNIST. Click \n",
      "it. You should now see the run you just completed. Refer to Figure  4-78 .\n",
      "Now that your run has shown up, click it. You should see all the \n",
      "parameters and metrics logged in that run. Refer to Figure  4-79 .\n",
      "Figure 4-77.  Looking at the MLFlow UI now. Notice that your \n",
      "experiment, PyTorch_MNIST, is created\n",
      "Figure 4-78.  The MLFlow UI showing your completed runChapter 4  Introdu CtIon to MLF Low197Also notice the model that’s been saved by MLFlow under artifacts. \n",
      "Refer to Figure  4-80 .\n",
      "Figure 4-79.  All the p arameters, metrics, and artifacts (the model) \n",
      "you specified have been logged\n",
      "Figure 4-80.  MLFlow has successfully logged the model as wellChapter 4  Introdu CtIon to MLF Low198 Loading an MLFlow Model\n",
      "Let’s now go over how to load this model using MLFlow. Copy the run \n",
      "ID, and head back to the notebook. Run the following, but replace the \n",
      "placeholders with your run ID:\n",
      "loaded_model = mlflow.pytorch.load_model(\"runs:/YOUR_RUN_ID/\n",
      "PyTorch_MNIST\")\n",
      "In our case, our run ID was 094a9f92cd714711926114b4c96f6d73 , so \n",
      "our code looks like Figure  4-81 .\n",
      "Now that’s done, so let’s make predictions and calculate the metrics \n",
      "again:\n",
      "preds = loaded_model.forward(torch.Tensor(x_test.float()).\n",
      "cuda())\n",
      "preds = np.round(preds.detach().cpu().numpy())\n",
      "eval_acc = accuracy_score(y_test, preds)\n",
      "auc_score = roc_auc_score(y_test, preds)\n",
      "print(\"eval_acc: \", eval_acc)\n",
      "print(\"auc_score: \", auc_score)\n",
      "Refer to Figure  4-82  to see the output.\n",
      "Figure 4-81.  Loading the logged MLFlow modelChapter 4  Introdu CtIon to MLF Low199As you can see, these metrics are the same as from the training run. \n",
      "You now know how to load a PyTorch model using MLFlow and how you \n",
      "can use it to make predictions.\n",
      "With that, you now know how to integrate MLFlow into your PyTorch \n",
      "experiments. Next, we will look at how you can integrate MLFlow into \n",
      "PySpark.\n",
      " MLFlow with PySpark\n",
      "In our final example, we will look at how MLFlow integrates with PySpark. \n",
      "Like in the scikit-learn example, we will be looking at the application of a \n",
      "logistic regression model to the credit card dataset. In fact, this code is very \n",
      "similar to the PySpark example from Chapter 2.\n",
      "Before we begin, here are the versions of the modules we are using, \n",
      "including CUDA and CuDNN:\n",
      "• PySpark  – 2.4.5\n",
      "• Matplotlib  – 3.2.1\n",
      "• Sklearn  – 0.22.2.post1\n",
      "• MLFlow  – 1.10.0\n",
      "• mump y – 1.18.5\n",
      "Figure 4-82.  The output of calculating the evaluation metrics from \n",
      "earlier but with the logged model. As you can see, the scores match \n",
      "exactlyChapter 4  Introdu CtIon to MLF Low200 Data Processing\n",
      "With that, let’s get started. First, you must import all the necessary modules \n",
      "and set up some variables for Spark:\n",
      "import pyspark #\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark import SparkConf, SparkContext\n",
      "from pyspark.sql.types import *\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "from pyspark.ml import Pipeline\n",
      "from pyspark.ml.classification import LogisticRegression\n",
      "import pyspark.sql.functions as F\n",
      "import os\n",
      "import seaborn as sns\n",
      "import sklearn #\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_auc_score, accuracy_score\n",
      "import matplotlib #\n",
      "import matplotlib.pyplot as plt\n",
      "import mlflow\n",
      "import mlflow.spark\n",
      "os.environ[\"SPARK_LOCAL_IP\"]='127.0.0.1'\n",
      "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
      "spark.sparkContext._conf.getAll()\n",
      "print(\"pyspark: {}\".format(pyspark.__version__))\n",
      "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
      "print(\"seaborn: {}\".format(sns.__version__))\n",
      "print(\"sklearn: {}\".format(sklearn.__version__))\n",
      "print(\"mlflow: {}\".format(mlflow.__version__))Chapter 4  Introdu CtIon to MLF Low201Refer to Figure  4-83 .\n",
      "Next, let’s load your data set and specify what columns you want to \n",
      "take:\n",
      "data_path = 'data/creditcard.csv'\n",
      "df = spark.read.csv(data_path, header = True,  \n",
      "inferSchema = True)\n",
      "labelColumn = \"Class\"\n",
      "columns = df.columns\n",
      "numericCols = columns\n",
      "numericCols.remove(\"Time\")\n",
      "numericCols.remove(labelColumn)\n",
      "print(numericCols)\n",
      "Figure 4-83.  Importing the necessary modules and printing their \n",
      "versionsChapter 4  Introdu CtIon to MLF Low202Refer to Figure  4-84  to see the output.\n",
      "Notice that you dropped the column Time  here, like with the scikit-  \n",
      "learn example. This column just adds a lot of extraneous information that \n",
      "doesn’t actually correlate very much with the label column and could even \n",
      "possibly make the learning task harder than it needs to be.\n",
      "Let’s see what the data frame looks like:\n",
      "df.toPandas().head()\n",
      "Refer to Figure  4-85  to see the output.\n",
      "Figure 4-84.  Loading the data and specifying the columns that you \n",
      "want as a list\n",
      "Figure 4-85.  Converting the Spark data frame to Pandas and \n",
      "checking the output. As you can see, the columns have loaded in \n",
      "correctly, along with the data. The column Time has not been \n",
      "dropped because you did not filter the data frame yetChapter 4  Introdu CtIon to MLF Low203You’ll notice that the columns you “dropped” are still showing up, like \n",
      "Time . You haven’t filtered the columns you want yet, which you are going \n",
      "to do now. Run the following to select the features you want from the data \n",
      "frame and create your normal and anomaly splits:\n",
      "stages = []\n",
      "assemblerInputs =   numericCols\n",
      "assembler = VectorAssembler(inputCols=assemblerInputs, \n",
      "outputCol=\"features\")\n",
      "stages += [assembler]\n",
      "dfFeatures = df.select(F.col(labelColumn).alias('label'), \n",
      "*numericCols )\n",
      "normal = dfFeatures.filter(\"Class == 0\").\n",
      "sample(withReplacement= False, fraction=0.5, seed=2020)\n",
      "anomaly = dfFeatures.filter(\"Class == 1\")\n",
      "normal_train, normal_test = normal.randomSplit([0.8, 0.2],  \n",
      "seed = 2020)\n",
      "anomaly_train, anomaly_test = anomaly.randomSplit([0.8, 0.2], \n",
      "seed = 2020)\n",
      "Refer to Figure  4-86  to see the code in a cell.\n",
      "Figure 4-86.  Selecting the columns that you want and defining your \n",
      "normal and anomaly train and test setsChapter 4  Introdu CtIon to MLF Low204Let’s look at the new data frame now:\n",
      "dfFeatures.toPandas().head()\n",
      "Refer to Figure  4-87 .\n",
      "Notice that the columns you dropped are gone. Now you know that \n",
      "normal and anomaly don’t have the features you dropped either and that \n",
      "everything is proceeding as planned. Let’s create the train and test sets:\n",
      "train_set = normal_train.union(anomaly_train)\n",
      "test_set = normal_test.union(anomaly_test)\n",
      "Refer to Figure  4-88 .\n",
      "Let’s now move on to creating the feature vector that the logistic \n",
      "regression model is going to use. Run the following to define the pipeline \n",
      "and create your final train and test sets:\n",
      "pipeline = Pipeline(stages = stages)\n",
      "pipelineModel = pipeline.fit(dfFeatures)\n",
      "train_set = pipelineModel.transform(train_set)\n",
      "Figure 4-87.  As y ou can see, Time has been dropped. This is the data \n",
      "frame that your training and testing sets are derived from\n",
      "Figure 4-88.  Concatenating the normal and anomaly sets to create \n",
      "the train and test setsChapter 4  Introdu CtIon to MLF Low205test_set = pipelineModel.transform(test_set)\n",
      "selectedCols = ['label', 'features'] + numericCols\n",
      "train_set = train_set.select(selectedCols)\n",
      "test_set = test_set.select(selectedCols)\n",
      "print(\"Training Dataset Count: \", train_set.count())\n",
      "print(\"Test Dataset Count: \", test_set.count())\n",
      "Refer to Figure  4-89 .\n",
      "Now that you’ve finished processing the data, let’s define a function to \n",
      "train the model and calculate some relevant metrics:\n",
      "def train(spark_model, train_set):\n",
      "    trained_model = spark_model.fit(train_set)\n",
      "    trainingSummary = trained_model.summary\n",
      "    pyspark_auc_score = trainingSummary.areaUnderROC\n",
      "    mlflow.log_metric(\"train_acc\", trainingSummary.accuracy)\n",
      "    mlflow.log_metric(\"train_AUC\", pyspark_auc_score)\n",
      "    print(\"Training Accuracy: \", trainingSummary.accuracy)\n",
      "    print(\"Training AUC:\", pyspark_auc_score)\n",
      "    return trained_model\n",
      "Figure 4-89.  Defining the pipeline used to create the feature vector \n",
      "that will be used to train the model. From the feature vector and the \n",
      "label vector, you define your final train and test setsChapter 4  Introdu CtIon to MLF Low206Refer to Figure  4-90  to see the function in a cell.\n",
      "Let’s now define a function to evaluate the model and calculate those \n",
      "metrics, too:\n",
      "def evaluate(spark_model, test_set):\n",
      "    evaluation_summary = spark_model.evaluate(test_set)\n",
      "    eval_acc = evaluation_summary.accuracy\n",
      "    eval_AUC = evaluation_summary.areaUnderROC\n",
      "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
      "    mlflow.log_metric(\"eval_AUC\", eval_AUC)\n",
      "    print(\"Evaluation Accuracy: \", eval_acc)\n",
      "    print(\"Evaluation AUC: \", eval_AUC)\n",
      "Refer to Figure  4-91 .\n",
      "Figure 4-90.  The code to train the PySpark logistic regression model \n",
      "and log the training accuracy and AUC score metricsChapter 4  Introdu CtIon to MLF Low207 MLFlow Run – Training, UI, and Loading an MLFlow \n",
      "Model\n",
      "Now that you have finished defining the training and evaluation functions \n",
      "along with the metrics you want to log, it’s time to start an MLFlow run and \n",
      "build a model:\n",
      "lr = LogisticRegression(featuresCol = 'features', labelCol = \n",
      "'label', maxIter=10)\n",
      "mlflow.set_experiment(\"PySpark_CreditCard\")\n",
      "with mlflow.start_run():\n",
      "    trainedLR = train(lr, train_set)\n",
      "    evaluate(trainedLR, test_set)\n",
      "     mlflow.spark.log_model(trainedLR,  \n",
      "\"creditcard_model_pyspark\")\n",
      "mlflow.end_run()\n",
      "Figure 4-91.  The code to evaluate the trained PySpark logistic \n",
      "regression model and log the evaluation accuracy and AUC score \n",
      "metricsChapter 4  Introdu CtIon to MLF Low208Refer to Figure  4-92 .\n",
      "Alright, now that MLFlow has finished logging everything and the \n",
      "run has ended, open up the MLFlow UI. You should see something like \n",
      "Figure  4-93 .\n",
      "Notice that a new experiment called PySpark_CreditCard  has been \n",
      "created. Click it, and you should see something like Figure  4-94 . If MLFlow \n",
      "didn’t log the run here, try rerunning the cell. It should log it correctly.\n",
      "Figure 4-92.  The output of the MLFlow run. The experiment has \n",
      "been created and the metrics and model successfully logged\n",
      "Figure 4-93.  The MLFlow UI showing that your experiment, \n",
      "PySpark_CreditCard, has been createdChapter 4  Introdu CtIon to MLF Low209If everything went well, you should see a run logged in this experiment. \n",
      "Click it, and you should see something like Figure  4-95 .\n",
      "Finally, in the artifacts section, click the folder that says  \n",
      "creditcard_model_pyspark  to expand it. You should see a folder called \n",
      "sparkml  that contains the PySpark logistic regression model. Refer to \n",
      "Figure  4-96 .\n",
      "Figure 4-94.  MLFlow UI showing that your run has successfully \n",
      "finished\n",
      "Figure 4-95.  Looking at the run, it appears that all of your metrics \n",
      "have successfully been loggedChapter 4  Introdu CtIon to MLF Low210Now that you’ve verified MLFlow has logged everything you specified, \n",
      "copy the run number at the top. Now go back to the notebook and run the \n",
      "following, replacing the placeholder with your run:\n",
      "model = mlflow.spark.load_model(\"runs:/YOUR_RUN_ID/ \n",
      "creditcard_model_pyspark\")\n",
      "In our case, our run was 58e6aac5d43948c6948bee29c0c04cca , so our \n",
      "cell looks like Figure  4-97 .\n",
      "Now that the model has been loaded, let’s make some predictions with \n",
      "it. Run the following:\n",
      "predictions = model.transform(test_set)\n",
      "y_true = predictions.select(['label']).collect()\n",
      "y_pred = predictions.select(['prediction']).collect()\n",
      "Refer to Figure  4-98  to see the code in a cell.\n",
      "Figure 4-96.  MLFlow has also logged the PySpark model. There is \n",
      "no concrete model file like with the TensorFlow or PyTorch examples \n",
      "because of the way PySpark stores its models\n",
      "Figure 4-97.  Loading the logged MLFlow modelChapter 4  Introdu CtIon to MLF Low211Let’s print out the evaluation accuracy and the AUC score:\n",
      "print(f\"AUC Score: {roc_auc_score(y_true, y_pred):.3%}\")\n",
      "print(f\"Accuracy Score: {accuracy_score(y_true, y_pred):.3%}\")\n",
      "Refer to Figure  4-99 .\n",
      "You will notice that the AUC score differs compared to what was \n",
      "calculated in the evaluation function. This is likely because PySpark \n",
      "calculates the ROC curve slightly differently because it has direct access \n",
      "to the model itself. On the other hand, with scikit-learn, you only have the \n",
      "true labels and the predictions to work with, so the ROC curve is calculated \n",
      "slightly differently.\n",
      "Finally, let’s construct the confusion matrix:\n",
      "conf_matrix = confusion_matrix(y_true, y_pred)\n",
      "ax = sns.heatmap(conf_matrix, annot= True,fmt='g')\n",
      "ax.invert_xaxis()\n",
      "ax.invert_yaxis()\n",
      "plt.ylabel('Actual')\n",
      "plt.xlabel('Predicted')\n",
      "Figure 4-98.  Making predictions with your loaded model\n",
      "Figure 4-99.  Printing out the evaluation metrics. The AUC score \n",
      "noticeably differs, but the accuracy score matches what was displayed \n",
      "during the MLFlow runChapter 4  Introdu CtIon to MLF Low212Refer to Figure  4-100 .\n",
      "From the confusion matrix, you can see that the AUC score as \n",
      "calculated by PySpark must be reflecting its performance on how well \n",
      "it classifies normal data. Looking at the anomalies, a fair chunk of the \n",
      "fraudulent data has been misclassified. Roughly speaking, the model only \n",
      "got two-thirds of the anomalies when evaluated on the test data. Perhaps \n",
      "this explains the disparity between what scikit-Learn says is the AUC score \n",
      "and what PySpark says is the AUC score. Both must have calculated the \n",
      "ROC curves slightly differently with PySpark’s graph somehow favoring the \n",
      "excellent true positive rate of the normal data’s classification.\n",
      "With that, you now know how to integrate MLFlow into your PySpark \n",
      "experiments.\n",
      "Next, we will take a look at how you can deploy your models locally \n",
      "and how you can query the models with samples of data and receive \n",
      "predictions.\n",
      "Figure 4-100.  Displaying the confusion matrix using the true values \n",
      "and the predictions made by the model you loadedChapter 4  Introdu CtIon to MLF Low213 Local Model Serving\n",
      " Deploying the Model\n",
      "Serving and querying models locally is very easy and can be done in the \n",
      "command line. You only need the experiment ID and the run ID to serve \n",
      "the model. This is where the print statement from earlier can apply, as it \n",
      "prints the run ID of that specific run. If you just want to serve the latest \n",
      "model, you may do so using that ID.\n",
      "Otherwise, you can look in the MLFlow UI, select a model run that \n",
      "suits your needs, and paste the run this way.\n",
      "Before you begin, go to the MLFlow UI once again, and click the \n",
      "experiment scikit_learn_experiment . Pick a run and copy the run \n",
      "ID. Don’t forget the model name that you logged the model with either, \n",
      "which should be log_reg_model .\n",
      "You may create a new notebook at this point to keep the code more \n",
      "organized, but be sure to import the following:\n",
      "import pandas as pd\n",
      "import mlflow\n",
      "import mlflow.sklearn\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score, accuracy_score, \n",
      "confusion_matrixChapter 4  Introdu CtIon to MLF Low214import numpy as np\n",
      "import subprocess\n",
      "import json\n",
      "You’ll notice that you are now importing subprocess. If you’re using the \n",
      "same notebook, make sure to import this module as well.\n",
      "Refer to Figure  4-101  to see this code in a cell.\n",
      "Now, open up your command prompt/terminal so that you can begin \n",
      "to serve your local model. First, you need to change your directory to one \n",
      "that contains the mlruns  folder with all your experiments. Next, you need \n",
      "two things: your model run  and your model name .\n",
      "Again, your model run can be anything you pick from the MLFlow UI \n",
      "or it can simply be the latest run. The model name is whatever you set it to \n",
      "when logging the model. In this case, it will be log_reg_model .\n",
      "Once you have that, run the following command in your command \n",
      "prompt/terminal. We have generalized the command, so be sure to replace \n",
      "the fields with your model run and model name, respectively:\n",
      "mlflow models serve --model-uri runs:/YOUR_MODEL_RUN/ \n",
      "YOUR_MODEL_NAME -p 1235\n",
      "Figure 4-101.  Importing the necessary modulesChapter 4  Introdu CtIon to MLF Low215In our case, our model run was 3862eb3bd89b43e8ace610c521d974e6 , \n",
      "and our model name was once again log_reg_model . And so, the \n",
      "command we ran looks like Figure  4-102 .\n",
      "In text, the command looks like this:\n",
      "mlflow models serve --model-uri runs:/3862eb3bd89b43e8ace610c52\n",
      "1d974e6/log_reg_model -p 1235\n",
      "MLFlow should start constructing a new conda environment right \n",
      "away that it will use to serve locally. In this environment, it installs basic \n",
      "packages and specific packages that the model needs to be able to run.\n",
      "After some time, you should see something like in Figure  4-103 .\n",
      "Figure 4-102.  The command that we ran to serve our model locally\n",
      "Figure 4-103.  The result of running the command to deploy \n",
      "the model locally. You might see something different, such as \n",
      "localhost:1235, but this is because we have docker installedChapter 4  Introdu CtIon to MLF Low216MLFlow should create a new conda environment before hosting the \n",
      "model on your local server. The port option -p lets you set a specific port to \n",
      "host the model on. We selected a specific port so that we can have MLFlow \n",
      "UI running at the same time, as both of them default to port 5000. In our \n",
      "case, our MLFlow UI is running on port 1234, so we are serving the model \n",
      "on port 1235.\n",
      " Querying the Model\n",
      "You are now ready to query the model with data and receive predictions. \n",
      "This is where the subprocess module comes in, and you’ll see why shortly. \n",
      "First, let’s load up your data frame again. Run the following code:\n",
      "df = pd.read_csv(\"data/creditcard.csv\")\n",
      "You should see something like Figure  4-104 .\n",
      "Next, select 80 values from your data frame to query your model with. \n",
      "Run the following code:\n",
      "input_json = df.iloc[:80].drop([\"Time\", \"Class\"],  \n",
      "axis=1).to_json(orient=\"split\")\n",
      "You should see something like Figure  4-105 .\n",
      "Figure 4-105.  Converting a selection of 80 rows, dropping the Time \n",
      "and Class columns since they were dropped in the original x_train \n",
      "used to train the model, to a JSON with a split orient\n",
      "Figure 4-104.  Loading the credit card datasetChapter 4  Introdu CtIon to MLF Low217The next step is important because of how you preprocessed the data \n",
      "before training your model originally. To show why it’s so important, we \n",
      "will quickly demonstrate the difference in evaluation metrics from passing \n",
      "in non-scaled data and scaled data. First of all, here is the code to send \n",
      "data to the model and receive predictions back:\n",
      "proc = subprocess.run([\"curl\",   \"-X\", \"POST\", \"-H\",  \n",
      "\"Content-  Type:application/json; format=pandas-split\",  \n",
      "\"--data\", input_json, \"http://127.0.0.1:1235/invocations\"], \n",
      "stdout=subprocess.PIPE, encoding='utf-8')\n",
      "output = proc.stdout\n",
      "df2 = pd.DataFrame([json.loads(output)])\n",
      "df2\n",
      "Essentially, what this does is run the following command within \n",
      "Python itself:\n",
      "curl -X POST -H \"Content-Type:application/json;  \n",
      "format=pandas-  split\" –data \"CONTENT_OF_INPUT_JSON\"    \n",
      "\"http://127.0.0.1:1235/invocations\"\n",
      "The core of the problem is that if you are running this in command \n",
      "line, pasting the JSON format data of the data frame can get very messy \n",
      "because there’s so many columns. That is why we chose to use subprocess \n",
      "as it is easier to directly pass in the JSON itself using a variable name, \n",
      "input_json  in this case, to hold the contents of the JSON.\n",
      "You should see something like Figure  4-106 .\n",
      "Now, you will query the model with input data that is not scaled.\n",
      "Figure 4-106.  Sending data to the locally hosted model and receiving \n",
      "predictions from the modelChapter 4  Introdu CtIon to MLF Low218 Querying Without Scaling\n",
      "You will keep the selection of 80 values from earlier and query the model. \n",
      "The model accepts data in the JSON format, so you will have to convert \n",
      "the format of your data before sending it to the model. Run the cell in \n",
      "Figure  4- 106.\n",
      "You should see something like Figure  4-107 .\n",
      "The resulting data frame is what you get by converting the predictions \n",
      "that you got back from the model into a data frame. Since you have the true \n",
      "predictions, let’s calculate an AUC score and an accuracy score to see how \n",
      "the model did. Run the following code:\n",
      "y_true = df.iloc[:80].Class\n",
      "df2 = df2.T\n",
      "eval_acc = accuracy_score(y_true, df2)\n",
      "y_true.iloc[-1] = 1\n",
      "eval_auc = roc_auc_score(y_true, df2)\n",
      "print(\"Eval Acc\", eval_acc)\n",
      "print(\"Eval AUC\", eval_auc)\n",
      "Figure 4-107.  The list of predictions that you get after querying the \n",
      "model with input_json. Notice that it’s predicting a lot of anomalies. \n",
      "This is the first red flag that indicates something’s wrongChapter 4  Introdu CtIon to MLF Low219First of all, you had to transpose df2 using .T so that you can get the \n",
      "predictions to be in a Pandas Series format. Next, the AUC score cannot \n",
      "be calculated if one of the arrays y_true  or y_preds  only have one class. \n",
      "In this case, y_true  is only comprised of normal values, so you had to \n",
      "manipulate the last value and make it 1 when it really isn’t just to get an \n",
      "AUC score. Of course, the resulting AUC score will be nonsense.\n",
      "You should see something like Figure  4-108 .\n",
      "As you can see, the accuracy score is horrible. This basically means \n",
      "that the model doesn’t know the difference between the anomalies and the \n",
      "normal points but seems to have some idea about normal points.\n",
      "The reason the model did so poorly despite doing so well during the \n",
      "training process is that the input data has not been scaled. You will see \n",
      "the difference in model performance when you now scale the data before \n",
      "passing it in.\n",
      "Figure 4-108.  Evaluating the accuracy and the AUC score from the \n",
      "predictions. The AUC score is nonsense, but the accuracy score reveals \n",
      "that the model has performed very poorlyChapter 4  Introdu CtIon to MLF Low220 Querying with Scaling\n",
      "You will take the same split of data except you will now scale it before \n",
      "passing it in. Run the following code to recreate the data that you used to fit \n",
      "the scaler when training the model originally:\n",
      "normal = df[df.Class == 0].sample(frac=0.5, random_state=2020).\n",
      "reset_index(drop= True)\n",
      "anomaly = df[df.Class == 1]\n",
      "normal_train, normal_test = train_test_split(normal,  \n",
      "test_size = 0.2, random_state = 2020)\n",
      "anomaly_train, anomaly_test = train_test_split \n",
      "(anomaly, test_size = 0.2, random_state = 2020)\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(pd.concat((normal, anomaly)).drop([\"Time\", \"Class\"], \n",
      "axis=1))\n",
      "You should see something like Figure  4-109 .\n",
      "Now that you have fit the scaler, let’s transform your data selection:\n",
      "scaled_selection = scaler.transform(df.iloc[:80].drop \n",
      "([\"Time\", \"Class\"], axis=1))\n",
      "input_json = pd.DataFrame \n",
      "(scaled_selection).to_json(orient=\"split\")\n",
      "Figure 4-109.  Recreating the original dataset that you used to fit the \n",
      "standard scaler when processing the data originally. Using this, you \n",
      "will transform your new sample of data and pass it into the modelChapter 4  Introdu CtIon to MLF Low221Refer to Figure  4-110 .\n",
      "Now run the following:\n",
      "proc =  subprocess.run([\"curl\",   \"-X\", \"POST\", \"-H\",  \n",
      "\"Content-  Type:application/json; format=pandas-split\",\n",
      "        \"--data\", input_json, \"http://127.0.0.1:1235/invocations\"],\n",
      "       stdout=subprocess.PIPE, encoding='utf-8')\n",
      "output = proc.stdout\n",
      "preds = pd.DataFrame([json.loads(output)])\n",
      "preds\n",
      "You should see something like Figure  4-111 .\n",
      "One thing to note is that you are scaling it on the combination of all \n",
      "normal data and all anomaly data, as you did when you were creating \n",
      "the train, test, and validation splits. Since the model was trained on data \n",
      "that was scaled on the partition of data you used in the training process \n",
      "Figure 4-110.  Scaling the selection of 80 values from the original \n",
      "data frame and converting it into a JSON format to be sent to the \n",
      "model\n",
      "Figure 4-111.  Querying the model with the scaled values. From a \n",
      "first glance, the predictions appear to be correct this time aroundChapter 4  Introdu CtIon to MLF Low222(the training, testing, and validation data together), passing in data scaled \n",
      "differently won’t result in the correct predictions. When you scale the new \n",
      "data, it must be scaled after fitting it on the training set.\n",
      "One problem that may eventually arise is that new data might have \n",
      "a different distribution than the original training data. This could lead to \n",
      "performance issues with the model, but really that’s a sign that you need to \n",
      "train your model to update it on the new data.\n",
      "Let’s check how your model did now:\n",
      "y_true = df.iloc[:80].Class\n",
      "preds = preds.T\n",
      "eval_acc = accuracy_score(y_true, preds)\n",
      "y_true.iloc[-1] = 1\n",
      "eval_auc = roc_auc_score(y_true, preds)\n",
      "print(\"Eval Acc\", eval_acc)\n",
      "print(\"Eval AUC\", eval_auc)\n",
      "Refer to Figure  4-112 .\n",
      "As you can see, the accuracy score is noticeably higher, and the \n",
      "model’s performance is reminiscent of when it was trained and evaluated. \n",
      "Unfortunately, the AUC score isn’t a very accurate reflection of the model’s \n",
      "Figure 4-112.  Checking the accuracy and the AUC scores of the \n",
      "predictions. The accuracy score is far better, but you will need more \n",
      "prediction data with both normal and anomaly values to be able to \n",
      "get AUC scoresChapter 4  Introdu CtIon to MLF Low223performance since the samples you are querying the model with only have \n",
      "normal data.\n",
      "Let’s see how the model performs when you query it with a larger \n",
      "sample of data.\n",
      " Batch Querying\n",
      "Unfortunately, there is a limit to how many data samples you can ask \n",
      "the model to make predictions on. The number 80 is really close to the \n",
      "maximum number of samples you can send at one time. So how do you get \n",
      "around this issue and make predictions on more than just 80 samples? For \n",
      "one, you can try batching the samples and making predictions one batch at \n",
      "a time.\n",
      "Run the following code:\n",
      "test = df.iloc[:8000]\n",
      "true = test.Class\n",
      "test = scaler.transform(test.drop([\"Time\", \"Class\"], axis=1))\n",
      "preds = []\n",
      "batch_size = 80\n",
      "for f in range(100):\n",
      "     sample = pd.DataFrame(test[f*batch_size:(f+1)*batch_size]).\n",
      "to_json(orient=\"split\")\n",
      "    proc = subprocess.run([\"curl\",   \"-X\", \"POST\", \"-H\",\n",
      "                           \"Content-Type:application/json; \n",
      "format=pandas-split\", \"--data\",\n",
      "                           sample, \"http://127.0.0.1:1235/\n",
      "invocations\"],\n",
      "                           stdout=subprocess.PIPE, \n",
      "encoding='utf-8')Chapter 4  Introdu CtIon to MLF Low224    output = proc.stdout\n",
      "    resp = pd.DataFrame([json.loads(output)])\n",
      "    preds = np.concatenate((preds, resp.values[0]))\n",
      "eval_acc = accuracy_score(true, preds)\n",
      "eval_auc = roc_auc_score(true, preds)\n",
      "print(\"Eval Acc\", eval_acc)\n",
      "print(\"Eval AUC\", eval_auc)\n",
      "Here, you are selecting the first 8,000 samples from the data frame. \n",
      "Since the batch size is 80, you have 100 batches that you are passing to the \n",
      "model. Of course, you must scale this data as well before passing it in. You \n",
      "will scale it in a manner similar to how you did it earlier: you will fit the \n",
      "scaler on the same normal and anomaly data that you used in the model \n",
      "training pipeline samples to transform the values you want to send to the \n",
      "model. Once finished, you should see something like Figure  4-113 . This \n",
      "might take several seconds to finish, so sit tight!\n",
      "Figure 4-113.  The results of querying the model with the first 8,000 \n",
      "samples in the data frame. Notice that the AUC score is far better \n",
      "samplesChapter 4  Introdu CtIon to MLF Low225This time, you don’t have to worry about only having one class in \n",
      "the entire data. This is because there are examples of anomalies in this \n",
      "selection of 8,000 data points, so the true labels and predictions should \n",
      "contain samples of both classes.\n",
      "You can see that the model performs quite well on this data, which \n",
      "includes data that the model has never seen before. Although you did \n",
      "end up using all of the anomalies when training the data, the model still \n",
      "performs well on the normal data, as evidenced by the relatively high AUC \n",
      "score.\n",
      "In fact, let’s plot a confusion matrix to see how the model did and \n",
      "what’s bringing down the AUC score. Run the following code:\n",
      "conf_matrix = confusion_matrix(true, preds)\n",
      "ax = sns.heatmap(conf_matrix, annot= True,fmt='g')\n",
      "ax.invert_xaxis()\n",
      "ax.invert_yaxis()\n",
      "plt.ylabel('Actual')\n",
      "plt.xlabel('Predicted')\n",
      "plt.title(\"Confusion Matrix\")\n",
      "Refer to Figure  4-114  to see the output.Chapter 4  Introdu CtIon to MLF Low226\n",
      "Figure 4-114.  The confusion matrix for the predictions and true \n",
      "values. The model performed excellently and was able to classify every \n",
      "normal point correctly and a majority of the anomaly points correctly \n",
      "samples\n",
      "As you can see, the confusion matrix shows that the model has \n",
      "performed very well on this data. Not only did it classify the normal points \n",
      "perfectly, but it even classified most of the anomaly points correctly as \n",
      "well.\n",
      "With that, you hopefully know more about the process of deploying \n",
      "and querying a model. When you deploy to a cloud platform, the querying \n",
      "process follows a similar path where you must deploy a model on the cloud \n",
      "platform and query it by sending in the data in a JSON format.\n",
      " Summary\n",
      "MLFlow is an API that can help you integrate MLOps principles into your \n",
      "existing code base, supporting a wide variety of popular frameworks. \n",
      "In this chapter, we covered how you can use MLFlow to log metrics, \n",
      "parameters, graphs, and the models themselves. Additionally, you learned \n",
      "how to load the logged model and make use of its functionality. As for \n",
      "frameworks, we covered how you can apply MLFlow to your experiments Chapter 4  Introdu CtIon to MLF Low227in scikit-learn, TensorFlow 2.0/Keras, PyTorch, and PySpark, and we also \n",
      "looked at how you can take one of these models, deploy it locally, and \n",
      "make predictions with your model.\n",
      "In the next chapter, we will look at how you can take your MLFlow \n",
      "models and use MLFlow functionality to help deploy them to Amazon \n",
      "SageMaker. Furthermore, we will also look at how you can make \n",
      "predictions using your deployed model.Chapter 4  Introdu CtIon to MLF Low229© Sridhar Alla, Suman Kalyan Adari 2021 \n",
      "S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \n",
      "https://doi.org/10.1007/978-1-4842-6549-9_5CHAPTER 5\n",
      "Deploying in AWS\n",
      "In this chapter, we will cover how you can operationalize your MLFlow \n",
      "models using AWS SageMaker. We will cover how you can upload your \n",
      "runs to S3 storage, how you can build and push an MLFlow Docker \n",
      "container image to AWS, and how you can deploy your model, query it, \n",
      "update the model once it is deployed, and remove a deployed model.\n",
      " Introduction\n",
      "In the previous chapter, you learned what MLFlow is and how you can \n",
      "utilize the functionality it provides to integrate MLOps principles into \n",
      "your code. You also looked at how to deploy a model to a local server and \n",
      "perform model inference. However, now it’s time to move to the next stage \n",
      "and explore how you can deploy your machine learning models to a cloud \n",
      "platform so that multiple entities can use its prediction services.\n",
      "Before you begin, here are some important prerequisites :\n",
      "• You must have the AWS command line interface (CLI) \n",
      "installed and have your credentials configured.\n",
      " – Once your credentials are verified, the AWS CLI lets \n",
      "you connect to your AWS workspace. From here, \n",
      "you can create new buckets, check your SageMaker \n",
      "endpoints, and so on all through the command \n",
      "line.230• You must have an Identity and Access Management \n",
      "(IAM) execution role defined that grants SageMaker \n",
      "access to your S3 buckets. Refer to Figure  5-8 to see \n",
      "more on this.\n",
      "• You must have Docker installed and working properly. \n",
      "Verify that you can build Docker images.\n",
      " – It is essential to have Docker working on your \n",
      "system because without it, MLFlow cannot build \n",
      "the Docker container image to push to the AWS \n",
      "ECR.\n",
      "We also recommend that you learn about AWS in general and how it \n",
      "works. Having background knowledge of AWS and how it works can help \n",
      "you understand this chapter and allow you to fix any issues much more \n",
      "easily.\n",
      "In detail, we will go over the following in this chapter:\n",
      "• Config uring AWS:  Here, you set up a bucket and push \n",
      "your mlruns  folders here to be stored on the cloud. \n",
      "These folders contain information about all of the runs \n",
      "associated with the experiments along with the logged \n",
      "models themselves. Next, you build a special Docker \n",
      "container as defined by MLFlow and push that to AWS \n",
      "ECR. SageMaker uses this container image to serve the \n",
      "MLFlow model.\n",
      "• Deploying a model to AWS SageMaker:  Here, you \n",
      "use the built-in MLFlow SageMaker module code to \n",
      "push a model to SageMaker. After SageMaker creates \n",
      "an endpoint, the model is hosted on here utilizing the \n",
      "docker image that you pushed earlier to the ECR.Chapter 5  Deploying in  aWS231• Making predictions:  Once the model has finished \n",
      "deployment and is ready to serve, you use Boto3 to \n",
      "query the model and receive predictions.\n",
      "• Switching models:  MLFlow provides functionality that \n",
      "enables you to switch out a deployed model with a new \n",
      "one. SageMaker essentially updates the endpoint with \n",
      "the new model you are trying to deploy.\n",
      "• Removing the deployed model:  Finally, MLFlow lets \n",
      "you remove your deployed model altogether and delete \n",
      "the endpoint. This is important to do so that you don’t \n",
      "incur the charges of leaving an endpoint running.\n",
      "Also, it is important to note that AWS is actively being worked on, and \n",
      "functionality and operating procedures can change! What that means is \n",
      "that something that works now may not work later on.\n",
      "However, MLFlow specifically provides support for SageMaker, so if \n",
      "something fundamental to how SageMaker runs changes in the future, \n",
      "MLFlow is likely to account for it in the next build.\n",
      "In the absolute worst-case scenario where that doesn’t happen, \n",
      "you can still run an MLFlow server and host it on AWS. You will still be \n",
      "able to deploy models and make inferences with them, and the overall \n",
      "functionality is still preserved. Instead of SageMaker directly hosting the \n",
      "model using an MLFlow container image, you would do something similar \n",
      "to the local model deployment experiment we did in Chapter 4, except \n",
      "you would connect to the server IP and port that the MLFlow server is \n",
      "hosted on.\n",
      "We will explore how to do this with Google Cloud, as MLFlow does not \n",
      "support Google Cloud like it does SageMaker and Azure.\n",
      "With that, let’s get started!Chapter 5  Deploying in  aWS232 Configuring AWS\n",
      "Before you can actually push any model to SageMaker, you need to set up \n",
      "your Amazon workspace. You can push models from your local mlruns  \n",
      "directory, similar to how you did local model deployment, but it is much \n",
      "more convenient and centralized to have all your runs be pushed to AWS \n",
      "and stored in a bucket. This way, all teams can access models that are \n",
      "stored here. In a sense, this can act as your “model registry, ” although it \n",
      "doesn’t offer the same functionality as the model registry provided by \n",
      "MLFlow.\n",
      "What MLFlow allows you to do is take specific runs and determine \n",
      "whether to stage that model to the development branch or to production. \n",
      "In this case, you can have buckets for each team, separated into \n",
      "development or production branches. It’s a couple extra steps on top of \n",
      "MLFlow’s model registry, but it would still allow you to enjoy the benefits \n",
      "of having a model registry.\n",
      "In this case, you will simply be creating one bucket to host all of your \n",
      "MLFlow runs. From here, you will be picking a specific run and deploying \n",
      "to SageMaker. To keep it simple, you will once again use the scikit-learn \n",
      "logistic regression model that you trained as the model you are deploying.\n",
      "So with that, create a simple bucket and name it something like \n",
      "mlflow-sagemaker . You can either create it through the AWS CLI or do so \n",
      "through the AWS console in your browser.\n",
      "We will do the latter so that you can visually see what Amazon is really \n",
      "doing when a bucket is created.\n",
      "Keep in mind that AWS is always working on its UI, so your screen may \n",
      "not look exactly like what is portrayed. That being said, you are still likely \n",
      "able to access S3 bucket storage services, so the core functionality should \n",
      "still be the same, despite the UI changes.\n",
      "When you log into your portal, you should see something like Figure  5- 1.Chapter 5  Deploying in  aWS233As you can see, you can look up services with the search bar. Here, \n",
      "type S3 and click the result that states “S3” with the description “Scalable \n",
      "Storage in the Cloud. ”\n",
      "You should go to a page that looks like Figure  5-2.\n",
      "Figure 5-1.  The home screen of the AWS console. Keep in mind that \n",
      "yours is likely to look different to the one shown here\n",
      "Figure 5-2.  What your screen might look like when you open the S3 \n",
      "bucket services module. We have greyed out the names of the buckets, \n",
      "but you can see string names hereChapter 5  Deploying in  aWS234You should see a button that says Create Bucket. Click it and you will \n",
      "see something like Figure  5-3.\n",
      "We named our bucket mlops-sagemaker-runs . You don’t have to worry \n",
      "about the rest of the options, so scroll down to the bottom and click Create \n",
      "Bucket. Once done, you should be able to see your bucket in the list of \n",
      "buckets.\n",
      "From here, let’s use a subprocess to sync the local mlruns  directory to \n",
      "this bucket. What this does is upload the entire mlruns  directory to your \n",
      "bucket, so that all of your runs are stored on the cloud.\n",
      "Figure 5-3.  This is how your bucket creation screen may look. In \n",
      "this case, you are just naming the bucket and aren’t concerned with \n",
      "anything elseChapter 5  Deploying in  aWS235First, collect the following attributes:\n",
      "• s3_bucket_name : What is the name of the S3 bucket you \n",
      "are trying to push to?\n",
      "• mlruns_directory : What is the location of the mlruns  \n",
      "directory you’re pushing to the bucket?\n",
      "Based on that, run the following. We included the bucket name and \n",
      "mlruns  directory in our case, so just replace them with your respective \n",
      "values.\n",
      "import subprocess\n",
      "s3_bucket_name = \"mlops-sagemaker-runs\"\n",
      "mlruns_direc = \"./mlruns/\"\n",
      "output = subprocess.run([\"aws\",   \"s3\", \"sync\", \"{}\".\n",
      "format(mlruns_direc), \"s3://{}\".format(s3_bucket_name)], \n",
      "stdout=subprocess.PIPE, encoding='utf-8')\n",
      "print(output.stdout)\n",
      "print(\"\\nSaved to bucket: \", s3_bucket_name)\n",
      "After running that code, you should see something similar to Figure  5- 4, \n",
      "letting you know that it has synchronized your local mlruns  directory with \n",
      "the bucket. If you see no output, that means there’s nothing new to push \n",
      "(if you are rerunning it). Ensure that the mlruns  directory is in the same \n",
      "directory as this notebook; otherwise it won’t be able to find it.Chapter 5  Deploying in  aWS236Once this is done, you can proceed to building the container that \n",
      "SageMaker will use to host the model once you get to deployment. To do \n",
      "that, run the following command in your terminal:\n",
      "mlflow sagemaker build-and-push-container\n",
      "Again, this requires you to have your Amazon credentials configured.\n",
      "You do not need to create a new docker image each time you use a new \n",
      "framework. This one image will be able to handle all your MLFlow models \n",
      "thanks to modularization. This is similar to the deployment pipeline we \n",
      "discussed in Chapter 3 from which you simply need to swap models in and \n",
      "out.\n",
      "This step can take some time, so sit back, relax, and let it do its thing. \n",
      "You should see something like Figure  5-5.\n",
      "Figure 5-4.  This is what your output may look like when you are first \n",
      "syncing your mlruns directory with the bucket. Make sure that your \n",
      "mlruns directory is in the same directory as this notebook fileChapter 5  Deploying in  aWS237Once this is finished, the console should output something like \n",
      "Figure  5-6.\n",
      "Now, you should be able to see a new container in the portal when you \n",
      "navigate to Amazon ECR.\n",
      "Figure 5-5.  Something similar to what you should see when you run \n",
      "the command to build the container\n",
      "Figure 5-6.  What you should see when the docker container image \n",
      "has successfully been built and pushed to Amazon ECRChapter 5  Deploying in  aWS238From your home console, navigate to Amazon ECR, and verify you see \n",
      "something called mlflow-pyfunc . You should see something like Figure  5- 7, \n",
      "confirming that the docker image has successfully been pushed to AWS ECR.\n",
      "With that, you have set up everything related to MLFlow functionality \n",
      "that you need in your AWS console in order to deploy your models to \n",
      "SageMaker.\n",
      "Let’s now look at deploying one of the models.\n",
      " Deploying a Model to AWS SageMaker\n",
      "To deploy a model to SageMaker, you need to gather the following \n",
      "information:\n",
      "• app_name\n",
      "• model_uri\n",
      "• execution_role\n",
      "• region\n",
      "• image_ecr_url\n",
      "The execution role refers to the Identity and Access Management \n",
      "(IAM) role, which you can find by searching for “IAM” in the console. Once \n",
      "you have created or selected an execution role (make sure it can access S3 \n",
      "and can perform get, put, delete, and list operations on it), copy the entire \n",
      "value that exists there.\n",
      "Figure 5-7.  After running the command, you should be able to see \n",
      "your container in the ECR repository listChapter 5  Deploying in  aWS239As for the specific policy that this role should follow, refer to Figure  5-8 \n",
      "to see how our IAM execution role is set up.\n",
      "As for the execution role ARN number, you should see something like \n",
      "Figure  5-8.\n",
      "Figure 5-8.  In the IAM tab, under policies, select (or create) the role \n",
      "you are going to use to execute the deployment process. There, you \n",
      "should be able to see the specific Policy ARN value, which you must \n",
      "copy and keep track of\n",
      "Make sure you have the Policy ARN value copied down. AWS lets you \n",
      "copy it to the clipboard if you click the little clipboard symbol next to the \n",
      "policy.\n",
      "To find the image_ecr_url  value, go back to the ECR and look for \n",
      "something like Figure  5-7. Now click it to see something like Figure  5-9.Chapter 5  Deploying in  aWS240Copy the value where it says Image URI, except for the version you \n",
      "want. We are running MLFlow version 1.10.0, so copy the value for that one.\n",
      "Next, find the specific run that you want to deploy. Go to your list of S3 \n",
      "buckets and click the one you created, which should be titled  \n",
      "mlops- sagemaker-  runs .\n",
      "In here, navigate until you see the folder with several runs displayed. \n",
      "We picked the top run. Refer to Figure  5-10 .\n",
      "Figure 5-10.  Look at your bucket to find the run you want to deploy. \n",
      "(These runs all have the same performance metrics, so it does not \n",
      "matter which one we pick. If it did, we could look at it through the \n",
      "MLFlow UI (ensuring the terminal is in the same directory as the \n",
      "same mlruns directory we pushed) and select the best run.) Also, \n",
      "remember to take note of the experiment ID and the name of the \n",
      "model you logged. You should be able to find it if you click the run ID \n",
      "and then artifacts. For our case, it is log_reg_model\n",
      "Figure 5-9.  The Image URI is the value you want to copyChapter 5  Deploying in  aWS241With all that information gathered, let’s proceed to the deployment. \n",
      "Run the following:\n",
      "import boto3\n",
      "import mlflow.sagemaker as mfs\n",
      "import json\n",
      "app_name = \"mlops-sagemaker\"\n",
      "execution_role_arn = \"arn:aws:iam::180072566886:role/ \n",
      "service-  role/AmazonSageMaker-ExecutionRole-20181112T142060\"\n",
      "image_ecr_url = \"180072566886.dkr.ecr.us-east-2.amazonaws.com/\n",
      "mlflow-pyfunc:1.10.0\"\n",
      "region = \"us-east-2\"\n",
      "s3_bucket_name = \"mlops-sagemaker-runs\"\n",
      "experiment_id = \"8\"\n",
      "run_id = \"1eb809b446d949d5a70a1e22e4b4f428\"\n",
      "model_name = \"log_reg_model\"\n",
      "model_uri = \"s3://{}/{}/{}/artifacts/{}/\".format \n",
      "(s3_bucket_name, experiment_id, run_id, model_name)\n",
      "This will set up all of the parameters that you will use to run the \n",
      "deployment code.\n",
      "Finally, let’s get on to the actual deployment code:\n",
      "mfs.deploy(app_name=app_name,\n",
      "           model_uri=model_uri,\n",
      "           execution_role_arn=execution_role_arn,\n",
      "           region_name=region,\n",
      "           image_url=image_ecr_url,\n",
      "           mode=mfs.DEPLOYMENT_MODE_CREATE)\n",
      "You should see something like Figure  5-11 .Chapter 5  Deploying in  aWS242This step can take a while. If you want to check on the status of your \n",
      "SageMaker endpoint, open up the portal and search for and navigate to \n",
      "SageMaker. There should be a section for Endpoints where you can see \n",
      "all of the SageMaker endpoints that exist. You should see your current \n",
      "endpoint with the status of “creating, ” as in Figure  5-12 .\n",
      "Figure 5-11.  You should see something like this when you are \n",
      "attempting to deploy the model. Don’t worry if it takes its timeChapter 5  Deploying in  aWS243Once this endpoint is successfully created, which you will know when \n",
      "you see the status update to “InService, ” you can now move on to making \n",
      "predictions.\n",
      " Making Predictions\n",
      "Making predictions is simple. All you need is the name of the endpoint and \n",
      "the functionality that boto3 provides in order for the model to be queried. \n",
      "Let’s define a function to query the model:\n",
      "def query(input_json):\n",
      "         client = boto3.session.Session().client \n",
      "(\"sagemaker-  runtime\", region)\n",
      "        response = client.invoke_endpoint(\n",
      "            EndpointName=app_name,\n",
      "            Body=input_json,\n",
      "            ContentType='application/json; format=pandas-  split',\n",
      "        )\n",
      "Figure 5-12.  What you should see in the Endpoints section of \n",
      "Amazon SageMaker. Once it has finished creating the endpoint, you \n",
      "should see it update the status to “InService. ”Chapter 5  Deploying in  aWS244        preds = response['Body'].read().decode(\"ascii\")\n",
      "        preds = json.loads(preds)\n",
      "        return preds\n",
      "Now, let’s load your data, process it, and scale it just like you did for the \n",
      "local model deployment example. Make sure that the folder data  exists, \n",
      "ensuring that creditcard.csv  exists within it. Run the following:\n",
      "import pandas as pd\n",
      "import mlflow\n",
      "import mlflow.sklearn\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score, accuracy_score, \n",
      "confusion_matrix\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"data/creditcard.csv\")\n",
      "Once the import  statements and the data frame has been loaded, run \n",
      "the following:\n",
      "normal = df[df.Class == 0].sample(frac=0.5,  random_state=2020).\n",
      "reset_index(drop= True)\n",
      "anomaly = df[df.Class == 1]\n",
      "normal_train, normal_test = train_test_split(normal,  \n",
      "test_size = 0.2, random_state = 2020)\n",
      "anomaly_train, anomaly_test = train_test_split(anomaly,  \n",
      "test_size = 0.2, random_state = 2020)Chapter 5  Deploying in  aWS245scaler = StandardScaler()\n",
      "scaler.fit(pd.concat((normal, anomaly)).drop([\"Time\",  \n",
      "\"Class\"], axis=1))\n",
      "Once this is all finished, run the following to ensure that the model is \n",
      "actually making predictions:\n",
      "scaled_selection = scaler.transform(df.iloc[:80].drop \n",
      "([\"Time\", \"Class\"], axis=1))\n",
      "input_json = pd.DataFrame \n",
      "(scaled_selection).to_json(orient=\"split\")\n",
      "pd.DataFrame(query(input_json)).T\n",
      "You should see an output like Figure  5-13 .\n",
      "Figure 5-13  shows a successful query of the model while it is hosted on \n",
      "a SageMaker endpoint and the predictions received as a response.\n",
      "Let’s run the batch query script with some modifications:\n",
      "test = pd.concat((normal.iloc[:1900], anomaly.iloc[:100]))\n",
      "true = test.Class\n",
      "test = scaler.transform(test.drop([\"Time\", \"Class\"], axis=1))\n",
      "preds = []\n",
      "Figure 5-13.  Querying the deployed model with the scaled data \n",
      "representing the first 80 rows of the data frame and getting a response \n",
      "backChapter 5  Deploying in  aWS246batch_size = 80\n",
      "for f in range(25):\n",
      "    print(f\"Batch {f}\", end=\"  - \")\n",
      "     sample =  pd.DataFrame(test[f*batch_size:(f+1)*batch_size]).\n",
      "to_json(orient=\"split\")\n",
      "    output = query(sample)\n",
      "    resp = pd.DataFrame([output])\n",
      "    preds = np.concatenate((preds, resp.values[0]))\n",
      "    print(\"Completed\")\n",
      "eval_acc = accuracy_score(true, preds)\n",
      "eval_auc = roc_auc_score(true, preds)\n",
      "print(\"Eval Acc\", eval_acc)\n",
      "print(\"Eval AUC\", eval_auc)\n",
      "Once finished, you should see something like Figure  5-14 .Chapter 5  Deploying in  aWS247All this is great, but what do you do when you want to switch the model \n",
      "that is deployed? Well, SageMaker allows you to update the endpoint and \n",
      "switch to a new model. Let’s look at how to do this.\n",
      " Switching Models\n",
      "Perhaps you want to update your model, or you have no more use for the \n",
      "current model and its prediction services so you want to replace it without \n",
      "having to delete and create a new endpoint. In this case, you can simply \n",
      "update the endpoint and swap out the model that is currently hosted on \n",
      "there. To do so, you only need to collect the new model_uri .\n",
      "Figure 5-14.  Output of the batch querying script. You included a mix \n",
      "of 100 anomalies with 1900 normal points so that you can get a better \n",
      "idea of how the model performs against anomalies as well. Otherwise, \n",
      "you would have gotten a handful of anomaliesChapter 5  Deploying in  aWS248This time, the model_uri  refers to the URI of the new model that you \n",
      "want to deploy. In your case, you are selecting the second run of the three \n",
      "runs you uploaded to your bucket. Everything else remains the same, so \n",
      "you only have to get a new model_uri .\n",
      "Now, run the following, replacing the run_id  value with your chosen \n",
      "run_id :\n",
      "new_run_id = \"3862eb3bd89b43e8ace610c521d974e6\"\n",
      "new_model_uri = \"s3://{}/{}/{}/artifacts/{}/\".format \n",
      "(s3_bucket_name, experiment_id, new_run_id, model_name)\n",
      "Now that you have run this, run the following code to update the \n",
      "model:\n",
      "mfs.deploy(app_name=app_name,\n",
      "           model_uri=new_model_uri,\n",
      "           execution_role_arn=execution_role_arn,\n",
      "           region_name=region,\n",
      "           image_url=image_ecr_url,\n",
      "           mode=mfs.DEPLOYMENT_MODE_REPLACE)\n",
      "You will find that this function looks quite similar to the one you \n",
      "used to deploy the model. The only parameter that differs is the mode, \n",
      "as you are now doing mfs.DEPLOYMENT_MODE_REPLACE  instead of mfs.\n",
      "DEPLOYMENT_MODE_CREATE .\n",
      "Refer to Figure  5-15  to see what the output should look like.  \n",
      "Note that this also can take some time to finish.Chapter 5  Deploying in  aWS249While this is running, you can check on the endpoint in your portal to \n",
      "see that it is now updating. Refer to Figure  5-16  to see this.\n",
      "Figure 5-15.  This is what your output should look like after running \n",
      "the update code\n",
      "Figure 5-16.  The endpoint is now updating. Once finished, it should \n",
      "show “InService” just like when the endpoint was being createdChapter 5  Deploying in  aWS250Once it finishes running, you can query this model again using the \n",
      "same function. You don’t have to modify the batch script either.\n",
      "Now that you know how to update the endpoint with a new model, we \n",
      "will look at how you can remove the endpoint and the deployed model.\n",
      " Removing Deployed Model\n",
      "Perhaps you have multiple endpoints each with a different model hosted, \n",
      "and you no longer want to keep an endpoint running because of the cost. \n",
      "To delete an endpoint, you only need the following information:\n",
      "• app_name\n",
      "• region\n",
      "With that information defined, which it already should be, you can \n",
      "simply run the following:\n",
      "mfs.delete(app_name=app_name,region_name=region)\n",
      "You should see it output something like Figure  5-17 . This process \n",
      "finishes quite quickly.\n",
      "You can go check the endpoint in the portal as well, and it should show \n",
      "something like Figure  5-18 .\n",
      "Figure 5-17.  The output of the deletion commandChapter 5  Deploying in  aWS251As you can see, the endpoint is now completely gone.\n",
      "One thing to note is that you should make sure you don’t accidentally \n",
      "leave any resources running because the costs can certainly stack up over \n",
      "time and put a dent in your wallet. For services like SageMaker endpoints, \n",
      "you are charged by the hour, so be sure to delete them once you’re done \n",
      "with them.\n",
      "As for the S3 bucket and the ECR container, those are a one-time \n",
      "charge that only bill for data transfer.\n",
      "With that, you now know how to operationalize your MLFlow model \n",
      "with AWS SageMaker.\n",
      " Summary\n",
      "MLFlow provides explicit AWS SageMaker support in its operationalization \n",
      "code. And so we covered how to upload your runs to an S3 bucket and \n",
      "how to create and push an MLFlow Docker container image for AWS \n",
      "SageMaker to use when operationalizing your models. We also covered \n",
      "Figure 5-18.  SageMaker endpoint resources after the deletion. There \n",
      "should be nothing here if the deletion process went successfullyChapter 5  Deploying in  aWS252how to deploy your model on an endpoint, query it, update the endpoint \n",
      "with a new model, and delete the endpoint. Hopefully now you now know \n",
      "how to operationalize your machine learning models with MLFlow and \n",
      "AWS SageMaker.\n",
      "In the next chapter, we will look at how you can operationalize your \n",
      "MLFlow models with Microsoft Azure.Chapter 5  Deploying in  aWS253© Sridhar Alla, Suman Kalyan Adari 2021 \n",
      "S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \n",
      "https://doi.org/10.1007/978-1-4842-6549-9_6CHAPTER 6\n",
      "Deploying in Azure\n",
      "In this chapter, we will cover how you can use Microsoft Azure to \n",
      "operationalize your MLFlow models. In particular, we will look at how \n",
      "you can also utilize Azure’s built-in functionality to deploy a model to a \n",
      "development branch and to a production branch, along with how you can \n",
      "query the models once deployed.\n",
      " Introduction\n",
      "In the previous chapter, we went over how to deploy your models to \n",
      "Amazon SageMaker, manage them through update or delete events, \n",
      "and query them. Now, we will shift our focus to show how you can \n",
      "operationalize your MLFlow models using Microsoft Azure.\n",
      "Before you begin, here is an important prerequisites :\n",
      "• Install azureml-sdk in your Python environment.\n",
      "Just like with AWS, Microsoft Azure is constantly being worked on and \n",
      "updated. Since MLFlow supports Microsoft Azure, you should be able \n",
      "to utilize MLFlow to operationalize your models. Any new functionality \n",
      "is sure to be documented by MLFlow, and in the absolute worst-case \n",
      "scenario, you should still be able to host a server on Azure and maintain \n",
      "your MLOps functionality that way.\n",
      "Again, we will explore how to do this in the next chapter when we look \n",
      "at how to operationalize your MLFlow models with the Google Cloud API.254In detail, we will go over the following in this chapter:\n",
      "• Configuring Azure:  Here, you basically use MLFlow’s \n",
      "functionality to build a container image for the model \n",
      "to be hosted in. Then, you push it to Azure’s Azure \n",
      "Container Instances (ACI), similar to how you pushed \n",
      "an image to the Amazon AWS Elastic Container \n",
      "Registry (ECR).\n",
      "• Deploying a model to Azure (dev stage):  Here, you \n",
      "use built-in azureml-sdk module code to push a \n",
      "model to Azure. However, this is a development stage \n",
      "deployment, so this model is not production-ready \n",
      "since its computational resources are limited.\n",
      "• Making predictions:  Once the model has finished \n",
      "deployment, it is ready to be queried. This is done \n",
      "through an HTTP request. This is how you can verify \n",
      "that your model works once hosted on the cloud since \n",
      "it’s in the development stage.\n",
      "• Deploying to production:  Here, you utilize MLFlow \n",
      "Azure module code to deploy the model to production \n",
      "by creating a container instance (or any other \n",
      "deployment configuration provided, like Azure \n",
      "Kubernetes Service).\n",
      "• Making predictions:  Similar to how you query the \n",
      "model in the dev stage, you query the model once it \n",
      "has been deployed to the production stage and run the \n",
      "batch query script from the previous chapter.\n",
      "• Switching models:  MLFlow does not provide explicit \n",
      "functionality to switch your models, so you must delete \n",
      "the service and recreate it with another model run.Chapter 6  Deploying in  azure255• Removing the deployed model:  Finally, you undo \n",
      "every deployment that you did and remove all \n",
      "resources. That is, you delete both the development \n",
      "and production branch services as well as the container \n",
      "registries and any additional services created once you \n",
      "are done.\n",
      "With that, let’s get started!\n",
      " Configuring Azure\n",
      "Before you can start using Azure’s functionality to operationalize your \n",
      "models, you must first create or connect to an existing Azure workspace. \n",
      "You can do this either through code or the UI in a browser.\n",
      "In your case, you will open up the portal in the browser and learn how \n",
      "to create a workspace. Refer to Figure  6-1.\n",
      "Figure 6-1.  An example of the Microsoft Azure portal home screenChapter 6  Deploying in  azure256Next, click the Create a resource option and search for “Machine \n",
      "Learning. ” You should see something like Figure  6-2.\n",
      "Click the Create button. You should see something like Figure  6-3. (We \n",
      "filled the fields with our own parameters.)\n",
      "Your subscription might differ from ours. For the resource group, we \n",
      "created a new one titled azure-mlops .\n",
      "The fields you completed in Figure  6-3 are enough to create your \n",
      "workspace. Next, click the Review + create option and click Create once \n",
      "Azure states that the validation procedure has been passed and allows you \n",
      "to click Create.\n",
      "Figure 6-2.  An example of the service “Machine Learning” provided \n",
      "by Azure. You want to create a workspace within this service, so click \n",
      "the Create buttonChapter 6  Deploying in  azure257This will take some time to deploy. Once the workspace has been \n",
      "created, go back to the home portal and click the All resources option. You \n",
      "should see something like Figure  6-4.\n",
      "Click your workspace, which should have an image of a chemical \n",
      "beaker next to it.\n",
      "In this overview, you will see several parameters associated with \n",
      "this workspace. Make sure to keep track of the following attributes of the \n",
      "workspace so that you can connect to it in the code:\n",
      "Figure 6-3.  Workspace creation UI (we filled in the fields with our \n",
      "own parameters)Chapter 6  Deploying in  azure258• workspace_name  (azure-mlops-workspace )\n",
      "• subscription  (The value where it says Subscription-  ID)\n",
      "• resource_group  (azure-mlops)\n",
      "• location  (East-US)\n",
      "Refer to Figure  6-5.\n",
      "Figure 6-5.  You should see something like this for your own \n",
      "workspace. Here we’ve censored potentially sensitive fields, but you \n",
      "should be able to see your own unique subscription ID on your screen. \n",
      "This is the value you want to use\n",
      "Figure 6-4.  You might see something like this when you look at the \n",
      "All resources optionChapter 6  Deploying in  azure259Now that you have that, run the following to create/connect to your \n",
      "own workspace:\n",
      "import azureml\n",
      "from azureml.core import Workspace\n",
      "workspace_name = \"MLOps-Azure\"\n",
      "workspace_location=\"East US\"\n",
      "resource_group = \"mlflow_azure\"\n",
      "subscription_id = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
      "workspace = Workspace.create(name = workspace_name,\n",
      "                             location = workspace_location,\n",
      "                             resource_group = resource_group,\n",
      "                             subscription_id = subscription_id,\n",
      "                             exist_ok= True)\n",
      "If you have successfully connected to your workspace, the cell should \n",
      "run without any issues.\n",
      "Next, you must build the MLFlow container image to be used by Azure. \n",
      "Here, you also specify the run of the model you are trying to deploy.\n",
      "In the case of Amazon SageMaker, you were able to reference runs \n",
      "from your local machine or runs from an S3 bucket. You can do the same \n",
      "thing for Azure, except using Azure’s storage entities called blobs.\n",
      "Either way, you need the run ID  of the model you are deploying and \n",
      "the artifact scheme  that the model is logged in. For the models you stored \n",
      "in Amazon S3 buckets, you used the scheme s3:/ , but this time you will \n",
      "just use a run locally. If you’d like, you can still use your Amazon S3 bucket \n",
      "or Google Cloud buckets. Where you store your run does not matter.\n",
      "Run the following, replacing the values with your specific run and \n",
      "storage scheme:\n",
      "run_id = \"1eb809b446d949d5a70a1e22e4b4f428\"\n",
      "model_name = \"log_reg_model\"\n",
      "model_uri = f\"runs:/{run_id}/{model_name}\"Chapter 6  Deploying in  azure260The model name should be the same in your case unless you changed \n",
      "it. Since we are using local runs, we have a URI starting with runs:/ . Again, \n",
      "change this to whatever is appropriate in your case.\n",
      "Finally, with all that information set, let’s create the container image:\n",
      "import mlflow.azureml\n",
      "model_image, azure_model =  mlflow.azureml.build_image  \n",
      "(model_uri=model_uri, \n",
      "workspace=workspace,\n",
      "                           model_name=\"sklearn_logreg_dev\", \n",
      "                           image_name=\"model\",\n",
      "                            description=\"SkLearn LogReg Model \n",
      "for Anomaly Detection\",\n",
      "                           synchronous= False)\n",
      "You should see something like Figure  6-6. You may or may not see the \n",
      "warning messages depending on your version of MLFlow.\n",
      "Figure 6-6.  Building and pushing the container to Azure’s container \n",
      "registry. Ignore the warning messages for now. You might not see these \n",
      "messages in the future. Since this is code created and maintained by \n",
      "MLFlow, it is likely that they will provide support for whatever new \n",
      "functionality Azure pushesChapter 6  Deploying in  azure261Next, run the following to check the status of the container:\n",
      "model_image.wait_for_creation(show_output= True)\n",
      "You should see something like Figure  6-7.\n",
      "Once the image has been created, you can now deploy your model.\n",
      " Deploying to Azure (Dev Stage)\n",
      "One interesting bit of functionality that Azure provides is the ACI \n",
      "webservice. This webservice is specifically used for the purposes of \n",
      "debugging or testing some model under development, hence why it is \n",
      "suitable for use in the development stage.\n",
      "You are going to deploy an ACI webservice instance based on the \n",
      "model image you just created.\n",
      "Run the following:\n",
      "from azureml.core.webservice import AciWebservice, Webservice\n",
      "aci_service_name = \"sklearn-model-dev\"\n",
      "aci_service_config = AciWebservice.deploy_configuration()\n",
      "aci_service =  Webservice.deploy_from_image  \n",
      "(name=aci_service_name,\n",
      "              image=model_image,\n",
      "               deployment_config=aci_service_config,\n",
      "              workspace=workspace)\n",
      "Figure 6-7.  Checking the output of the progress in the image creation \n",
      "operationChapter 6  Deploying in  azure262You should see something like Figure  6-8.\n",
      "This exact way of starting the service may be deprecated in the near \n",
      "future in favor of Environments. For the time being, you should still be able \n",
      "to start an ACI service in this manner, but the important thing to know \n",
      "is that there is a web service specifically tailored for development stage \n",
      "testing.\n",
      "Now run the following to check the progress:\n",
      "aci_service.wait_for_deployment(show_output= True)\n",
      "You should see something like Figure  6-9.\n",
      "Before making your predictions, let’s first verify that you can reach your \n",
      "service:\n",
      "aci_service.scoring_uri\n",
      "Figure 6-9.  The output you should see from checking if the \n",
      "deployment has succeeded\n",
      "Figure 6-8.  The output of creating the ACI service. It seems that this \n",
      "function may be removed in the future, but for now this is one way to \n",
      "access the ACI service and deploy the modelChapter 6  Deploying in  azure263You should see something like Figure  6-10 . If not, try going into your \n",
      "resources in the portal to verify that a new container exists with the name \n",
      "sklearn-model-dev . If not, try rerunning the cells in the same order. It \n",
      "should display some URI this time.\n",
      "You should see something like Figure  6-10 .\n",
      "You can now make predictions with this model.\n",
      " Making Predictions\n",
      "Now you need to acquire some data to predict with.\n",
      "Just like before, you will be loading the credit card dataset, \n",
      "preprocessing it, and setting aside a small batch that you will query the \n",
      "model with. Run the following blocks of code, and make sure you have the \n",
      "folder named data  in this directory with creditcard.csv  in it:\n",
      "import pandas as pd\n",
      "import mlflow\n",
      "import mlflow.sklearn\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score, accuracy_score, \n",
      "confusion_matrix\n",
      "Figure 6-10.  The scoring URI is displayed, indicating that you can \n",
      "connect to it and make predictionsChapter 6  Deploying in  azure264import numpy as np\n",
      "import subprocess\n",
      "import json\n",
      "df = pd.read_csv(\"data/creditcard.csv\")\n",
      "Once you have loaded all the modules and have loaded the data, run \n",
      "the following:\n",
      "normal = df[df.Class == 0].sample(frac=0.5, random_state=2020).\n",
      "reset_index(drop= True)\n",
      "anomaly = df[df.Class == 1]\n",
      "normal_train, normal_test = train_test_split(normal, test_size \n",
      "= 0.2, random_state = 2020)\n",
      "anomaly_train, anomaly_test = train_test_split(anomaly,  \n",
      "test_size = 0.2, random_state = 2020)\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(pd.concat((normal, anomaly)).drop([\"Time\", \"Class\"], \n",
      "axis=1))\n",
      "In cells, the above two blocks of code should look like Figure  6-11 .Chapter 6  Deploying in  azure265Once you are all done with preparing the data, let’s define a function to \n",
      "help you query the deployed model:\n",
      "import requests\n",
      "import json\n",
      "def query(scoring_uri, inputs):\n",
      "    headers = {\n",
      "    \"Content-Type\": \"application/json\",\n",
      "    }\n",
      "     response = requests.post(scoring_uri, data=inputs, \n",
      "headers=headers)\n",
      "    preds = json.loads(response.text)\n",
      "    return preds\n",
      "Figure 6-11.  The import statements and data processing code. You \n",
      "also define the scaler here and fit it to the data, just as you did when \n",
      "originally training these modelsChapter 6  Deploying in  azure266Now you can select a few points and make a prediction:\n",
      "data_selection = df.iloc[:80].drop([\"Time\", \"Class\"], axis=1)\n",
      "input_json =  pd.DataFrame(scaler.transform(data_selection)).\n",
      "to_json(orient=\"split\")\n",
      "preds = query(scoring_uri=aci_service.scoring_uri, \n",
      "inputs=input_json)\n",
      "pd.DataFrame(preds).T\n",
      "Together, you should see something like Figure  6-12 .\n",
      "As you can see, the model has returned predictions that look correct \n",
      "(thanks to the scaling).\n",
      "Now that you know how to deploy to a development branch, let’s look \n",
      "at how you can deploy the model to production using built-in MLFlow \n",
      "functionality.\n",
      "Figure 6-12.  Querying the model deployed on an ACI webservice \n",
      "with some sample data and receiving a responseChapter 6  Deploying in  azure267 Deploying to Production\n",
      "MLFlow provides Azure support and helps us deploy our models directly, \n",
      "using a container instance by default.\n",
      "Let’s get straight into it. Run the following, replacing the names with \n",
      "anything else preferred:\n",
      "azure_service, azure_model = mlflow.azureml.deploy(model_uri,\n",
      "                      workspace,\n",
      "                      service_name=\"sklearn-logreg\",\n",
      "                      model_name=\"log-reg-model\",\n",
      "                      synchronous= True)\n",
      "It’s worth mentioning that you can deploy to a specific web service. By \n",
      "default, MLFlow will host the model on a container instance, but you can \n",
      "specify a computer cluster. To learn more, refer to the documentation here: \n",
      "www.mlflow.org/docs/latest/python_api/mlflow.azureml.html .\n",
      "Once the code finishes running, which can take some time, you can \n",
      "also check to see if the URI can be printed:\n",
      "azure_service.scoring_uri\n",
      "Together, you should see something like Figure  6-13 .\n",
      "Figure 6-13.  Successfully creating the endpoint and verifying that the \n",
      "service has a URIChapter 6  Deploying in  azure268Now that you have successfully deployed your model, let’s move on to \n",
      "making predictions.\n",
      " Making Predictions\n",
      "Now that you have your model deployed, let’s run your code to make \n",
      "predictions.\n",
      "First of all, let’s run the following to make sure that you are receiving \n",
      "predictions. You should already have defined input_json :\n",
      "preds = query(scoring_uri=azure_service.scoring_uri, \n",
      "inputs=input_json)\n",
      "pd.DataFrame(preds).T\n",
      "You should now see something like Figure  6-14 .\n",
      "Now, let’s run your batch querying script:\n",
      "test = pd.concat((normal.iloc[:1900], anomaly.iloc[:100]))\n",
      "true = test.Class\n",
      "test = scaler.transform(test.drop([\"Time\", \"Class\"], axis=1))\n",
      "preds = []\n",
      "Figure 6-14.  Querying the deployed model with your batch of scaled \n",
      "data to ensure it worksChapter 6  Deploying in  azure269batch_size = 80\n",
      "for f in range(25):\n",
      "    print(f\"Batch {f}\", end=\"  - \")\n",
      "     sample = pd.DataFrame(test[f*batch_size:(f+1)*batch_size]).\n",
      "to_json(orient=\"split\")\n",
      "     output = query(scoring_uri=azure_service.scoring_uri, \n",
      "inputs=sample)\n",
      "    resp = pd.DataFrame([output])\n",
      "    preds = np.concatenate((preds, resp.values[0]))\n",
      "    print(\"Completed\")\n",
      "eval_acc = accuracy_score(true, preds)\n",
      "eval_auc = roc_auc_score(true, preds)\n",
      "print(\"Eval Acc\", eval_acc)\n",
      "print(\"Eval AUC\", eval_auc)\n",
      "Once finished, you should see something like Figure  6-15 .Chapter 6  Deploying in  azure270With that, you now know how to query your deployed model and make \n",
      "predictions with it. This should be the same procedure if you’ve opted to \n",
      "deploy to a specific compute cluster with, for example, Azure Kubernetes \n",
      "Service.\n",
      " Cleaning Up\n",
      "Unfortunately, there does not seem to be any specific functionality to \n",
      "update  the service with a new model. The procedure seems to be to delete \n",
      "the service and create a new service with another model URI.\n",
      "So, with that, let’s now look at how you can remove all the services you \n",
      "just created.\n",
      "Figure 6-15.  The results of running the batch querying script. This \n",
      "effectively made predictions on 2,000 data pointsChapter 6  Deploying in  azure271Run the following:\n",
      "aci_service.delete()\n",
      "azure_service.delete()\n",
      "Refer to Figure  6-16 .\n",
      "Now, navigate to the All resources section again from the home portal. \n",
      "Check every item with the resource group type named Container Instance. \n",
      "You should see that there are none. Figure  6-17  shows what this might \n",
      "look like. (We have a container instance here, but it is unrelated.) Since \n",
      "you deleted the services just now, you should not see sklearn-logreg  or \n",
      "sklearn-model-dev .\n",
      "Figure 6-17.  You should not see any resources titled sklearn-logreg or \n",
      "sklearn-model-dev of type container instance. (There is one here, but \n",
      "it is not related to the experiments from above, and only exists to show \n",
      "what a resource with this resource type looks like.)\n",
      "Figure 6-16.  Deleting the web services you launched earlierChapter 6  Deploying in  azure272If you want to remove services from here, you can simply delete the \n",
      "container instances or other services, as in Figure  6-18 .\n",
      "You can now delete everything else (or just the new resources created \n",
      "for this chapter) in your UI following this same procedure to clean up your \n",
      "Azure workspace.\n",
      "With that, you now know how to use MLFlow to deploy a model on \n",
      "Microsoft Azure.\n",
      "It’s worth mentioning that Azure has a lot of additional functionality \n",
      "relating to monitoring your machine learning experiments and more, \n",
      "but that might also come with additional costs depending on the \n",
      "depth of functionality you are going after. Be sure to refer to their \n",
      "excellent documentation if you’d like to learn more about Azure and its \n",
      "functionality.\n",
      " Summary\n",
      "Like Amazon AWS, Microsoft Azure is a cloud platform that performs many \n",
      "advanced services for a wide range of users. In particular, Azure has a lot \n",
      "of support for operationalizing machine learning models using built-in \n",
      "functionality separate from MLFlow.\n",
      "Figure 6-18.  Deleting services manually through the All resources UIChapter 6  Deploying in  azure273In this chapter, you learned how to build a container image for a \n",
      "specific MLFlow model run, deploy it in a development setting/production \n",
      "setting, and query the model on Microsoft Azure.\n",
      "In the next chapter, we will look at how you can use Google Cloud as \n",
      "a platform to operationalize your MLFlow models. There is no explicit \n",
      "MLFlow support for Google Cloud, so you will be adopting a different \n",
      "approach where you serve the models on a server hosted on Google Cloud \n",
      "and make predictions that way.Chapter 6  Deploying in  azure275© Sridhar Alla, Suman Kalyan Adari 2021 \n",
      "S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \n",
      "https://doi.org/10.1007/978-1-4842-6549-9_7CHAPTER 7\n",
      "Deploying in Google\n",
      "In this chapter, we will cover how you can use MLFlow and Google Cloud \n",
      "to operationalize your models even without MLFlow providing explicit \n",
      "deployment support for Google Cloud.\n",
      "More specifically, we will cover how to set up your Google Cloud \n",
      "bucket and virtual machine (used to run the server) and how you can \n",
      "operationalize and query your models.\n",
      " Introduction\n",
      "In the previous chapter, we went over how you can deploy your models to \n",
      "Microsoft Azure, manage them through update or delete events, and query \n",
      "them. This time, we will explore how you can operationalize your models \n",
      "using Google Cloud.\n",
      "MLFlow does not provide explicit support for deploying in Google \n",
      "Cloud like it does with AWS SageMaker and Microsoft Azure, and so you \n",
      "will approach this a bit differently from how you operationalized models in \n",
      "the previous two chapters.\n",
      "This time, you will use the same model serving functionality that you \n",
      "used in Chapter 4 except you will host it on a Google Cloud machine that \n",
      "is accessible by the Internet. However, deployment is far quicker this way \n",
      "since you don’t have to wait for the creation of an endpoint. Furthermore, \n",
      "once you set up the machine, swapping models is very simple, and you can \n",
      "serve multiple models by using different ports.276It’s worth noting that Google Cloud has an assortment of advanced \n",
      "tools and functionality dedicated to machine learning, such as Kubeflow. \n",
      "Kubeflow is a tool that allows you to essentially integrate your machine \n",
      "learning lifecycles into Kubernetes. And so all your machine learning \n",
      "pipelines are managed through Kubernetes. Kubeflow also integrates into \n",
      "the Google Cloud platform, seeing as how Kubernetes was built by Google. \n",
      "In this chapter, we will just go over how you can deploy MLFlow logged \n",
      "models. We won’t get into any of the platform-specific tools that help \n",
      "manage your machine learning lifecycles.\n",
      "Before you begin, here is an important prerequisite :\n",
      "• Download and install the Google Cloud SDK so you can \n",
      "use the CLI to connect to your server.\n",
      "In detail, we will go over the following in this chapter:\n",
      "• Configuring Google:  This is perhaps the hardest step \n",
      "in this deployment process. First, you set up a bucket \n",
      "and push the contents of your mlruns  folder to be \n",
      "stored on the cloud.\n",
      "Next, you set up the virtual machine that will host \n",
      "your server when you deploy the model. This \n",
      "involves installing Conda and MLFlow.\n",
      "Finally, you set up a firewall to allow your server \n",
      "to have inbound access through the default port \n",
      "of 5000 that MLFlow uses so that you can actually \n",
      "connect to this server through your Jupyter \n",
      "notebook.\n",
      "• Deploying and querying the model:  Here, you check \n",
      "the IP address, pick a run, and launch the code to serve \n",
      "the model. Then, you query the model and run the \n",
      "batch query script as well.Chapter 7  Deploying in  google277• Updating and removing a deployment:  Here, you stop \n",
      "deployment and simply rerun the model serving script \n",
      "with a different model run to fulfill model switching \n",
      "functionality. After you have updated the model, \n",
      "removing the deployment is as easy as stopping the \n",
      "model serving.\n",
      "• Cleaning up:  Here, you go through all of the new \n",
      "services you used and delete them all so as not to incur \n",
      "any charges.\n",
      "With that, let’s get started!\n",
      " Configuring Google\n",
      "Most of the work that is involved in deploying your models using Google \n",
      "Cloud is actually taken up by the configuration process. Once you set up \n",
      "the storage and the machine to host your model, model serving becomes \n",
      "an extremely easy task. To switch up models, you only need to change up \n",
      "the model run and let MLFlow take care of the rest.\n",
      "As for where you are storing the models, you will be using Google \n",
      "Cloud Storage to do so. Once again, this fulfills a functionality similar to \n",
      "storing your runs in Amazon S3 buckets or Azure blobs. The purpose of \n",
      "pushing all of your runs to the cloud is so that there is a centralized storage \n",
      "container that holds the models. Now anyone can access them anywhere \n",
      "around the world, and there are no issues with version mismatch where \n",
      "your copy of the run happens to differ with someone else’s. In a sense, this \n",
      "is serving the role of a model registry, just without the added functionality \n",
      "of the MLFlow Model Registry.Chapter 7  Deploying in  google278 Bucket Storage\n",
      "And so, let’s begin. First, open up the Google Cloud portal. You should \n",
      "see something like Figure  7-1. Be aware, though, that Google Cloud is also \n",
      "constantly being updated, so your portal screen may look different.\n",
      "Notice the scroll bar on the left side of the screen. This is where you can \n",
      "look at the services Google Cloud provides. Scroll to the section that says \n",
      "Storage, and click the service named Storage. You should see something \n",
      "similar to Figure  7-2.\n",
      "Figure 7-1.  What our Google Cloud portal screen looks like\n",
      "Figure 7-2.  Something similar to what you might see. In your case, \n",
      "you might not have any buckets hereChapter 7  Deploying in  google279Click the button that says CREATE BUCKET. Type in mlops-storage .\n",
      "Next, where it asks for a location type, select the Region option to have \n",
      "the lowest costs. Refer to Figure  7-3.\n",
      "Keep the rest of the options as is and click the Create button. You \n",
      "should now see something that looks like Figure  7-4.\n",
      "Figure 7-3.  Specifying the storage option for your bucket. Select \n",
      "Region to keep the costs the lowest, although with the amount of data \n",
      "you are pushing, the actual costs are very little\n",
      "Figure 7-4.  What your bucket might look like after creationChapter 7  Deploying in  google280From here, you want to upload your MLFlow experiments (the \n",
      "content of your mlruns  directory) as folders, so click Upload Folder, and \n",
      "upload all of the folders inside the mlruns  directory. You can leave out \n",
      "the folder named .trash . In our case, we only uploaded the experiment \n",
      "using scikit-learn  and left the rest out since we won’t be using the other \n",
      "experiments.\n",
      "You should see something like Figure  7-5 when finished.\n",
      "With that, you have finished configuring your storage. The next thing to \n",
      "configure is the virtual machine that will be hosting your model.\n",
      "Figure 7-5.  Our bucket after uploading the contents of our mlruns \n",
      "directory. We only uploaded the experiment using scikit-learn to save \n",
      "on costsChapter 7  Deploying in  google281\n",
      "Figure 7-6.  What your VM Instances screen may look like. In our \n",
      "case, we already have another machine running, but that is irrelevant \n",
      "since we are creating a new machine Configuring the Virtual Machine\n",
      "After going back to the portal, scroll to the Compute section and click the \n",
      "Compute Engine option. You should see something like Figure  7-6. You \n",
      "want to make sure you’re in the portal for the service titled VM Instances.Chapter 7  Deploying in  google282Now, click Create Instance and you should see something like Figure  7- 7.\n",
      "In our case, we filled in or selected the options that we want our VM \n",
      "machine to use. We named our machine mlops-server, selected our region \n",
      "(it autoselects a zone for you), and specified that we want to use Ubuntu \n",
      "18.04 LTS. Finally, at the end, we want to allow HTTPS traffic from the \n",
      "internet.\n",
      "Finally, when finished, you should be able to see your VM machine on \n",
      "the list of machines. What you want to do now is to open your VM machine \n",
      "instance by clicking the name mlops-server . This should take you to a \n",
      "screen that looks like Figure  7-8.\n",
      "Figure 7-7.  The options you can fill in when creating your VM \n",
      "machine instance. You should match the selections shown in the \n",
      "figure to ensure consistency with our resultsChapter 7  Deploying in  google283Now look at the box that says SSH. There should be a little down arrow \n",
      "indicating that it is a drop-down list of something. Click that arrow and \n",
      "select the View gcloud command option. Refer to Figure  7-9.\n",
      "Figure 7-8.  What you should see when you click mlops-server. Notice \n",
      "the box that says SSH. You will use that shortly\n",
      "Figure 7-9.  The drop-down options for connecting to this VM instanceChapter 7  Deploying in  google284This should take you to a popup window that looks like Figure  7-10 . \n",
      "You have two options: running that command in a new instance of the \n",
      "Google Cloud SDK CLI (in our case, we had to search “Google Cloud SDK \n",
      "Shell” and it opened a configured Google Cloud terminal instance), or \n",
      "running it through a shell directly on the portal page itself. You can do \n",
      "either option, as both connect to the VM anyway.\n",
      "Copy and paste that command in your terminal to connect to the \n",
      "VM. When finished running, you should see something like Figure  7-11 , \n",
      "where it opens up a PuTTY instance of the actual shell inside the VM.\n",
      "Figure 7-10.  The command that lets you connect to the VM via \n",
      "SSH. You can also run it within the portal page itself if you’d likeChapter 7  Deploying in  google285This is where you must configure your VM so that it can host your \n",
      "MLFlow models.\n",
      "First, run the following commands:\n",
      "sudo apt update\n",
      "sudo apt upgrade\n",
      "Answer “y” to any prompts.\n",
      "Once finished, you can now install Conda. Without Conda, MLFlow \n",
      "won’t be able to reconstruct the environment that the MLFlow model \n",
      "was logged in. This is part of MLFlow’s modularization. In the case of \n",
      "SageMaker and Azure, you built containers that, as their name suggests, \n",
      "“contain” these Conda environments already. This way, SageMaker does \n",
      "not have to reinstall any Conda packages once the container is in the \n",
      "cloud. It simply has to run an instance of the container and it already has \n",
      "everything configured.\n",
      "First, find out how to install Anaconda on Linux by going to its \n",
      "webpage. An install link should be provided. Copy the link and paste it \n",
      "somewhere. You will retrieve that link using a command.\n",
      "Figure 7-11.  The result of running the gcloud command that the \n",
      "portal provided. On the right, you can see a PuTTY terminal where \n",
      "you have the shell open inside the VMChapter 7  Deploying in  google286Run the following one at a time:\n",
      "cd /tmp\n",
      "curl -O https://repo.anaconda.com/archive/ \n",
      "Anaconda3-2020.07-  Linux- x86_64.sh\n",
      "You should see something like Figure  7-12 .\n",
      "Next, let’s install Anaconda by running the following. You can type in \n",
      "bash Anaconda  and press Tab to autofill the rest of the script name.\n",
      "bash https://repo.anaconda.com/archive/ \n",
      "Anaconda3-2020.07-  Linux- x86_64.sh\n",
      "It should ask you to look through the license agreement. At the end, \n",
      "answer yes, and press Enter to confirm the default installation location. \n",
      "Conda should then proceed with the installation. Answer yes to any \n",
      "further prompts. Once it’s done, restart the shell (close the PuTTY client \n",
      "and rerun the command or cloud shell), and you should now have Conda \n",
      "fully configured.\n",
      "As you will now see, Conda has already started the base environment. \n",
      "Let’s create a new environment by running the following code:\n",
      "conda create -n mlflow python=3.7\n",
      "Answer “y” to any following prompts, and you should see something \n",
      "like Figure  7-13 .\n",
      "Figure 7-12.  The output of fetching the Anaconda installation scriptChapter 7  Deploying in  google287Next, you will install the following packages: mlflow  and  \n",
      "google-  cloud-  storage . The former is self-explanatory: you will need MLFlow \n",
      "to do anything with MLFlow. You need google-cloud-storage because you are \n",
      "going to access your runs from the Google storage bucket from earlier.\n",
      "Run the following:\n",
      "conda activate mlflow\n",
      "pip install mlflow google-cloud-storage\n",
      "Running this code should also install all of the dependencies. In the \n",
      "future, should you need to install any more dependencies, it’s as simple as \n",
      "activating the mlflow environment and using pip install  to get any more \n",
      "packages or update existing packages.\n",
      "Once it has finished installing everything, you should see something \n",
      "like Figure  7-14 .\n",
      "Figure 7-13.  If you see this, then your Conda environment has \n",
      "successfully installedChapter 7  Deploying in  google288With that, you have fully configured your VM. All that is left is to \n",
      "configure the firewall.\n",
      " Configuring the Firewall\n",
      "First, you need to look at the internal IP that your VM instance is using. To \n",
      "do that, run the following:\n",
      "ifconfig\n",
      "You should see something like Figure  7-15 .\n",
      "Figure 7-14.  The final output after finishing installing the necessary \n",
      "packages in the Conda environmentChapter 7  Deploying in  google289Make a note of the internal IP , which we have highlighted in red. In \n",
      "your case, it will be different.\n",
      "Now, you must add a firewall to allow access to your server once it is \n",
      "started. Go back to the portal, scroll to the section that says Networking, \n",
      "and click the VPC Networks option. You should see something like \n",
      "Figure  7-16 .\n",
      "Figure 7-15.  Something similar to what you should see when you \n",
      "run the command. We have highlighted in red where you can find the \n",
      "internal IP of your machine. In our case, it is 10.142.0.4. Yours will be \n",
      "differentChapter 7  Deploying in  google290Now, click Firewall and then click Create Firewall Rule. Namely, you \n",
      "want to enter the following values:\n",
      "• Name: mlflow-server\n",
      "• Target tags : mlops-server, http-server, https-server\n",
      "• Source IP ranges : 0.0.0.0/0\n",
      "• Protocols and Ports : Check TCP and type 5000\n",
      "If you made a mistake, you can edit the firewall rules. You should see \n",
      "something like Figure  7-17 .\n",
      "Figure 7-16.  The VPC Networks module in the portal. Click the \n",
      "Firewall option to look at the firewall optionsChapter 7  Deploying in  google291\n",
      "Figure 7-17.  What your firewall configuration should look like. We \n",
      "have autofilled the values with our ownChapter 7  Deploying in  google292Now click Create. You are done configuring the firewall and \n",
      "configuring everything else in Google Cloud. Now you can move on to \n",
      "deploying your model.\n",
      " Deploying and Querying the Model\n",
      "With your virtual machine fully configured, it’s time to deploy your model.\n",
      "Make sure you still have that internal IP logged in. Go back to the \n",
      "PuTTY client and now enter the following command:\n",
      "mlflow models serve -m  gs://mlops-storage/EXPERIMENT_ID/RUN_ID/\n",
      "artifacts/MODEL_NAME -h 10.142.0.4\n",
      "Our command looks like the following. We simply took the first run in \n",
      "the Google Storage bucket.\n",
      "mlflow models serve -m gs://mlops-storage/8/1eb809b446d949d5a70\n",
      "a1e22e4b4f428/artifacts/log_reg_model -h 10.142.0.4\n",
      "You should see something like Figure  7-18 .Chapter 7  Deploying in  google293\n",
      "Figure 7-18.  This is what your output should look like if it \n",
      "successfully built the Conda environment and is now serving the \n",
      "model\n",
      "There’s only one more step that remains before you can successfully \n",
      "make predictions with this model. You must now see what your external \n",
      "IP is. To do so, go back to the VM Instances page to find your VM machine. \n",
      "You should see something like Figure  7-19 .Chapter 7  Deploying in  google294Once you have the external IP address, copy it down somewhere.\n",
      "Now you can start up your Jupyter notebook and query this model.\n",
      "In a Jupyter notebook cell, run the following. Make sure you have the \n",
      "data  folder in the same directory as this notebook, and that the data  folder \n",
      "contains the creditcard.csv  file:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import roc_auc_score, accuracy_score, \n",
      "confusion_matrix\n",
      "Figure 7-19.  The VM Instances section in the portal should display \n",
      "the external IP of your server. We have highlighted ours in red, but \n",
      "yours is most likely something differentChapter 7  Deploying in  google295import numpy as np\n",
      "import subprocess\n",
      "import json\n",
      "df = pd.read_csv(\"data/creditcard.csv\")\n",
      "Next, you define your query()  function that you will use to get model \n",
      "predictions:\n",
      "def query(input_json):\n",
      "    proc = subprocess.run([\"curl\",   \"-X\", \"POST\", \"-H\", \n",
      "\"Content-Type:application/json; format=pandas-split\",\n",
      "                       \"--data\", input_json, \n",
      "\"http://34.75.74.9:5000/invocations\"],\n",
      "                      stdout=subprocess.PIPE, encoding='utf-8')\n",
      "    output = proc.stdout\n",
      "    preds = json.loads(output)\n",
      "    return preds\n",
      "Notice that the IP is now http://34.75.74.9:5000/invocations . \n",
      "Basically, your IP should take the form of http://YOUR_EXTERNAL_\n",
      "IP:5000/invocations , replacing the placeholder with the external IP \n",
      "address of your VM.\n",
      "Let’s now query your model:\n",
      "input_json = df.iloc[:80].drop([\"Time\", \"Class\"],  \n",
      "axis=1).to_json(orient=\"split\")\n",
      "pd.DataFrame(query(input_json)).T\n",
      "Altogether, you should see something like Figure  7-20 .Chapter 7  Deploying in  google296As expected, the predictions aren’t correct because you did not scale \n",
      "the data before querying the model with it. However, you have verified that \n",
      "you have queried the correct address and that the model is able to return \n",
      "predictions.\n",
      "Now run the following cells:\n",
      "normal = df[df.Class == 0].sample(frac=0.5, random_state=2020).\n",
      "reset_index(drop= True)\n",
      "anomaly = df[df.Class == 1]\n",
      "normal_train, normal_test = train_test_split(normal, test_size \n",
      "= 0.2, random_state = 2020)\n",
      "anomaly_train, anomaly_test = train_test_split(anomaly,  \n",
      "test_size = 0.2, random_state = 2020)\n",
      "Figure 7-20.  The output of querying the model with the first 80 rows \n",
      "of your data frameChapter 7  Deploying in  google297scaler = StandardScaler()\n",
      "scaler.fit(pd.concat((normal, anomaly)).drop([\"Time\", \"Class\"], \n",
      "axis=1))\n",
      "test = pd.concat((normal.iloc[:1900], anomaly.iloc[:100]))\n",
      "true = test.Class\n",
      "test = scaler.transform(test.drop([\"Time\", \"Class\"], axis=1))\n",
      "preds = []\n",
      "batch_size = 80\n",
      "for f in range(25):\n",
      "    print(f\"Batch {f}\", end=\"  - \")\n",
      "     sample = pd.DataFrame(test[f*batch_size:(f+1)*batch_size]).\n",
      "to_json(orient=\"split\")\n",
      "    output = query(sample)\n",
      "    resp = pd.DataFrame([output])\n",
      "    preds = np.concatenate((preds, resp.values[0]))\n",
      "    print(\"Completed\")\n",
      "eval_acc = accuracy_score(true, preds)\n",
      "eval_auc = roc_auc_score(true, preds)\n",
      "print(\"Eval Acc\", eval_acc)\n",
      "print(\"Eval AUC\", eval_auc)\n",
      "Once finished, you should see something like Figure  7-21 .Chapter 7  Deploying in  google298 Updating and Removing a Deployment\n",
      "Updating the model deployment is extremely easy. With how you set it \n",
      "up, it’s only a matter of quitting the model serving command (Ctrl-C), and \n",
      "rerunning the command with a different run ID.\n",
      "Let’s try deploying a different run. In your case, check your Google \n",
      "Storage bucket and pick the second run.\n",
      "In our case, we ran the following:\n",
      "mlflow models serve -m gs://mlops-storage/8/3862eb3bd89b43e8ace\n",
      "610c521d974e6/artifacts/log_reg_model -h 10.142.0.4\n",
      "As you can see in Figure  7-22 , it successfully deployed, and we can \n",
      "simply query it using the same script.\n",
      "Figure 7-21.  The results of running your batch query scriptChapter 7  Deploying in  google299As for removing a deployment, all you have to do is just cancel the \n",
      "command with Ctrl-C and your deployment is now cancelled.\n",
      "With that, you now know how to serve models, switch a model and \n",
      "deploy a different one, and remove a deployment by simply canceling the \n",
      "model serving command.\n",
      " Cleaning Up\n",
      "It’s time to delete every instance of a service that you created so that you \n",
      "won’t incur any charges. Here’s a list of all of the services you used:\n",
      "• Google Cloud Storage Bucket\n",
      "• Compute Engine VM Instance\n",
      "• Networking Firewall Rule\n",
      "Figure 7-22.  Deploying a different model run using the same \n",
      "command conventionChapter 7  Deploying in  google300Beginning with your VM Instance, you want to click STOP to first \n",
      "stop the VM from running. You should see something like Figure  7-23  \n",
      "depending on where you access this VM.\n",
      "After that, you can simply click DELETE to remove the VM. Stopping \n",
      "the VM only ensures that you won’t be billed for CPU/GPU utilization, but \n",
      "it won’t stop any charges that result from services linked to the VM.\n",
      "Next, let’s go to the Storage bucket. Simply check your bucket and click \n",
      "DELETE to remove this storage. Refer to Figure  7-24 .\n",
      "Lastly, you may remove the firewall rule as well, but be sure to not \n",
      "remove any other rules that you might have in there.\n",
      "Figure 7-23.  The VM instance after stopping it\n",
      "Figure 7-24.  Removing your storage bucketChapter 7  Deploying in  google301With that, your workspace should be cleaned up, and there shouldn’t \n",
      "be any more services that may incur charges.\n",
      " Summary\n",
      "Google Cloud is a cloud platform that provides many advanced services for \n",
      "a wide range of users. While MLFlow does not explicitly provide support \n",
      "for deployment for Google Cloud, you are still able to operationalize your \n",
      "models using MLFlow’s model serving functionality and Google Cloud’s \n",
      "compute engine to serve the models on the cloud.\n",
      "In this chapter, you learned how to set up Google Cloud so that it can \n",
      "deploy your models on a virtual machine. In particular, you looked at \n",
      "how you can push your MLFlow runs to a bucket, how you can set up the \n",
      "Conda environment on a virtual machine, how you can set up a firewall \n",
      "to allow your model to be accessed in order to be queried, and how you \n",
      "can manage your deployments by simply switching out run IDs (and \n",
      "experiment IDs where appropriate).\n",
      "In the Appendix, you can look at how Databricks helps you \n",
      "operationalize your models and manage them through the use of a model \n",
      "registry.Chapter 7  Deploying in  google303© Sridhar Alla, Suman Kalyan Adari 2021 \n",
      "S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \n",
      "https://doi.org/10.1007/978-1-4842-6549-9APPENDIX\n",
      " Databricks\n",
      "In this appendix, we will cover what Databricks is as well as how you \n",
      "can utilize its built-in MLFlow functionality to log MLFlow runs within \n",
      "Databricks itself, how to deploy models from Databricks to Azure, and how \n",
      "the MLFlow model registry works in Databricks.\n",
      " Introduction\n",
      "Databricks is an open platform and cloud service that provides \n",
      "interoperability with other popular AI and data services like AWS and \n",
      "Microsoft Azure. Databricks also created Apache Spark, Delta Lake, and \n",
      "MLFlow (see Chapter 4 to learn what MLFlow is).\n",
      "Before we begin, you will need a Databricks account. You have the \n",
      "option of creating a “community edition” account, which is free to users \n",
      "but is limited in its functionality. You will be able to use basic MLFlow \n",
      "functionality on top of whatever Python functionality you have (PySpark is \n",
      "supported, for example), but you will not be able to use the model registry \n",
      "functionality.\n",
      "To sign up for one, head on over to this website:  \n",
      "https://community.cloud.databricks.com/ .\n",
      "Otherwise, you will have to pay to be able to use Databricks by \n",
      "choosing a subscription plan for your account.304With Databricks, you can integrate with Amazon AWS or Microsoft \n",
      "Azure. If you choose to subscribe to a plan from Databricks, you will be \n",
      "integrating with AWS. However, you can also deploy Databricks in Azure, \n",
      "which you can find more information about here:  \n",
      " https://azure.microsoft.com/en-us/services/databricks/ .\n",
      "Be warned, although Microsoft Azure does offer a free, 14-day trial of \n",
      "Databricks, you cannot create clusters without upgrading to the premium \n",
      "version of Azure Databricks (with a paid Azure subscription).\n",
      "In this appendix, we will be using the community edition  of \n",
      "Databricks, which is free to sign up for an use. The only exception here is \n",
      "the section in which we cover the model registry, which seems to only be \n",
      "available to premium Databricks users.\n",
      "In detail, we will go over the following:\n",
      "• Logging MLFlow runs within Databricks:  You can run \n",
      "your Jupyter notebooks within Databricks itself, which \n",
      "provides functionality to import your old notebooks. \n",
      "For this part, you will import your notebook from \n",
      "Chapter 4 where you conduct experiments using scikit-  \n",
      "learn. All runs will be logged within Databricks.\n",
      "• MLFlow UI:  Databricks has a built-in MLFlow UI that \n",
      "allows you to see all of your runs per experiment just as \n",
      "you would in the browser. You will look at your experiment \n",
      "using this UI and inspect a run that you will log.\n",
      "• Deploying to AWS/Azure:  Depending on what you integrate \n",
      "with, you can deploy your models to one of these services. In \n",
      "this chapter, we will be deploying to Microsoft Azure.\n",
      "• MLFlow Model Registry:  With premium Databricks \n",
      "(non-community edition), you have the added \n",
      "capability of having a model registry. Here, we will go \n",
      "over what the model registry is and how it works.\n",
      "With that, let’s get started!Appendix  dAtAbricks305 Running Experiments in Databricks\n",
      "Once you have Databricks set up, whether in community edition or \n",
      "otherwise, you should be greeted with a home screen that looks somewhat \n",
      "like Figure A-1.\n",
      "Where it says Common Tasks, go down until you see the option titled \n",
      "New MLFlow Experiment. Click this option.\n",
      "You can type in any other name you like, but you should see something \n",
      "like Figure A-2.\n",
      "Figure A-1.  The Databricks home screen. If you have the community \n",
      "edition, you won’t have the Models tab on the navigation bar to the \n",
      "left, but otherwise it should look about the sameAppendix  dAtAbricks306\n",
      "Figure A-2.  The screen you should see when creating an MLFlow \n",
      "experiment\n",
      "Figure A-3.  The screen displayed after experiment creation. Note that \n",
      "the experiment name is now /Users/sadari@bluewhale.one/sklearn. \n",
      "Be sure to make note of this as this is the full experiment name you \n",
      "will use when setting the experiment in the codeGo ahead and click Create. You should now see the MLFlow UI \n",
      "displaying the details of this experiment. Of course, there are no runs since \n",
      "you just created it. You should see something like Figure A-3.Appendix  dAtAbricks307Something important to mention is that the experiment name in this \n",
      "case is not sklearn, but rather it is /Users/sadari@bluewhale.one/sklearn \n",
      "in its entirety . Whatever you see is what you will be using when setting the \n",
      "experiment in the notebook code.\n",
      "With that, simply click Databricks to return to the home screen.\n",
      "You now have two choices:\n",
      " 1. Create a new notebook and fill in the cells from scratch.\n",
      " 2. Import your MLFlow scikit-learn notebook from \n",
      "Chapter 4.\n",
      "In this chapter, you will be importing the MLFlow scikit-learn \n",
      "notebook, but you will be making a few changes in order to ensure that it is \n",
      "adapted to work with Databricks.\n",
      "Before you even begin with the notebook, however, you need to create \n",
      "the cluster  that will run your notebook code. To do this, click the New \n",
      "Cluster option, and you should see something like Figure A-4.\n",
      "Figure A-4.  Cluster creation UI in the community edition of \n",
      "Databricks. Here, the name and the 7.2 ML runtime are autofilledAppendix  dAtAbricks308Make sure that you have the same runtime as in Figure A-4, or at least \n",
      "something that has “ML ” in the runtime name. Once finished, click the \n",
      "Create Cluster option.\n",
      "After that, you’ll be taken to a UI that shows all the clusters you have. \n",
      "Refresh if the cluster does not immediately show up. This can take a bit, so \n",
      "in the meantime, let’s head back to the home screen.\n",
      "At this point, you can proceed with your notebook. On the left \n",
      "navigation pane, click Home > Users (if it’s not selected for you), and \n",
      "then click your username to open a dropdown window. You should see \n",
      "something like Figure A-5.\n",
      "Figure A-5.  Home menu that allows you to import a notebook. Don’t \n",
      "worry about the other files you see here; you are likely to only have the \n",
      "experiment named sklearn and perhaps the Quickstart Notebook fileAppendix  dAtAbricks309Click Import and navigate to your MLFlow notebook from Chapter 4 (if \n",
      "you have one just for scikit-learn, that is preferable).\n",
      "You will now be taken to a notebook with all the contents of the \n",
      "notebook you just imported, except for the outputs.\n",
      "Before you get to run this, you must import your data. To do this, refer \n",
      "to Figure A-6. You must click File ➤ Upload Data in the dropdown menu.\n",
      "Leaving everything else as is, click Browse and locate and upload your \n",
      "credit card dataset ( creditcard.csv ).\n",
      "This will take some time to upload due to the size of the file, but once \n",
      "it is all done, click Next, which will give you code samples that tell you how \n",
      "to import this file. Make sure you have selected pandas. You can now paste \n",
      "this code and try to run it. In our case, we had an error stating that the file \n",
      "Figure A-6.  Uploading the data so that it can be accessed by this \n",
      "notebookAppendix  dAtAbricks310did not exist, so we instead loaded it with Spark and converted it into a \n",
      "pandas data frame, which does work for some reason given the same file \n",
      "path.\n",
      "Before you can execute anything, make sure that the cluster has \n",
      "finished building. Above the first cell in the notebook, you’ll notice a bar \n",
      "that says “detached. ” Click it and you should see your cluster available \n",
      "here. If the cluster is ready to use, there should be a green dot beside \n",
      "it. Otherwise, it will have the loading circle indicating that it’s still \n",
      "configuring.\n",
      "Go ahead and click the cluster. Once it is finished, you should see \n",
      "something like Figure A-7.\n",
      "Now you can begin with the modifications to the code. Let’s start with \n",
      "the import statements. Change the first cell to look like the following:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib #\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import sklearn #\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "Figure A-7.  An indication that the cluster is ready to use. If you see \n",
      "the green dot, you can now execute the cells in the notebookAppendix  dAtAbricks311from sklearn.metrics import roc_auc_score, plot_roc_curve, \n",
      "confusion_matrix, accuracy_score\n",
      "from sklearn.model_selection import KFold\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark import SparkConf, SparkContext\n",
      "import os\n",
      "import mlflow\n",
      "import mlflow.sklearn\n",
      "print(\"Numpy: {}\".format(np.__version__))\n",
      "print(\"Pandas: {}\".format(pd.__version__))\n",
      "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
      "print(\"seaborn: {}\".format(sns.__version__))\n",
      "print(\"Scikit-Learn: {}\".format(sklearn.__version__))\n",
      "print(\"MLFlow: {}\".format(mlflow.__version__))\n",
      "print(\"PySpark: {}\".format(pyspark.__version__))\n",
      "Here, you have added extra import statements so that you import \n",
      "PySpark.\n",
      "Create a new cell beneath your first cell, adding the following:\n",
      "os.environ[\"SPARK_LOCAL_IP\"]='127.0.0.1'\n",
      "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
      "spark.sparkContext._conf.getAll()Appendix  dAtAbricks312You should see something like Figure A-8 when executed.\n",
      "The next cell should be where you were loading the pandas data frame. \n",
      "Change it to be just the following:\n",
      "df = spark.read.csv(\"/FileStore/tables/creditcard.csv\",  \n",
      "header = True, inferSchema = True).toPandas()\n",
      "df = df.drop(\"Time\", axis=1)\n",
      "If you run this cell and the next, which should be df.head() , you \n",
      "should see something like Figure A-9.\n",
      "Figure A-8.  Running the first two cells and ensuring you have a \n",
      "Spark contextAppendix  dAtAbricks313\n",
      "Figure A-9.  Ensuring that you have successfully loaded the data \n",
      "frame in PySpark and have converted it to pandas\n",
      "At this point, simply run the rest of the code up until the cell where you \n",
      "actually start the MLFlow run.\n",
      "You must split up this cell to ensure everything logs to the same run. \n",
      "And so, you can create a new cell if you wish, with the following content:\n",
      "sk_model = LogisticRegression(random_state=None, max_iter=400, \n",
      "solver='newton-cg')\n",
      "mlflow.set_experiment(\"/Users/sadari@bluewhale.one/sklearn\")\n",
      "train(sk_model, x_train, y_train)\n",
      "Here are the next three cells. Each text box is supposed to be its own \n",
      "cell:\n",
      "evaluate(sk_model, x_test, y_test)\n",
      "mlflow.sklearn.log_model(sk_model, \"log_reg_model\")\n",
      "mlflow.end_run()Appendix  dAtAbricks314Together, they should look like Figure A-10 .\n",
      "Now, run these cells. You should now see all of this logged in the \n",
      "experiment.\n",
      "To view your runs, click Workspace in the navigation pane, and then \n",
      "click sklearn and the experiment name. You should see a run logged there. \n",
      "Click it, and you should see something like Figure A-11 , with all the metrics \n",
      "and artifacts logged successfully.\n",
      "Figure A-10.  Splitting up the code to log the relevant metrics and \n",
      "artifacts to ensure everything ends up in the same run. It seems \n",
      "counterintuitive, but lumping it all under the same run with mlflow.\n",
      "start_run() seems to cause the runs to failAppendix  dAtAbricks315With that, you are now ready to deploy. Logging MLFlow runs is as \n",
      "simple as in Databricks. One of the added benefits of Databricks is that \n",
      "it integrates Spark within its functionality, so if you primarily want to log \n",
      "PySpark models, Databricks might be ideal for you.\n",
      " Deploying to Azure\n",
      "Since we have already looked at how to deploy to Azure, we will get straight \n",
      "to the point. If you would like to explore this process in more detail, refer to \n",
      "Chapter 6.\n",
      "Figure A-11.  Viewing the metrics and artifacts of the run and \n",
      "ensuring they were logged successfullyAppendix  dAtAbricks dAtAbricks316 Connecting to the Workspace\n",
      "In this step, you are simply connecting to an existing workspace through \n",
      "Databricks. It’s important to note that Databricks does not have azureml-  \n",
      "sdk installed, so you must do so yourself. Luckily, Jupyter allows you to do \n",
      "this in a cell, so simply run the following:\n",
      "!pip install azureml-sdk\n",
      "Next, run the following, replacing all the placeholders with your own \n",
      "corresponding values:\n",
      "import azureml\n",
      "from azureml.core import Workspace\n",
      "workspace_name = \"databricks-deploy\" # Your workspace name\n",
      "workspace_location=\"East US\" # Your region\n",
      "resource_group = \"azure-mlops\" #Your resource group\n",
      "subscription_id = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
      "# Your subscription ID above\n",
      "workspace = Workspace.create(name = workspace_name,\n",
      "                             location = workspace_location,\n",
      "                             resource_group = resource_group,\n",
      "                             subscription_id = subscription_id,\n",
      "                             exist_ok=True)\n",
      "When you run this, you should see something like Figure A-12  asking \n",
      "you for authentication. Simply follow the instructions and you should be \n",
      "good to go.Appendix  dAtAbricks317\n",
      "Figure A-12.  The cell asking for authentication as you attempt to \n",
      "connect to an existing workspace. Follow the instructions, and the cell \n",
      "should finish with the statement, “Deployed Workspace with name \n",
      "databricks-deploy. Took __ seconds”\n",
      "Once this finishes, you can proceed with building and pushing a \n",
      "container image using MLFlow functionality. Before you do that, make sure \n",
      "to keep track of your run ID (you should be able to see this in Figure A-11 ), \n",
      "and copy that information in the cell below:\n",
      "run_id = \"dabea5a03050455aa5ad4a61fa548093\"\n",
      "model_name = \"log_reg_model\"\n",
      "model_uri = f\"runs:/{run_id}/{model_name}\"\n",
      "Next up are the two cells with MLFlow code to build and push the \n",
      "container image:\n",
      "import mlflow.azureml\n",
      "model_image, azure_model =  mlflow.azureml.build_image  \n",
      "(model_uri=model_uri, workspace=workspace,\n",
      "                                   model_name=\"sklearn_logreg\",\n",
      "                                   image_name=\"model\",\n",
      "                                    description=\"SkLearn LogReg  \n",
      "Model for Anomaly Detection\",\n",
      "                                   synchronous=False)\n",
      "model_image.wait_for_creation(show_output=True)Appendix  dAtAbricks318\n",
      "Figure A-13.  The three cells from above and their outputs. Here, you \n",
      "specify a model run and then build and push a container to Azure \n",
      "based on that modelTogether, the cells should look like Figure A-13 .\n",
      "With this step finished, you are ready to deploy the model using \n",
      "MLFlow Azure.\n",
      "To do so, simply run the following:\n",
      "azure_service, azure_model = mlflow.azureml.deploy(model_uri,\n",
      "                             workspace,\n",
      "                             service_name=\"sklearn-logreg\",\n",
      "                             model_name=\"log-reg-model\",\n",
      "                             synchronous=True)\n",
      "With that, let’s now check the URI that you will use to query, just to \n",
      "ensure that it has successfully deployed:\n",
      "azure_service.scoring_uriAppendix  dAtAbricks319Upon success, you should see something that looks like Figure A-14  for \n",
      "both output cells.\n",
      "Since there is a URI, you know that your model’s been deployed \n",
      "successfully. You can move on to the querying process now.\n",
      " Querying the Model\n",
      "Before you make any predictions with your model, you need to define a \n",
      "query function:\n",
      "import requests\n",
      "import json\n",
      "def query(scoring_uri, inputs):\n",
      "    headers = {\n",
      "    \"Content-Type\": \"application/json\",\n",
      "    }\n",
      "Figure A-14.  The output of deploying the model as well as checking \n",
      "the scoring URI of the serviceAppendix  dAtAbricks320     response = requests.post(scoring_uri, data=inputs, \n",
      "headers=headers)\n",
      "    preds = json.loads(response.text)\n",
      "    return preds\n",
      "Let’s use your batch query code to query your deployed model and \n",
      "get some relevant metrics. Fortunately, you should already have your \n",
      "scaler object from earlier when you processed the data in the MLFlow \n",
      "experiment.\n",
      "Simply run the following:\n",
      "test = pd.concat((normal.iloc[:1900], anomaly.iloc[:100]))\n",
      "true = test.Class\n",
      "test = scaler.transform(test.drop([\"Class\"], axis=1))\n",
      "preds = []\n",
      "batch_size = 80\n",
      "for f in range(25):\n",
      "    print(f\"Batch {f}\", end=\"  - \")\n",
      "     sample = pd.DataFrame(test[f*batch_size:(f+1)*batch_size]).\n",
      "to_json(orient=\"split\")\n",
      "     output = query(scoring_uri=azure_service.scoring_uri, \n",
      "inputs=sample)\n",
      "    resp = pd.DataFrame([output])\n",
      "    preds = np.concatenate((preds, resp.values[0]))\n",
      "    print(\"Completed\")\n",
      "eval_acc = accuracy_score(true, preds)\n",
      "eval_auc = roc_auc_score(true, preds)\n",
      "print(\"Eval Acc\", eval_acc)\n",
      "print(\"Eval AUC\", eval_auc)Appendix  dAtAbricks321Your output should look somewhat like Figure A-15 .\n",
      "With that, you now know how to log MLFlow runs in Databricks and \n",
      "deploy models to a cloud platform.\n",
      "To delete the deployment, simply run the following:\n",
      "azure_service.delete()\n",
      "Be sure to delete all the resources that you created for this deployment \n",
      "as well.\n",
      "The procedure for AWS is very similar to what you did in Chapter 5, but \n",
      "you just need to set up AWS to allow Databricks to access it.\n",
      "Databricks has tutorials on how you can accomplish all of that as well. \n",
      "One of the perks of Databricks is that they have extensive documentation \n",
      "about almost everything, especially MLFlow.\n",
      "Figure A-15.  The output of the batch query script. If you cannot see \n",
      "an output past batch 20, resize the output by holding the little arrow \n",
      "on the bottom rightAppendix  dAtAbricks322 MLFlow Model Registry\n",
      "In this section, we will briefly discuss the model registry. To use the model \n",
      "registry, you do need a premium subscription to Databricks and whatever \n",
      "cloud platform service you choose to deploy Databricks on (either AWS or \n",
      "Azure).\n",
      "With MLFlow, Databricks provides built-in model registry functionality \n",
      "so that users can define what stage a particular model is in. The MLFlow \n",
      "Model Registry allows for greater collaboration between various teams, \n",
      "letting them all develop and maintain models at various stages in the \n",
      "model life cycle and manage them all in a centralized, organized region.\n",
      "The user is in control of the lifecycle stage changes (experimentation, \n",
      "testing, production) of the models with options between automatic and \n",
      "manual control. The MLFlow Model Registry tracks the history of the \n",
      "model and allows for governance in managing who is able to approve \n",
      "changes.\n",
      "Some concepts to know:\n",
      "• Registered model:  Once registered in the MLFlow \n",
      "Model Registry, it has a unique name, version, stage, \n",
      "and more.\n",
      "• Stage:  Some preset stages are None, Staging, \n",
      "Production, and Archived. The user can also create \n",
      "custom stages for each model version to represent its \n",
      "lifecycle. Model stage transitions are either requested  \n",
      "or approved , depending on the user’s level of \n",
      "management.\n",
      "• Description:  The user can annotate the model for the \n",
      "team.\n",
      "• Activities:  MLFlow records a registered model’s \n",
      "activities, providing a history of the model’s stages.Appendix  dAtAbricks323Some features include\n",
      "• Cen tral repository:  Register MLFlow models to a \n",
      "centralized location.\n",
      "• Model versioning:  Keep track of the version history of \n",
      "models. Now, a model built for a specific task can have \n",
      "several versions.\n",
      "• Model stage:  Model versions have stages to represent \n",
      "the cycle as a whole. Together with model versioning, \n",
      "older model versions can gradually become phased out \n",
      "while the newest versions are sent to staging first, for \n",
      "example.\n",
      "• Model stage transitions:  Respond to new changes \n",
      "and events with automation. Training scripts can be \n",
      "automated to train new models automatically and \n",
      "assign them to staging, for example.\n",
      "• CI/CD w orkflow integration:  Monitor changes to the \n",
      "CI/CD pipelines as new versions are registered and \n",
      "have their deployment stages changed. This allows for \n",
      "better governance over the deployment process.\n",
      "• Model serving:  MLFlow models can be served on \n",
      "Databricks through REST APIs, on top of deploying \n",
      "them on a cloud service like AWS or Azure.\n",
      "With that, let’s look at how you can register your model in Databricks.\n",
      "First, head over to your MLFlow experiment and pick a run. Scroll \n",
      "down to artifacts and click the folder that contains your model. If you \n",
      "don’t have premium Databricks, you won’t be able to see this Register \n",
      "Model button . If you click the button and click Create New Model in the \n",
      "dropdown menu, you will see something like Figure A-16 .Appendix  dAtAbricks324Once finished, the Register Model button should be replaced by a \n",
      "green checkmark and a link to the model version page of this specific \n",
      "model.\n",
      "On this page, you can set the model’s stage, which is one of None, \n",
      "Staging, Production, or Archived if you’re only using preset stages. \n",
      "Furthermore, you can add a description to this specific model.\n",
      "On top of that, you can also request to change the model’s stage (and \n",
      "add an optional comment to add some context), which can be approved, \n",
      "rejected, or canceled.\n",
      "This allows you to now keep better track of your models by knowing \n",
      "their present stages. There is also support for model versioning, so there \n",
      "can be multiple versions of the model, with the possibility of setting a \n",
      "model stage for each, which you can view at once.\n",
      "To view all the models that you registered, you can simply click the \n",
      "Models tab in Databricks, as shown in Figure A-17 .\n",
      "Figure A-16.  Registering a MLFlow modelAppendix  dAtAbricks325With the model registry that you looked at in prior chapters, where it’s \n",
      "just putting the models in a centralized area, you don’t have this type of \n",
      "functionality. If you were to implement this, it would have to be through an \n",
      "external program, although it’s actually a relatively simple task considering \n",
      "how everything is modularized for you.\n",
      "With regular MLFlow, this requires you to have a MLFlow server that \n",
      "saves the runs in a mysql, myssl, sqlite, or postgresql dialect. Then, when \n",
      "you open the UI that pertains to this specific server’s storage, you can \n",
      "register models and have all of the MLFLow Model Registry functionality.\n",
      "All of that can get pretty complicated, so Databricks takes care of it all \n",
      "for you, if you have the premium version of Databricks and a subscription \n",
      "to either AWS or Azure, whichever platform you deployed Databricks to.\n",
      "And that’s all there is to the MLFlow Model Registry in Databricks.\n",
      "With that, you now know how to run Jupyter notebooks in Databricks, \n",
      "how to log MLFlow runs and conduct experiments, and how to deploy \n",
      "your models to a cloud platform.\n",
      "Figure A-17.  The navigation pane on the left side of premium \n",
      "Databricks, deployed in Azure in this instance, with the Models tab \n",
      "that will take you to the model registryAppendix  dAtAbricks326 Summary\n",
      "Databricks is a cloud platform that integrates with Amazon AWS or \n",
      "Microsoft Azure. As the creator of MLFlow, Databricks integrates MLFlow \n",
      "functionality into its services, allowing you to run all the MLFlow \n",
      "experiments you’d like to on the cloud. Furthermore, it also takes care of \n",
      "the mechanisms behind running a model registry for you, allowing you to \n",
      "take full advantage of MLFlow on the cloud.\n",
      "In this appendix, you learned how to import your existing notebook, \n",
      "create a MLFlow experiment, and log your own MLFlow runs. On top of \n",
      "that, you also looked at deploying this model to Azure within Databricks \n",
      "itself, and you looked at the model registry and how it works in Databricks.\n",
      "With this, you now know how to take your existing machine \n",
      "learning experiments and operationalize them very easily with MLFlow. \n",
      "Furthermore, you also know how to deploy your models to three different \n",
      "cloud platforms: Amazon AWS, Microsoft Azure, and Google Cloud. \n",
      "With this chapter, you’ve also added Databricks to that list, although it’s \n",
      "mostly for running your MLFlow experiments on. That being said, you can \n",
      "definitely run MLFlow experiments and log your runs on the other cloud \n",
      "platforms; it’s just far easier to do so within Databricks.Appendix  dAtAbricks327© Sridhar Alla, Suman Kalyan Adari 2021 \n",
      "S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \n",
      "https://doi.org/10.1007/978-1-4842-6549-9Index\n",
      "A, B\n",
      "Analyze data, 80\n",
      "AWS SageMaker\n",
      "Amazon ECR, 237\n",
      "attributes, 235\n",
      "configuration, 232–234\n",
      "container, 237\n",
      "deploy a model, 238–243\n",
      "ECR repository list, 238\n",
      "mlruns directory, 235,  236\n",
      "predictions, 243–245,  247\n",
      "removing deployed  \n",
      "model, 250,  251\n",
      "switching models, 247–249\n",
      "C\n",
      "Command line interface (CLI), 229\n",
      "Continuous delivery, 87\n",
      "Continuous integration/\n",
      "continuous delivery of \n",
      "pipelines, 88\n",
      "automated model building, 106\n",
      "automated training  \n",
      "pipeline, 111\n",
      "data analysis, 106\n",
      "deploy pipeline, 110\n",
      "feature store, 105model registry, 111\n",
      "model services, 111\n",
      "modularized code, 106\n",
      "package store, 109,  110\n",
      "reflection on setup, 112\n",
      "source repository, 106\n",
      "testing, 107–109\n",
      "training pipeline trigger, 112\n",
      "user data collection, 111\n",
      "Continuous model delivery, 87, 95\n",
      "automated model building, 97\n",
      "automated training pipeline, 99\n",
      "data analysis, 96\n",
      "deploy pipeline, 99\n",
      "feature store, 95, 96\n",
      "model registry, 100\n",
      "model services, 100\n",
      "modularized code, 98\n",
      "performance, 101\n",
      "reflection on setup, 102–104\n",
      "training pipeline trigger, 101\n",
      "Convolutional neural network \n",
      "(CNN), 6\n",
      "Credit card data set\n",
      "kaggle website page, 10\n",
      "loading data set, 11–15\n",
      "normal and fraudulent, 16–18\n",
      "packages, 11328plotting\n",
      "anomalies, 23\n",
      "functions, 24\n",
      "graphs, 22, 32, 33, 36\n",
      "scatterplot, 35, 38\n",
      "V12 consistency, 37\n",
      "values, 21, 25–28,  30\n",
      "visualize relationships, 19, 20\n",
      "D\n",
      "Databricks\n",
      "cluster, 307\n",
      "community edition, 304\n",
      "definition, 303\n",
      "deploying Azure, 315–319,  321\n",
      "import statements, 310,  312\n",
      "metrics, 314,  315\n",
      "MLFlow displaying, 306\n",
      "MLFlow registry, 322–325\n",
      "notebook, 308–310\n",
      "PySpark, 313\n",
      "running experiment, 305\n",
      "Data patterns, 102\n",
      "Deployment stage, 89\n",
      "data collection, 92\n",
      "data store, 93\n",
      "model deployment, 92\n",
      "model services, 92\n",
      "Developmental operations \n",
      "(DevOps), 84, 85E\n",
      "Experimental stage, 86, 89\n",
      "data analysis, 90, 91\n",
      "data store, 90\n",
      "model building stage, 91\n",
      "process raw data, 90\n",
      "F\n",
      "False negatives, 57\n",
      "False positives, 57\n",
      "G\n",
      "Google Cloud\n",
      "cleaning up, 299,  300\n",
      "configuration, 277\n",
      "creation, 279\n",
      "deploy/querying, 292–294,   \n",
      "296,  298\n",
      "drop-down options, 283\n",
      "Firewall, 288–291\n",
      "install Anaconda, 286–288\n",
      "MLOps-server, 283\n",
      "portal screen, 278\n",
      "PuTTY terminal, 285\n",
      "Scikit-learn, 280\n",
      "SSH, 284\n",
      "storage, 278,  279\n",
      "update/remove, 298,  299\n",
      "VM, 281,  282Credit card data set ( cont .)Index329H\n",
      "Hyperparameter settings, 42, 82\n",
      "I, J\n",
      "Identity and Access Management \n",
      "(IAM), 230\n",
      "K\n",
      "KFold() function, 58\n",
      "k-fold cross validation, 82\n",
      "L\n",
      "Local model serving\n",
      "deploy, 213–216\n",
      "querying, 216,  217\n",
      "batching, 223,  224,  226\n",
      "with scaling, 220–222\n",
      "without scaling, 218,  219\n",
      "M, N\n",
      "Machine learning\n",
      "solutions\n",
      "high variance, 8\n",
      "identification problem, 2–6\n",
      "identification training/\n",
      "evaluating/validating, 6, 7\n",
      "model architecture, 7\n",
      "predicting, 9\n",
      "tuning hyperparameter, 8Manual implementation, 87–89\n",
      "reflection on setup, 93, 94\n",
      "Manual trigger, 101\n",
      "Microsoft Azure\n",
      "cleaning up, 270–272\n",
      "configuration, 255\n",
      "container image, 260\n",
      "deploying, 261–263\n",
      "home screen, 255\n",
      "output, 261\n",
      "predictions, 263–266,  268–270\n",
      "production, 267,  268\n",
      "workspace, 256–258\n",
      "MLFlow, 125\n",
      "parameter tuning\n",
      "broad search, 150,   \n",
      "152–164\n",
      "guided search, 164–170\n",
      "PySpark\n",
      "data processing, 200–207\n",
      "training/loading, 207–212\n",
      "PyTorch\n",
      "checking the run, 196,  197\n",
      "data processing, 184–189\n",
      "loading, 198,  199\n",
      "training/evaluation, 190,  \n",
      "191,  193–195\n",
      "Scikit-Learn, 129\n",
      "data processing, 129–135\n",
      "loading logged model, \n",
      "148–150\n",
      "logging/viewing, 139–148\n",
      "training/evaluating, 136–138Index330TensorFlow 2.0 (Keeras), 170\n",
      "checking the run, 179–181\n",
      "data processing, 171–174\n",
      "loading, 181–183\n",
      "training/evaluation, 175–178\n",
      "MLOps setups\n",
      "implementation, 122,  123\n",
      "manual implementation, 87\n",
      "Modularization, 98\n",
      "Monitoring, 83\n",
      "O\n",
      "Overfitting, 81\n",
      "P, Q\n",
      "Performance issues, 102\n",
      "Pipelines and automation\n",
      "data preprocessing, 115,  116\n",
      "model evaluation, 118,  119\n",
      "model selection, 114\n",
      "model summary, 121,  122\n",
      "model validation, 119,  120\n",
      "testing, 113\n",
      "training process, 116,  117\n",
      "plot_histogram() function, 26\n",
      "plot_scatter() function, 28\n",
      "Principal component analysis \n",
      "(PCA), 4PySpark\n",
      "data processing, 67–69,  71–73\n",
      "model evaluation, 74, 76, 77\n",
      "model training, 73\n",
      "versions, 66\n",
      "R\n",
      "Raw data, 79\n",
      "S\n",
      "Scheduled training, 101\n",
      "Scikit-learn\n",
      "data processing, 43, 45–51\n",
      "model evaluation, 53–57\n",
      "model training, 52, 53\n",
      "model validation, 58, 60–66\n",
      "versions, 42\n",
      "T, U\n",
      "Testing, 82\n",
      "Training set, 80\n",
      "True negatives, 57\n",
      "True positives, 57\n",
      "V, W, X, Y, Z\n",
      "Validation, 82, 83Credit card data set ( cont .)Index\n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Connection To DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create The LangChain Embedding and LLM Objects For Later Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\gemini\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "c:\\anaconda3\\envs\\gemini\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm= OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "embedding= OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Your LangChain Vector Store (Backed By Astra DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_vector_store= Cassandra(\n",
    "    embedding=embedding,\n",
    "    table_name='qa_mini_demo',\n",
    "    session=None,\n",
    "    keyspace=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter= CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "\n",
    ")\n",
    "texts= text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beginning MLOps \\nwith MLFlow      \\nDeploy Models in AWS SageMaker,  \\nGoogle Cloud, and Microsoft Azure\\n—\\nSridhar Alla\\nSuman Kalyan AdariBeginning MLOps \\nwith MLFlow\\nDeploy Models in\\xa0AWS \\nSageMaker, Google Cloud, \\nand\\xa0Microsoft Azure\\nSridhar\\xa0Alla\\nSuman\\xa0Kalyan\\xa0AdariBeginning MLOps with MLFlow\\nISBN-13 (pbk): 978-1-4842-6548-2   ISBN-13 (electronic): 978-1-4842-6549-9\\nhttps://doi.org/10.1007/978-1-4842-6549-9\\nCopyright © 2021 by Sridhar Alla, Suman Kalyan Adari \\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or \\npart of the material is concerned, specifically the rights of translation, reprinting, reuse of \\nillustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way,',\n",
       " 'part of the material is concerned, specifically the rights of translation, reprinting, reuse of \\nillustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, \\nand transmission or information storage and retrieval, electronic adaptation, computer software, \\nor by similar or dissimilar methodology now known or hereafter developed.\\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark \\nsymbol with every occurrence of a trademarked name, logo, or image we use the names, logos, \\nand images only in an editorial fashion and to the benefit of the trademark owner, with no \\nintention of infringement of the trademark. \\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if',\n",
       " 'intention of infringement of the trademark. \\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if \\nthey are not identified as such, is not to be taken as an expression of opinion as to whether or not \\nthey are subject to proprietary rights.\\nWhile the advice and information in this book are believed to be true and accurate at the date of \\npublication, neither the authors nor the editors nor the publisher can accept any legal \\nresponsibility for any errors or omissions that may be made. The publisher makes no warranty, \\nexpress or implied, with respect to the material contained herein.\\nManaging Director, Apress Media LLC: Welmoed Spahr\\nAcquisitions Editor: Celestin Suresh John\\nDevelopment Editor: Laura Berendson\\nCoordinating Editor: Aditee Mirashi',\n",
       " 'Managing Director, Apress Media LLC: Welmoed Spahr\\nAcquisitions Editor: Celestin Suresh John\\nDevelopment Editor: Laura Berendson\\nCoordinating Editor: Aditee Mirashi\\nCover designed by eStudioCalamar\\nCover image designed by Freepik (www.freepik.com)\\nDistributed to the book trade worldwide by Springer Science+Business Media New\\xa0York, 1 \\nNew\\xa0York Plaza, Suite 4600, New\\xa0York, NY 10004-1562, USA.\\xa0Phone 1-800-SPRINGER, fax (201) \\n348-4505, e-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, \\nLLC is a California LLC and the sole member (owner) is Springer Science + Business Media \\nFinance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware  corporation.\\nFor information on translations, please e-mail booktranslations@springernature.com; for',\n",
       " 'Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware  corporation.\\nFor information on translations, please e-mail booktranslations@springernature.com; for \\nreprint, paperback, or audio rights, please e-mail bookpermissions@springernature.com.\\nApress titles may be purchased in bulk for academic, corporate, or promotional use. eBook \\nversions and licenses are also available for most titles. For more information, reference our Print \\nand eBook Bulk Sales web page at www.apress.com/bulk-sales.\\nAny source code or other supplementary material referenced by the author in this book is \\navailable to readers on GitHub via the book’s product page, located at www.apress.com/ \\n978-1-4842-6548-2. For more detailed information, please visit www.apress.com/source-code.',\n",
       " 'available to readers on GitHub via the book’s product page, located at www.apress.com/ \\n978-1-4842-6548-2. For more detailed information, please visit www.apress.com/source-code.\\nPrinted on acid-free paperSridhar\\xa0Alla\\nDelran, NJ, USASuman\\xa0Kalyan\\xa0Adari\\nTampa, FL, USAiiiTable of Contents\\nChapter 1 :   Getting Started: Data Analysis  �������������������������������������������� 1\\nIntroduction and Premise  �������������������������������������������������������������������������������������� 1\\nCredit Card Data Set ��������������������������������������������������������������������������������������������10\\nLoading the Data Set  ������������������������������������������������������������������������������������� 11',\n",
       " 'Loading the Data Set  ������������������������������������������������������������������������������������� 11\\nNormal Data and Fraudulent Data  ����������������������������������������������������������������� 16\\nPlotting  ���������������������������������������������������������������������������������������������������������� 19\\nSummary �������������������������������������������������������������������������������������������������������������39\\nChapter 2 :   Building Models  ���������������������������������������������������������������� 41\\nIntroduction  ��������������������������������������������������������������������������������������������������������� 41\\nScikit-Lear n �������������������������������������������������������������������������������������������������������� 42',\n",
       " 'Scikit-Lear n �������������������������������������������������������������������������������������������������������� 42\\nData Processing ���������������������������������������������������������������������������������������������43\\nModel Training  ����������������������������������������������������������������������������������������������� 52\\nModel Evaluation  ������������������������������������������������������������������������������������������� 53\\nModel Validation  �������������������������������������������������������������������������������������������� 58',\n",
       " 'Model Validation  �������������������������������������������������������������������������������������������� 58\\nPySpark  ��������������������������������������������������������������������������������������������������������������� 66About the Authors  ������������������������������������������������������������������������������� vii\\nAbout the T echnical Reviewer  ������������������������������������������������������������� ix\\nAcknowledgments  ������������������������������������������������������������������������������� xi\\nIntroduction  ��������������������������������������������������������������������������������������� xiiiivData Processing ���������������������������������������������������������������������������������������������67',\n",
       " 'Model Training  ����������������������������������������������������������������������������������������������� 73\\nModel Evaluation  ������������������������������������������������������������������������������������������� 74\\nSummary �������������������������������������������������������������������������������������������������������������77\\nChapter 3 :   What Is MLOps?  ���������������������������������������������������������������� 79\\nIntroduction  ��������������������������������������������������������������������������������������������������������� 79\\nMLOps Setups  ����������������������������������������������������������������������������������������������������� 87\\nManual Implementation  ��������������������������������������������������������������������������������� 88',\n",
       " 'Manual Implementation  ��������������������������������������������������������������������������������� 88\\nContinuous Model Delivery  ���������������������������������������������������������������������������� 95\\nContinuous Integration/Continuous Delivery of Pipelines  ���������������������������� 105\\nPipelines and Automation  ��������������������������������������������������������������������������������� 113\\nJourney Through a Pipeline  ������������������������������������������������������������������������� 114\\nHow to Implement MLOps  ��������������������������������������������������������������������������������� 122\\nSummary �����������������������������������������������������������������������������������������������������������124',\n",
       " 'Summary �����������������������������������������������������������������������������������������������������������124\\nChapter 4 :   Introduction to MLFlow  ��������������������������������������������������� 125\\nIntroduction  ������������������������������������������������������������������������������������������������������� 125\\nMLFlo w with Scikit-Learn  ��������������������������������������������������������������������������������� 129\\nData Processing �������������������������������������������������������������������������������������������129\\nTraining and Evaluating with MLFlow  ���������������������������������������������������������� 136\\nLogging and Viewing MLFlow Runs  ������������������������������������������������������������� 139',\n",
       " 'Training and Evaluating with MLFlow  ���������������������������������������������������������� 136\\nLogging and Viewing MLFlow Runs  ������������������������������������������������������������� 139\\nModel Validation (Parameter Tuning) with MLFlow ��������������������������������������150\\nMLFlow and Other Frameworks  ������������������������������������������������������������������������ 170\\nMLFlow with TensorFlow 2 �0 (Keras)  ����������������������������������������������������������� 170\\nMLFlow with PyTorch �����������������������������������������������������������������������������������183\\nMLFlow with PySpark  ���������������������������������������������������������������������������������� 199',\n",
       " 'MLFlow with PySpark  ���������������������������������������������������������������������������������� 199\\nLocal Model Serving  ����������������������������������������������������������������������������������������� 213Table of Con TenTsvDeploying the Model  ������������������������������������������������������������������������������������ 213\\nQuerying the Model  ������������������������������������������������������������������������������������� 216\\nSummary �����������������������������������������������������������������������������������������������������������226\\nChapter 5 :   Deplo ying in AWS  ������������������������������������������������������������ 229\\nIntroduction  ������������������������������������������������������������������������������������������������������� 229',\n",
       " 'Introduction  ������������������������������������������������������������������������������������������������������� 229\\nConfiguring A WS ����������������������������������������������������������������������������������������������� 232\\nDeploying a Model to AWS SageMaker  ������������������������������������������������������������� 238\\nMaking Predictions  ������������������������������������������������������������������������������������������� 243\\nSwitching Models ����������������������������������������������������������������������������������������������247\\nRemoving Deployed Model ��������������������������������������������������������������������������������250\\nSummary �����������������������������������������������������������������������������������������������������������251',\n",
       " 'Summary �����������������������������������������������������������������������������������������������������������251\\nChapter 6 :   Deplo ying in Azure  ���������������������������������������������������������� 253\\nIntroduction  ������������������������������������������������������������������������������������������������������� 253\\nConfiguring Azur e ���������������������������������������������������������������������������������������������� 255\\nDeploying to Azure (Dev Stage)  ������������������������������������������������������������������������� 261\\nMaking Predictions  ������������������������������������������������������������������������������������������� 263\\nDeploying to Production  ������������������������������������������������������������������������������������ 267',\n",
       " 'Deploying to Production  ������������������������������������������������������������������������������������ 267\\nMaking Predictions  ������������������������������������������������������������������������������������������� 268\\nCleaning Up  ������������������������������������������������������������������������������������������������������� 270\\nSummary �����������������������������������������������������������������������������������������������������������272\\nChapter 7 :   Deplo ying in Google  �������������������������������������������������������� 275\\nIntroduction  ������������������������������������������������������������������������������������������������������� 275',\n",
       " 'Introduction  ������������������������������������������������������������������������������������������������������� 275\\nConfiguring Google  �������������������������������������������������������������������������������������������� 277\\nBucket Storage  �������������������������������������������������������������������������������������������� 278\\nConfiguring the Virtual Machine  ������������������������������������������������������������������ 281\\nConfiguring the Firewall  ������������������������������������������������������������������������������ 288Table of Con TenTsviDeploying and Querying the Model  ������������������������������������������������������������������� 292\\nUpdating and Removing a Deployment  ������������������������������������������������������������� 298',\n",
       " 'Updating and Removing a Deployment  ������������������������������������������������������������� 298\\nCleaning Up  ������������������������������������������������������������������������������������������������������� 299\\nSummary �����������������������������������������������������������������������������������������������������������301\\n  Appendix: Databricks  ����������������������������������������������������������������������� 303\\n  Introduction  ������������������������������������������������������������������������������������������������������� 303\\n  Running Experiments in Databric ks ������������������������������������������������������������������ 305\\n  Deploying to Azure  �������������������������������������������������������������������������������������������� 315',\n",
       " 'Deploying to Azure  �������������������������������������������������������������������������������������������� 315\\n  Connecting to\\xa0the\\xa0Workspace  ��������������������������������������������������������������������������� 316\\n  Querying the\\xa0Model  ������������������������������������������������������������������������������������������� 319\\n  MLFlow Model Registry  ������������������������������������������������������������������������������������ 322\\n  Summary �����������������������������������������������������������������������������������������������������������326\\n Index  ������������������������������������������������������������������������������������������������� 327Table of Con TenTsviiAbout the Authors\\nSridhar\\xa0Alla \\xa0is the founder and CTO of',\n",
       " 'Index  ������������������������������������������������������������������������������������������������� 327Table of Con TenTsviiAbout the Authors\\nSridhar\\xa0Alla \\xa0is the founder and CTO of \\nBluewhale.one, the company behind the \\nproduct Sas2Py ( www.sas2py.com ), which \\nfocuses on the automatic conversion of SAS \\ncode to Python. Bluewhale also focuses on \\nusing AI to solve key problems ranging from \\nintelligent email conversation tracking to \\nissues impacting the retail industry and more. \\nHe has deep expertise in building AI-driven \\nbig data analytical practices on both the public cloud and in-house \\ninfrastructures. He is a published author of books and an avid presenter at \\nnumerous Strata, Hadoop World, Spark Summit, and other conferences.',\n",
       " 'infrastructures. He is a published author of books and an avid presenter at \\nnumerous Strata, Hadoop World, Spark Summit, and other conferences. \\nHe also has several patents filed with the US PTO on large-scale computing \\nand distributed systems. He has extensive hands-on experience in most of \\nthe prevalent technologies, including Spark, Flink, Hadoop, AWS, Azure, \\nTensorFlow, and others. He lives with his wife, Rosie, and daughters, \\nEvelyn and Madelyn, in New Jersey, United States, and in his spare time \\nloves to spend time training, coaching, and attending meetups. He can be \\nreached at sid@bluewhale.one .  \\nviiiSuman\\xa0Kalyan \\xa0Adari \\xa0is a current Senior and \\nundergraduate researcher at the University \\nof Florida specializing in deep learning',\n",
       " 'reached at sid@bluewhale.one .  \\nviiiSuman\\xa0Kalyan \\xa0Adari \\xa0is a current Senior and \\nundergraduate researcher at the University \\nof Florida specializing in deep learning \\nand its practical use in various fields such \\nas computer vision, adversarial machine \\nlearning, natural language processing \\n(conversational AI) , anomaly detection, \\nand more. He was a presenter at the IEEE \\nDependable Systems and Networks International Conference workshop \\non Dependable and Secure Machine Learning held in Portland, Oregon, \\nUnited States in June 2019. He is also a published author, having worked \\non a book focusing on the uses of deep learning in anomaly detection.  \\nHe can be reached at sadari@ufl.edu .  \\nabouT The auThorsixAbout the Technical Reviewer\\nPramod\\xa0Singh \\xa0is a Manager, Data Science',\n",
       " 'He can be reached at sadari@ufl.edu .  \\nabouT The auThorsixAbout the Technical Reviewer\\nPramod\\xa0Singh \\xa0is a Manager, Data Science \\nat Bain & Company. He has over 11 years \\nof rich experience in the Data Science field \\nworking with multiple product- and service-\\nbased organizations. He has been part of \\nnumerous ML and AI large scale projects. He \\nhas published three books on large scale data \\nprocessing  and machine learning. He is also a \\nregular speaker at major AI conferences such \\nas the O’Reilly AI & Strata conference.  \\nxiAcknowledgments\\nSridhar Alla\\nI would like to thank my wonderful wife, Rosie Sarkaria, and my \\nbeautiful, loving daughters, Evelyn and Madelyn, for all the love and \\npatience during the many months I spent writing this book. I would also',\n",
       " 'beautiful, loving daughters, Evelyn and Madelyn, for all the love and \\npatience during the many months I spent writing this book. I would also \\nlike to thank my parents, Ravi and Lakshmi Alla, for all the support and \\nencouragement they continue to bestow upon me.\\nSuman Kalyan Adari\\nI would like to thank my parents, Venkata and Jyothi Adari, and my \\nloving dog, Pinky, for supporting me throughout the entire process.  \\nI would especially like to thank my sister, Niharika Adari, for helping me \\nwith edits and proofreading and helping me write the appendix chapter.xiiiIntroduction\\nThis book is intended for all audiences ranging from beginners at machine \\nlearning, to advanced machine learning engineers, or even to machine \\nlearning researchers who wish to learn how to better organize their',\n",
       " 'learning, to advanced machine learning engineers, or even to machine \\nlearning researchers who wish to learn how to better organize their \\nexperiments.\\nThe first two chapters cover the premise of the problem followed by \\nthe book, which is that of integrating MLOps principles into an anomaly \\ndetector model based on the credit card dataset. The third chapter covers \\nwhat MLOps actually is, how it works, and why it can be useful.\\nThe fourth chapter goes into detail about how you can implement and \\nutilize MLFlow in your existing projects to reap the benefits of MLOps with \\njust a few lines of code.\\nThe fifth, sixth, and seventh chapters all go over how you can \\noperationalize your model and deploy it on AWS, Microsoft Azure, and',\n",
       " 'just a few lines of code.\\nThe fifth, sixth, and seventh chapters all go over how you can \\noperationalize your model and deploy it on AWS, Microsoft Azure, and \\nGoogle Cloud, respectively. The seventh chapter goes over how you \\ncan host a model on a virtual machine and connect to the server from \\nan external source to make your predictions, so should any MLFlow \\nfunctionality described in the book become outdated, you can always go \\nfor this approach and simply serve models on some cluster on the cloud.\\nThe last chapter, Appendix, goes over how you can utilize Databricks, \\nthe creators of MLFlow, to organize your MLFlow experiments and deploy \\nyour models.\\nThe goal of the book is to hopefully impart to you, the reader, \\nknowledge of how you can use the power of MLFlow to easily integrate',\n",
       " 'your models.\\nThe goal of the book is to hopefully impart to you, the reader, \\nknowledge of how you can use the power of MLFlow to easily integrate \\nMLOps principles into your existing projects. Furthermore, we hope that \\nyou will become more familiar with how you can deploy your models to \\nthe cloud, allowing you to make model inferences anywhere on the planet \\nso as long as you are able to connect to the cloud server hosting the model.xivAt the very least, we hope that more people do begin to adopt MLFlow \\nand integrate it into their workflows, since even as a tool to organize your \\nworkspace, it massively improves the management of your machine \\nlearning experiments and allows you to keep track of the entire model \\nhistory of a project.',\n",
       " 'workspace, it massively improves the management of your machine \\nlearning experiments and allows you to keep track of the entire model \\nhistory of a project.\\nResearchers may find MLFlow to be useful when conducting \\nexperiments, as it allows you to log plots on top of any custom-defined \\nmetric of your choosing. Prototyping becomes much easier, as you can \\nnow keep track of that one model which worked perfectly as a proof-of-  \\nconcept and revert back to those same weights at any time while you keep \\ntuning the hyperparameters. Hyperparameter tuning becomes much \\nsimpler and more organized, allowing you to run a complex script that \\nsearches over several different hyperparameters at once and log all of the \\nresults using MLFlow.',\n",
       " 'simpler and more organized, allowing you to run a complex script that \\nsearches over several different hyperparameters at once and log all of the \\nresults using MLFlow.\\nWith all the benefits that MLFlow and the corresponding MLOps \\nprinciples offer to machine learning enthusiasts of all professions, there \\nreally are no downsides to integrating it into current work environments. \\nWith that, we hope you enjoy the rest of the book!InTrodu CTIon1© Sridhar Alla, Suman Kalyan Adari 2021 \\nS. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \\nhttps://doi.org/10.1007/978-1-4842-6549-9_1CHAPTER 1\\nGetting Started: Data \\nAnalysis\\nIn this chapter, we will go over the premise of the problem we are attempting \\nto solve with the machine learning solution we want to operationalize. We',\n",
       " 'Getting Started: Data \\nAnalysis\\nIn this chapter, we will go over the premise of the problem we are attempting \\nto solve with the machine learning solution we want to operationalize. We \\nwill also begin data analysis and feature engineering of our data set.\\n Introduction and\\xa0Premise\\nWelcome to Beginning MLOps with MLFlow ! In this book, we will be taking \\nan example problem, developing a machine learning solution to it, and \\noperationalizing our model on AWS SageMaker, Microsoft Azure, Google \\nCloud, and Datarobots. The problem we will be looking at is the issue of \\nperforming anomaly detection on a credit card data set. In this chapter, we \\nwill explore this data set and show the overall structure while explaining a \\nfew techniques on analyzing this data. This data set can be found at',\n",
       " 'will explore this data set and show the overall structure while explaining a \\nfew techniques on analyzing this data. This data set can be found at  \\nwww.kaggle.com/mlg-ulb/creditcardfraud .\\nIf you are already familiar with how to analyze data and build machine \\nlearning models, feel free to grab the data set and skip ahead to 3 to jump \\nright into MLOps.2Otherwise, we will first go over the general process of how machine \\nlearning solutions are generally created. The process goes something  \\nlike this:\\n 1. Identification of the problem:  First of all, you need \\nto have an idea of what the problem is, what can be \\ndone about it, what has been done about it, and why \\nit is a problem worth solving.\\nHere’s an example of a problem: an invasive snake',\n",
       " 'to have an idea of what the problem is, what can be \\ndone about it, what has been done about it, and why \\nit is a problem worth solving.\\nHere’s an example of a problem: an invasive snake \\nspecies harmful to the local environment has \\ninfested a region. This species is highly venomous \\nand looks very similar to a harmless species of snake \\nnative to this same environment. Furthermore, \\nthe invasive species is destructive to the local \\nenvironment and is outcompeting the local species.\\nIn response, the local government has issued a \\nstatement encouraging citizens to go out and kill \\nthe venomous, invasive species on sight, but the \\nproblem is that it turns out citizens have been killing \\nthe local species as well due to how easy it is to \\nconfuse the two species.',\n",
       " 'the venomous, invasive species on sight, but the \\nproblem is that it turns out citizens have been killing \\nthe local species as well due to how easy it is to \\nconfuse the two species.\\nWhat can be done about this? A possible solution \\nis to use the power of machine learning and build \\nan application to help citizens identify the snake \\nspecies. What has been done about it? Perhaps \\nsomeone released an app that does a poor job at \\ndistinguishing the two species, which doesn’t help \\nremedy the current situation. Perhaps fliers have \\nbeen given out, but it can be hard to identify every \\nmember of a species correctly based on just one \\npicture.Chapter 1  Gettin G Started: data analy SiS3Why is it a problem worth solving? The native \\nspecies is important to the local environment.',\n",
       " 'member of a species correctly based on just one \\npicture.Chapter 1  Gettin G Started: data analy SiS3Why is it a problem worth solving? The native \\nspecies is important to the local environment. \\nKilling the wrong species can end up exacerbating \\nthe situation and lead to the invasive species \\nclaiming the environment over the native \\nspecies. And so building a computer vision-based \\napplication that can discern between the various \\nsnake species (and especially the two species \\nrelevant to the problem) could be a great way to help \\ncitizens get rid of the right snake species.\\n 2. Collection of data:  After you’ve identified the \\nproblem, you want to collect the relevant data. \\nIn the context of the snake species classification \\nproblem, you want to find images of various snake',\n",
       " 'problem, you want to collect the relevant data. \\nIn the context of the snake species classification \\nproblem, you want to find images of various snake \\nspecies in your region. The location depends on \\nhow big of a scale your project will operate on. Is it \\ngoing to identify any snake in the world? Just snakes \\nin Florida?\\nIf you can afford to do so, the more data you collect, \\nthe better the potential training outcomes will be. \\nMore training examples can introduce increased \\nvariety to your model, making it better in the long \\nrun. Deep learning models scale in performance with \\nlarge volumes of data, so keep that in mind as well.\\n 3. Data analysis:  Once you’ve collected all the raw \\ndata, you want to clean it up, process it, and format',\n",
       " 'large volumes of data, so keep that in mind as well.\\n 3. Data analysis:  Once you’ve collected all the raw \\ndata, you want to clean it up, process it, and format \\nit in a way that allows you to analyze the data better.\\nFor images, this could be something like applying an \\nalgorithm to crop out unnecessary parts of the image \\nto focus solely on the snake. Additionally, maybe Chapter 1  Gettin G Started: data analy SiS4you want to center-crop the image to remove all the \\nextra visual information in the data sample. Either \\nway, raw image data is rarely ever in good enough \\ncondition to be used directly; it almost always \\nrequires processing to get the relevant data you want.\\nFor unstructured data like images, formatting this \\ndata in a way good enough to analyze it could be',\n",
       " 'requires processing to get the relevant data you want.\\nFor unstructured data like images, formatting this \\ndata in a way good enough to analyze it could be \\nsomething like creating a directory with all of the \\nrespective snake species and the relevant image \\ndata. From there, you can look at the count of \\nimages for each snake species class that you have \\nand determine if you need to retrieve more samples \\nfor a particular species or not.\\nFor structured data, say the credit-card data set, \\nprocessing the raw data can mean something like \\ngetting rid of any entries with null values in them. \\nFormatting them in a way so you can analyze \\nthem better can involve dimensionality-reduction \\ntechniques such as principal component analysis \\n(PCA). Note: It turns out that most of the data in the',\n",
       " 'Formatting them in a way so you can analyze \\nthem better can involve dimensionality-reduction \\ntechniques such as principal component analysis \\n(PCA). Note: It turns out that most of the data in the \\ncredit card data set has actually been processed with \\nPCA in part to preserve the privacy of the users the \\ndata has been extracted from.\\nAs for the analysis, you can construct multiple \\ngraphs of different features to get an idea of the \\noverall distribution and how the features look \\nplotted against each other. This way, you can see any \\nsignificant relationships between certain features \\nthat you might keep in mind when creating your \\ntraining data.Chapter 1  Gettin G Started: data analy SiS5There are some tools you can use in order to find',\n",
       " 'that you might keep in mind when creating your \\ntraining data.Chapter 1  Gettin G Started: data analy SiS5There are some tools you can use in order to find \\nout what features have the greatest influence on \\nthe label, such as phi-k correlation . By allowing \\nyou to see the different correlation values between \\nthe individual features and the target label, you can \\ngain a deeper understanding of the relationships \\nbetween the features in this data set. If needed, you \\ncan also drop features that aren’t very influential \\nfrom the data. In this step, you really want to get a \\nsolid understanding of your data so you can apply a \\nmodel architecture that is most suitable for it.\\n 4. Feature engineering and data processing:  Now you \\ncan use the knowledge you gained from analyzing',\n",
       " 'model architecture that is most suitable for it.\\n 4. Feature engineering and data processing:  Now you \\ncan use the knowledge you gained from analyzing \\nthe various features and their relationships to each \\nanother to potentially construct new features from \\ncombinations of several existing ones. For example, \\nthe Titanic data set is a great example that you can \\napply feature engineering to. In this case, you can take \\ninformation such as class, age, fare, number of siblings, \\nnumber of parents, and so on to create as many \\nfeatures as you can think up.\\nFeature engineering is really about giving your \\nmodel a deeper context so it can learn the task \\nbetter. You don’t necessarily want to create random \\nfeatures for the sake of it, but something that’s',\n",
       " 'model a deeper context so it can learn the task \\nbetter. You don’t necessarily want to create random \\nfeatures for the sake of it, but something that’s \\npotentially relevant like number of female relatives, \\nfor example. (Since females were more likely \\nto survive the sinking of the Titanic, could it be \\npossible that if a person had more female relatives, \\nthey were less likely to survive as preference was \\ngiven to their female relatives instead?)Chapter 1  Gettin G Started: data analy SiS6The next step after feature engineering is data \\nprocessing, which is a step involving all preparations \\nmade to process the data to be passed into the model. \\nIn the context of the snake species image data, this \\ncould involve normalizing all the values to be between',\n",
       " 'made to process the data to be passed into the model. \\nIn the context of the snake species image data, this \\ncould involve normalizing all the values to be between \\n0 and 1 as well as “batching” the data into groups.\\nThis step also usually creates several subsets of \\nyour initial data: a training data set , a testing data \\nset, and a validation data set . We will go into more \\ndetail on the purpose of each of these data sets \\nlater. For now, a training data set  contains the data \\nyou want the model to learn from, the testing data \\nset contains data you want to evaluate the model’s \\nperformance on, and the validation data set  is \\nused to either select a model or help tune a model’s \\nhyperparameters to draw out a better performance.\\n 5. Build the model:  Now that the data processing',\n",
       " 'used to either select a model or help tune a model’s \\nhyperparameters to draw out a better performance.\\n 5. Build the model:  Now that the data processing \\nis done, this step is all about selecting the proper \\narchitecture and building the model. For the snake \\nspecies image data, a good choice would be to use a \\nconvolutional neural network (CNN) because they \\nwork very well for any tasks involving images. From \\nthere, it is up to you to define the specific architecture \\nof the model with respect to its layer composition.\\n 6. Training, evaluating, and validating:  When you’re \\ntraining your CNN model, you’re usually passing in \\nbatches of data until the entire data makes a full pass \\nthrough the model. From the results of this “forward',\n",
       " 'training your CNN model, you’re usually passing in \\nbatches of data until the entire data makes a full pass \\nthrough the model. From the results of this “forward \\npass, ” calculations are made that tell the model how to \\nadjust the weights as they are made going backwards \\nacross the network in what’s called the “backward Chapter 1  Gettin G Started: data analy SiS7pass. ” The training process is essentially where the \\nmodel learns how to perform the task and gets better \\nat it the more examples it sees.\\nAfter the training process, either the evaluation step \\nor the validation step can come next. As long as the \\ntesting set and validation set come from different \\ndistributions (the validation set can be derived from \\nthe training set, while the testing set can be derived',\n",
       " 'testing set and validation set come from different \\ndistributions (the validation set can be derived from \\nthe training set, while the testing set can be derived \\nfrom the original data), the model is technically seeing \\nnew data in the evaluation and validation processes. \\nThe model will never learn anything from the \\nevaluation data, so you can test your model anytime.\\nModel evaluation is where the model’s performance \\nmetrics such as accuracy, precision, recall, and so on are \\nevaluated on a data set that it has never seen before. We \\nwill go into more detail on the evaluation step once it \\nbecomes more relevant in the next chapter, Chapter 2.\\nDepending on the context, the exact purpose of \\nvalidation can differ, along with the question of',\n",
       " 'becomes more relevant in the next chapter, Chapter 2.\\nDepending on the context, the exact purpose of \\nvalidation can differ, along with the question of \\nwhether or not evaluation should be performed first \\nafter training. Let’s define several sample scenarios \\nwhere you would use validation:\\n• Selecting a model architecture:  Of several \\nmodel types or architectures, you use k-fold \\ncross-validation, for example, to quickly train and \\nevaluate each of the models on some data partition \\nof the validation set to get an idea of how they are \\nperforming. This way, you can get a good idea of \\nwhich model is performing best, allowing you to pick \\na model and continue with the rest of the process.Chapter 1  Gettin G Started: data analy SiS8• Selecting the best model:  Of several trained',\n",
       " 'which model is performing best, allowing you to pick \\na model and continue with the rest of the process.Chapter 1  Gettin G Started: data analy SiS8• Selecting the best model:  Of several trained \\nmodels, you can use something like k-fold cross-  \\nvalidation to quickly evaluate each model on the \\nvalidation data to allow you to get an idea of which \\nones are performing best.\\n• Tuning hyperparameters:  Quickly train a model \\nand test it with different hyperparameter setups to \\nget an idea of which configurations work better. You \\ncan start with a broad range of hyperparameters. \\nFrom there, you can use the results to narrow \\nthe range of hyperparameters until you get to a \\nconfiguration where you are satisfied. Models \\nin deep learning, for example, can have many',\n",
       " 'From there, you can use the results to narrow \\nthe range of hyperparameters until you get to a \\nconfiguration where you are satisfied. Models \\nin deep learning, for example, can have many \\nhyperparameters, so using validation to tune those \\nhyperparameters can work well in deep learning \\nsettings. Just beware of diminishing returns. After a \\ncertain precision with the hyperparameter setting, \\nyou’re not going to see that big of a performance \\nboost in the model.\\n• Indication of high variance:  This validation data \\nis slightly different from the other three examples. \\nIn the case of neural networks, this data is derived \\nfrom a small split of the training data. After one full \\npass of the training data, the model evaluates on \\nthis validation data to calculate metrics such as loss',\n",
       " 'from a small split of the training data. After one full \\npass of the training data, the model evaluates on \\nthis validation data to calculate metrics such as loss \\nand accuracy.\\nIf your training accuracy is high and training loss \\nis low, but the validation accuracy is low and the \\nvalidation loss is high, that’s an indication that \\nyour model suffers from high variance . What Chapter 1  Gettin G Started: data analy SiS9this means is that your model has not learned to \\ngeneralize what it is “learning” to new data, as the \\nvalidation data in this case is comprised of data it \\nhas never seen before. In other words, your model \\nis overfitting . The model just isn’t recreating the \\nkind of performance it gets on the training data on \\nnew data that it hasn’t seen before.',\n",
       " 'has never seen before. In other words, your model \\nis overfitting . The model just isn’t recreating the \\nkind of performance it gets on the training data on \\nnew data that it hasn’t seen before.\\nIf your model has poor training accuracy and high \\ntraining loss, then your model suffers from high \\nbias , meaning it isn’t learning how to perform the \\ntask correctly on the training data at all.\\nThis little validation split during the training \\nprocess can give you an early indication of when \\noverfitting is occurring.\\n 7. Predicting:  Once the model has been trained, \\nevaluated, and validated, it is then ready to make \\npredictions. In the context of the snake species \\ndetector, this step involves passing in visual images \\nof the snake in question to get some prediction back.',\n",
       " 'predictions. In the context of the snake species \\ndetector, this step involves passing in visual images \\nof the snake in question to get some prediction back. \\nFor example, if the model is supposed to detect \\nthe snake, draw a box around it, and label it (in an \\nobject detection task), it will do so and display the \\nresults in real time in the application.\\nIf it just classifies the snake in the picture, the user \\nsimply sends their photo of a snake to the model \\n(via the application) to get a species classification \\nprediction along with perhaps a probability \\nconfidence score.\\nHopefully now you have a better idea of what goes on when creating \\nmachine learning solutions.Chapter 1  Gettin G Started: data analy SiS10With all that in mind, let’s get started on the example, where you will',\n",
       " 'machine learning solutions.Chapter 1  Gettin G Started: data analy SiS10With all that in mind, let’s get started on the example, where you will \\nuse the credit card data set to build simple anomaly detection models \\nusing the data.\\n Credit Card Data Set\\nBefore you perform any data analysis, you need to first collect your data. \\nOnce again, the data set can be found at the following link:  www.kaggle.\\ncom/mlg-ulb/creditcardfraud .\\nFollowing the link, you should see something like the following in \\nFigure\\xa0 1-1.\\nFrom here, you want to download the data set by clicking the \\nDownload (144 MB) button next to New Notebook. It should take you to \\na sign-in page if you’re not already signed in, but you should be able to \\ndownload the data set after that.',\n",
       " 'Download (144 MB) button next to New Notebook. It should take you to \\na sign-in page if you’re not already signed in, but you should be able to \\ndownload the data set after that.\\nFigure 1-1.  Kaggle website page on the credit card dataChapter 1  Gettin G Started: data analy SiS11Once the zip file finishes downloading, simply extract it somewhere \\nto reveal the credit card data set. Now let’s open up Jupyter and explore \\nthis data set. Before you start this step, let’s go over the exact packages and \\ntheir versions:\\n• Python 3.6.5\\n• numpy 1.18.3\\n• pandas 0.24.2\\n• matplotlib 3.2.1\\nTo check your package versions, you can run a command like\\npip show package_name\\nAlternatively, you can run the following code to display the version in \\nthe notebook itself:\\nimport module_name',\n",
       " \"To check your package versions, you can run a command like\\npip show package_name\\nAlternatively, you can run the following code to display the version in \\nthe notebook itself:\\nimport module_name\\nprint(module_name.__version__)\\nIn this case, module_name  is the name of the package you’re importing, \\nsuch as numpy.\\n Loading the\\xa0Data Set\\nLet’s begin! First, open a new notebook and import all of the dependencies \\nand set global parameters for this notebook:\\n%matplotlib inline\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom pylab import rcParams\\nrcParams['figure.figsize'] = 14, 8Chapter 1  Gettin G Started: data analy SiS12Refer to Figure\\xa0 1-2.\\nNow that you have imported the necessary libraries, you can load the\",\n",
       " 'from pylab import rcParams\\nrcParams[\\'figure.figsize\\'] = 14, 8Chapter 1  Gettin G Started: data analy SiS12Refer to Figure\\xa0 1-2.\\nNow that you have imported the necessary libraries, you can load the \\ndata set. In this case, the data  folder exists in the same directory as the \\nnotebook file and contains the creditcard.csv  file. Here is the code:\\ndata_path = \"data/creditcard.csv\"\\ndf = pd.read_csv(data_path)\\nRefer to Figure\\xa0 1-3.\\nFigure 1-2.  Jupyter notebook cell with some import statements \\nas well as a global parameter definition for the size of all \\nmatplotlib plotsChapter 1  Gettin G Started: data analy SiS13Now that the data frame has been loaded, let’s take a look at its contents:\\ndf.head()\\nRefer to Figure\\xa0 1-4.',\n",
       " 'matplotlib plotsChapter 1  Gettin G Started: data analy SiS13Now that the data frame has been loaded, let’s take a look at its contents:\\ndf.head()\\nRefer to Figure\\xa0 1-4.\\nFigure 1-3.  Defining the data path to the credit card data set .csv file, \\nreading its contents, and creating a pandas data frame object\\nFigure 1-4.  Calling the head() function on the data frame to display \\nthe first five rows of the data frameChapter 1  Gettin G Started: data analy SiS14If you are not familiar with the df.head(n)  function, it essentially \\nprints the first n rows of the data frame. If you did not pass any arguments, \\nlike in the figure above, then the function defaults to a value of five, \\nprinting the first five rows of the data frame.',\n",
       " 'prints the first n rows of the data frame. If you did not pass any arguments, \\nlike in the figure above, then the function defaults to a value of five, \\nprinting the first five rows of the data frame.\\nFeel free to play around with that function as well as use the scroll bar \\nto explore the rest of the features.\\nNow, let’s look at some basic statistical values relating to the values in \\nthis data frame:\\ndf.describe()\\nRefer to Figure\\xa0 1-5.\\nFeel free to scroll right and look at the various statistics for the rest of \\nthe columns. As you can see in Figure\\xa0 1-5, the function generates statistical \\nsummaries for data in each of the columns in the data frame.\\nThe main takeaway here is that there are a huge number of data points.',\n",
       " 'summaries for data in each of the columns in the data frame.\\nThe main takeaway here is that there are a huge number of data points. \\nIn fact, you can check the shape of the data frame by simply calling\\ndf.shape\\nRefer to Figure\\xa0 1-6.\\nFigure 1-5.  Calling the describe() function on the data frame to get \\nstatistical summaries of the data in each columnChapter 1  Gettin G Started: data analy SiS15There are 284,807 rows and 31 columns in this data frame. That’s a lot \\nof entries! Not only that, but if you look at Figure\\xa0 1-5, you’ll see that the \\nvalues can get really large for the column Time . In fact, keep scrolling right, \\nand you’ll see that values can get very large for the column Amount  as well. \\nRefer to Figure\\xa0 1-7.',\n",
       " 'values can get really large for the column Time . In fact, keep scrolling right, \\nand you’ll see that values can get very large for the column Amount  as well. \\nRefer to Figure\\xa0 1-7.\\nAs you can see, there are at least two columns with very large values. \\nWhat this tells you is that later on, when building the various data sets for \\nthe model training process, you definitely need to scale down the data. \\nOtherwise, such large data values can potentially mess up the training \\nprocess.\\nFigure 1-6.  Calling the shape() function on the data frame to get an \\noutput in the format (number_of_rows, number_of_columns)\\nFigure 1-7.  Scrolling right in the output of the describe function \\nreveals that the maximum data value in the column Amount is also',\n",
       " 'output in the format (number_of_rows, number_of_columns)\\nFigure 1-7.  Scrolling right in the output of the describe function \\nreveals that the maximum data value in the column Amount is also \\nvery large, just like the maximum data value in the column TimeChapter 1  Gettin G Started: data analy SiS16 Normal Data and\\xa0Fraudulent Data\\nSince there are only two classes, normal  and fraud , let’s split up the data \\nframe by class and continue with the data analysis. In the context of \\nanomaly detection, the fraud  class is also the anomaly  class, hence why \\nwe chose to name the data frame representing fraudulent transaction \\ndata anomalies  and interchangeably refer to this class as either fraud  or \\nanomaly .\\nHere is the code:\\nanomalies = df[df.Class == 1]\\nnormal = df[df.Class == 0]',\n",
       " 'data anomalies  and interchangeably refer to this class as either fraud  or \\nanomaly .\\nHere is the code:\\nanomalies = df[df.Class == 1]\\nnormal = df[df.Class == 0]\\nAfter that, run the following in a separate cell:\\nprint(f\"Anomalies: {anomalies.shape}\")\\nprint(f\"Normal: {normal.shape}\")\\nRefer to Figure\\xa0 1-8.\\nFrom here, you can see that the data is overwhelmingly biased towards \\nnormal data, and that anomalies only comprise a vast minority of data \\npoints in the overall data set. What this tells you is that you will have to \\ncraft the training, evaluation, and validation sets more carefully so each of \\nthese sets will have a good representation of anomaly data.\\nFigure 1-8.  Defining data frames for fraudulent/anomalous data',\n",
       " 'these sets will have a good representation of anomaly data.\\nFigure 1-8.  Defining data frames for fraudulent/anomalous data \\nand for normal data and printing their shapesChapter 1  Gettin G Started: data analy SiS17In fact, let’s look at this disparity in a graphical manner just to see how \\nlarge the difference is:\\nclass_counts = pd.value_counts(df[\\'Class\\'], sort = True)\\nclass_counts.plot(kind = \\'bar\\', rot=0)\\nplt.title(\"Class Distribution\")\\nplt.xticks( range(2), [\"Normal\", \"Anomaly\"])\\nplt.xlabel(\"Label\")\\nplt.ylabel(\"Counts\")\\nRefer to Figure\\xa0 1-9.\\nThe graph visually shows the immense difference between the number \\nof data values of the two classes.\\nFigure 1-9.  A graph visually demonstrating the difference in counts',\n",
       " 'Refer to Figure\\xa0 1-9.\\nThe graph visually shows the immense difference between the number \\nof data values of the two classes.\\nFigure 1-9.  A graph visually demonstrating the difference in counts \\nfor normal data and anomalous dataChapter 1  Gettin G Started: data analy SiS18So now you can begin analyzing some of the characteristics of data \\npoints in each class. First of all, the columns in this data set are Time , \\nvalues V1 through V28, Amount , and Class .\\nSo, do anomalous data values comprise transactions with excessive \\namounts? Let’s look at some statistical summary values for Amount :\\nanomalies.Amount.describe()\\nRefer to Figure\\xa0 1-10  for the output.\\nIt seems like the data is skewed right, and that anomalous transactions',\n",
       " 'anomalies.Amount.describe()\\nRefer to Figure\\xa0 1-10  for the output.\\nIt seems like the data is skewed right, and that anomalous transactions \\ncomprise values that are not very high. In fact, most of the transactions are less \\nthan $100, so it’s not like fraudulent transactions are high-value transactions.\\nnormal.Amount.describe()\\nRefer to Figure\\xa0 1-11  for the output.\\nFigure 1-10.  Output of the describe() function on the data frame for \\nfradulent values for the column Amount\\nFigure 1-11.  Output of the describe() function on the data frame for \\nnormal values for the column AmountChapter 1  Gettin G Started: data analy SiS19If you look at the normal data, it’s even more skewed right than the \\nanomalies. Most of the transactions are below $100, and some of the',\n",
       " 'anomalies. Most of the transactions are below $100, and some of the \\namounts can get very high to values like $25,000.\\n Plotting\\nLet’s now turn to a graphical approach to help visually illustrate this better. \\nFirst, you define some functions to help plot the various columns of the \\ndata to make it much easier to visualize the various relationships:\\ndef plot_histogram(df, bins, column, log_scale= False):\\n    bins = 100\\n    anomalies = df[df.Class == 1]\\n    normal = df[df.Class == 0]\\n    fig, (ax1, ax2) = plt.subplots(2, 1, sharex= True)\\n    fig.suptitle(f\\'Counts of {column} by Class\\')\\n    ax1.hist(anomalies[column], bins = bins, color=\"red\")\\n    ax1.set_title(\\'Anomaly\\')\\n    ax2.hist(normal[column], bins = bins, color=\"orange\")\\n    ax2.set_title(\\'Normal\\')\\n    plt.xlabel(f\\'{column}\\')',\n",
       " 'ax1.hist(anomalies[column], bins = bins, color=\"red\")\\n    ax1.set_title(\\'Anomaly\\')\\n    ax2.hist(normal[column], bins = bins, color=\"orange\")\\n    ax2.set_title(\\'Normal\\')\\n    plt.xlabel(f\\'{column}\\')\\n    plt.ylabel(\\'Count\\')\\n    if log_scale:\\n        plt.yscale(\\'log\\')\\n    plt.xlim((np.min(df[column]), np.max(df[column])))\\n    plt.show()Chapter 1  Gettin G Started: data analy SiS20def plot_scatter(df, x_col, y_col, sharey = False):\\n    anomalies = df[df.Class == 1]\\n    normal = df[df.Class == 0]\\n     fig, (ax1, ax2) = plt.subplots(2, 1, sharex= True, \\nsharey=sharey)\\n    fig.suptitle(f\\'{y_col} over {x_col} by Class\\')\\n     ax1.scatter(anomalies[x_col], anomalies[y_col], color=\\'red\\')\\n    ax1.set_title(\\'Anomaly\\')\\n    ax2.scatter(normal[x_col], normal[y_col], color=\\'orange\\')',\n",
       " 'ax1.scatter(anomalies[x_col], anomalies[y_col], color=\\'red\\')\\n    ax1.set_title(\\'Anomaly\\')\\n    ax2.scatter(normal[x_col], normal[y_col], color=\\'orange\\')\\n    ax2.set_title(\\'Normal\\')\\n    plt.xlabel(x_col)\\n    plt.ylabel(y_col)\\n    plt.show()\\nRefer to Figure\\xa0 1-12  to see the code in cells.\\nFigure 1-12.  Each of the plotter functions in their own Jupyter cellsChapter 1  Gettin G Started: data analy SiS21Now, let’s start by plotting values for Amount  by Class  for the entire \\ndata frame:\\nplt.scatter(df.Amount, df.Class)\\nplt.title(\"Transaction Amounts by Class\")\\nplt.ylabel(\"Class\")\\nplt.yticks( range(2), [\"Normal\", \"Anomaly\"])\\nplt.xlabel(\"Transaction Amounts ($)\")\\nplt.show()\\nRefer to Figure\\xa0 1-13 .\\nIt seems like there are some massive outliers in the normal data set, as',\n",
       " 'plt.yticks( range(2), [\"Normal\", \"Anomaly\"])\\nplt.xlabel(\"Transaction Amounts ($)\")\\nplt.show()\\nRefer to Figure\\xa0 1-13 .\\nIt seems like there are some massive outliers in the normal data set, as \\nsuspected. However, the graph isn’t very informative in telling you about \\nFigure 1-13.  A scatterplot of data values in the data frame \\nencompassing all the data values. The plotted columns are Amount \\non the x-axis and Class on the y-axisChapter 1  Gettin G Started: data analy SiS22value counts, so let’s use the plotting functions defined earlier to draw \\ngraphs that provide more context:\\nbins = 100\\nplot_histogram(df, bins, \"Amount\", log_scale= True)\\nRefer to Figure\\xa0 1-14 .\\nFrom this, you can definitely notice a right skew as well as the massive',\n",
       " 'graphs that provide more context:\\nbins = 100\\nplot_histogram(df, bins, \"Amount\", log_scale= True)\\nRefer to Figure\\xa0 1-14 .\\nFrom this, you can definitely notice a right skew as well as the massive \\noutliers present in the normal data. Since you can’t really see much of the \\nanomalies, let’s create another plot:\\nplt.hist(anomalies.Amount, bins = bins, color=\"red\")\\nplt.show()\\nFigure 1-14.  A histogram of counts for data values organized into \\nintervals in the column Amount in the data frame. The number of \\nbins is 100, meaning the interval of each bar in the histogram is the \\nrange of the data in the column Amount divided by the number of binsChapter 1  Gettin G Started: data analy SiS23Refer to Figure\\xa0 1-15 .\\nThe anomalies seem to be right skewed as well, but much more heavily',\n",
       " 'The anomalies seem to be right skewed as well, but much more heavily \\nso. This means that the majority of anomalous transactions actually have \\nquite low transaction amounts.\\nAlright, so what about time? Let’s plot another basic scatterplot:\\nplt.scatter(df.Time, df.Class)\\nplt.title(\"Transactions over Time by Class\")\\nplt.ylabel(\"Class\")\\nplt.yticks( range(2), [\"Normal\", \"Anomaly\"])\\nplt.xlabel(\"Time (s)\")\\nplt.show()\\nRefer to Figure\\xa0 1-16 .\\nFigure 1-15.  A histogram of just the values in the anomaly data \\nframe for the column Amount. The number of bins is also 100 here, as \\nit will be for the rest of the examplesChapter 1  Gettin G Started: data analy SiS24This graph isn’t very informative, but it does tell you that fraudulent',\n",
       " 'it will be for the rest of the examplesChapter 1  Gettin G Started: data analy SiS24This graph isn’t very informative, but it does tell you that fraudulent \\ntransactions are pretty spread out over the entire timeline. Once again, let’s \\nuse the plotter functions to get an idea of the counts:\\nplot_scatter(df, \"Time\", \"Amount\")\\nRefer to Figure\\xa0 1-17 .\\nFigure 1-16.  A scatterplot for values in the data frame df with data \\nin the column Time on the x-axis and data in the column Class in the \\ny-axisChapter 1  Gettin G Started: data analy SiS25You have a better context now, but it doesn’t seem to tell you much. \\nYou can see that fraudulent transactions occur throughout the entire \\ntimeline and that there is no specific period of time when it seems like',\n",
       " 'You can see that fraudulent transactions occur throughout the entire \\ntimeline and that there is no specific period of time when it seems like \\nhigher-value transactions occur. There do seem to be two main clusters, \\nbut this could also be a result of the lack of data points compared to the \\nnormal points.\\nLet’s now look at the histogram to take into account frequencies:\\nplot_scatter(df, \"Time\", \"Amount\")\\nRefer to Figure\\xa0 1-18 .\\nFigure 1-17.  Using the plot_scatter() function to plot data values for \\nthe columns Time on the x-axis and Amount on the y-axis in the df \\ndata frameChapter 1  Gettin G Started: data analy SiS26From this, you get a really good context of the amount of fraudulent/\\nanomalous transactions going on over time. For the normal data, it seems',\n",
       " 'data frameChapter 1  Gettin G Started: data analy SiS26From this, you get a really good context of the amount of fraudulent/\\nanomalous transactions going on over time. For the normal data, it seems \\nthat they occur in waves. For the anomalies, there doesn’t seem to be a \\nparticular peak time; they just occur throughout the entire timespan.\\nIt does appear that that they have defined spikes near the start of \\nthe first transaction, and that some of the spikes do occur where normal \\ntransactions are in the “trough” of the wave pattern shown. However, \\na good portion of the fraudulent transactions still occur where normal \\ntransactions are at a maximum.\\nSo what does the data for the other columns look like? Let’s look at \\nsome interesting plots for V1:\\nplot_histogram(df, bins, \"V1\")',\n",
       " 'transactions are at a maximum.\\nSo what does the data for the other columns look like? Let’s look at \\nsome interesting plots for V1:\\nplot_histogram(df, bins, \"V1\")\\nRefer to Figure\\xa0 1-19 .\\nFigure 1-18.  Using the plot_histogram() function to plot data values \\nfor the column Time in the df data frameChapter 1  Gettin G Started: data analy SiS27Here, you can see a clear difference in the distribution of points for \\neach class over the same V1 values. The range of values that the fraudulent \\ntransactions encompass extend well into the values for V1. Let’s keep \\nexploring, looking at how the values for Amount  relate to V1:\\nplot_scatter(df, \"Amount\", \"V1\", sharey= True)\\nWhat the sharey  parameter does is it forces both subplots to share',\n",
       " 'exploring, looking at how the values for Amount  relate to V1:\\nplot_scatter(df, \"Amount\", \"V1\", sharey= True)\\nWhat the sharey  parameter does is it forces both subplots to share \\nthe same y-axis, meaning the plots are displayed on the same scale. You \\nare specifying this so it will be easier to tell what the distribution of the \\nanomalous points looks like in comparison to the normal points. Refer to \\nFigure\\xa0 1-20 .\\nFigure 1-19.  Using the plot_histogram() function to plot the data in \\nthe column V1\\xa0in dfChapter 1  Gettin G Started: data analy SiS28From this graph, the fraudulent points don’t seem out of place \\ncompared to all of the other normal points.\\nLet’s continue and look at how time relates to the values for V1:\\nplot_scatter(df, \"Time\", \"V1\", sharey= True)\\nRefer to Figure\\xa0 1-21 .',\n",
       " 'compared to all of the other normal points.\\nLet’s continue and look at how time relates to the values for V1:\\nplot_scatter(df, \"Time\", \"V1\", sharey= True)\\nRefer to Figure\\xa0 1-21 .\\nFigure 1-20.  Using the plot_scatter() function to plot the values in the \\ncolumns Amount on the x-axis and V1 on the y-axis in dfChapter 1  Gettin G Started: data analy SiS29Other than a few defined spikes that stand out from where the normal \\npoints would have been, most of the fraudulent data in this context seems \\nto blend in with the normal data.\\nDoing this one at a time for all of the other values will be tedious, so \\nlet’s just plot them all at once using a simple script. Here is the code to plot \\nall of the frequency counts for each column from V1 to V28:\\nfor f in range(1, 29):\\n    print(f\\'V{f} Counts\\')',\n",
       " \"let’s just plot them all at once using a simple script. Here is the code to plot \\nall of the frequency counts for each column from V1 to V28:\\nfor f in range(1, 29):\\n    print(f'V{f} Counts')\\n    plot_histogram(df, bins, f'V{f}')\\nRefer to Figure\\xa0 1-22 .\\nFigure 1-21.  Using the plot_scatter() function to plot the values in the \\ncolumns Time on the x-axis and V1 on the y-axis in dfChapter 1  Gettin G Started: data analy SiS30Since the output has been minimized, hover where the bar darkens \\nand click to expand the output so you can see the graphs a lot better. Refer \\nto Figure\\xa0 1-23 .\\nNow you should see something like in Figure\\xa0 1-24 .\\nFigure 1-22.  A script to plot histograms using the plot_histogram() \\nfunction for data in each column from V1 to V28\\xa0in df\",\n",
       " 'to Figure\\xa0 1-23 .\\nNow you should see something like in Figure\\xa0 1-24 .\\nFigure 1-22.  A script to plot histograms using the plot_histogram() \\nfunction for data in each column from V1 to V28\\xa0in df\\nFigure 1-23.  Hovering over the bar to the left of the plots (it should \\ndarken and show the tooltip as shown) and clicking it to expand the \\noutputChapter 1  Gettin G Started: data analy SiS31Scrolling through, you can see a lot of interesting graphs such as \\nFigure\\xa0 1-25  and Figure\\xa0 1-26 .\\nFigure 1-24.  What the expanded output should look like. All of the \\ngraphs should display continuously, as depicted in the figureChapter 1  Gettin G Started: data analy SiS32In this case, you can see a clear differentiation between the fraudulent',\n",
       " 'graphs should display continuously, as depicted in the figureChapter 1  Gettin G Started: data analy SiS32In this case, you can see a clear differentiation between the fraudulent \\ndata and the normal data that you didn’t see in the graphs earlier. And \\nso, features such as V12 are certainly more important in helping give the \\nmodel a better context.\\nFigure 1-25.  A histogram of data for the column V12\\xa0in df. As you \\ncan see, there is a very clear deviation seen with the anomalous values \\ncompared to the normal values. Both plots share the same x-axis \\nscale, so while the counts might be very low compared to the normal \\nvalues, they are still spread out far more than the normal values for',\n",
       " 'scale, so while the counts might be very low compared to the normal \\nvalues, they are still spread out far more than the normal values for \\nthe same range of V12 column valuesChapter 1  Gettin G Started: data analy SiS33This time you can see an even bigger difference between fraudulent \\ndata and normal data. Once again, it’s features like V12 and V17 that hold \\nthe data that will help the model understand how to differentiate between \\nthe anomalies and the normal points.\\nTo minimize the output, click the same bar as earlier when you \\nexpanded the output. Let’s now look at how all of these data points vary \\naccording to time:\\nfor f in range(1, 29):\\n    print(f\\'V{f} vs Time\\')\\n    plot_scatter(df, \"Time\", f\\'V{f}\\', sharey= True)\\nOnce again, expand the output and explore the graphs. Refer to',\n",
       " 'according to time:\\nfor f in range(1, 29):\\n    print(f\\'V{f} vs Time\\')\\n    plot_scatter(df, \"Time\", f\\'V{f}\\', sharey= True)\\nOnce again, expand the output and explore the graphs. Refer to \\nFigure\\xa0 1-27  and Figure\\xa0 1-28  to see some interesting results.\\nFigure 1-26.  A histogram of data for the column V17\\xa0in df. Just like \\nwith the column V12, there is also a clear deviation seen with the \\nanomalous values compared to the normal values. This indicates \\nthat the column V17 is more likely to help the model learn how to \\ndifferentiate between normal and fraudulent transactions than some \\nof the other columns that don’t show such a devianceChapter 1  Gettin G Started: data analy SiS34Once again, with V12 you can see a significant difference between the',\n",
       " 'of the other columns that don’t show such a devianceChapter 1  Gettin G Started: data analy SiS34Once again, with V12 you can see a significant difference between the \\nanomalies and the normal data points. A good portion of the anomalies \\nremain hidden within the normal data points, but a significant amount of \\nthem can be differentiated from the rest.\\nFigure 1-27.  The scatterplot for Time on the x-axis and V12 on the \\ny-axis shows a deviation between the anomalies and the normal data \\npoints. Although a significant portion of the anomalies fall under the \\nband of normal points, there are still a good number of anomalies \\nthat fall out of that range. And so you can see that against Time, the \\ndata for the column V12 also shows this deviation from the normal',\n",
       " \"that fall out of that range. And so you can see that against Time, the \\ndata for the column V12 also shows this deviation from the normal \\ndata pointsChapter 1  Gettin G Started: data analy SiS35The difference between the anomalies and the normal points are \\nhighlighted even further when looking at V17. It seems that even in \\nrelation to time, columns V12 and V17 hold data that best help distinguish \\nfraudulent transactions from normal transactions. You can see in the \\ngraph that a few normal points are with the anomalous points as well, but \\nhopefully the model can learn the true difference taking into account all of \\nthe data.\\nFinally, let’s see the relationship between each of these columns and \\nAmount :\\nfor f in range(1, 29):\\n    print(f'Amount vs V{f}')\",\n",
       " 'the data.\\nFinally, let’s see the relationship between each of these columns and \\nAmount :\\nfor f in range(1, 29):\\n    print(f\\'Amount vs V{f}\\')\\n    plot_scatter(df,   f\\'V{f}\\', \"Amount\", sharey= True)\\nThis time there seems to be a few more graphs more clearly showing \\nthe differences between the normal and fraudulent points. Refer to \\nFigure\\xa0 1-29 , Figure\\xa0 1-30 , and Figure\\xa0 1-31 .\\nFigure 1-28.  The scatterplot for Time on the x-axis and V17 on the \\ny-axis shows a deviation between the anomalies and the normal data \\npoints. As with the values for V12, you can observe another deviation \\nbetween the normal points and the fraudulent points. In this case, the \\ndifference seems to be a bit more pronounced, as the anomalies seem',\n",
       " 'between the normal points and the fraudulent points. In this case, the \\ndifference seems to be a bit more pronounced, as the anomalies seem \\nto be more spread out than in Figure\\xa0 1-27Chapter 1  Gettin G Started: data analy SiS36The graphs from V9 through V12 all show a clear differentiation \\nbetween the anomalies and the normal points, even if a good portion \\nof the anomalies are within the cluster of normal points. One thing to \\nnote is that it may not be the same anomalies that differ each time in the \\ngraphs, allowing the model to better learn how to differentiate between the \\nanomalies and the normal points.\\nFigure 1-29.  Looking at the scatterplot for Amount on the y-axis and \\nV10 on the x-axis, you can see a pronounced deviation of fraudulent',\n",
       " 'anomalies and the normal points.\\nFigure 1-29.  Looking at the scatterplot for Amount on the y-axis and \\nV10 on the x-axis, you can see a pronounced deviation of fraudulent \\npoints from the normal points. For the relationship of the V columns \\nagainst Amount, it seems that more columns show an increased \\ndeviation compared to the earlier plots. This difference is not so large, \\nas you still see that a sizeable portion of the anomalies are within the \\nnormal data cluster. However, this still gives the model some context \\nin how a fraudulent transaction differs from a normal transactionChapter 1  Gettin G Started: data analy SiS37You can once again see that V12 consistently differentiates between \\nanomalies and normal data. However, there is still the problem of a good',\n",
       " 'anomalies and normal data. However, there is still the problem of a good \\nportion of the anomalies staying hidden within the normal data cluster.\\nFigure 1-30.  A scatterplot for the column Amount on the y-axis and \\nV12 on the x-axis. Once again, you can see a pronounced deviation of \\nfraudulent points from the normal points. In this case, the majority of \\nfraudulent points seem to deviate from the normal point cluster. You \\ncan also see that there is a band of normal points far from the main \\ncluster, and that the band coincides with the anomalous data points. \\nIt is a possible reason to keep in mind if the model classifies points like \\nthese as anomaliesChapter 1  Gettin G Started: data analy SiS38You can also see that this differentiation between normal points and',\n",
       " 'these as anomaliesChapter 1  Gettin G Started: data analy SiS38You can also see that this differentiation between normal points and \\nfraudulent points holds for V17 looking at transaction amounts.\\nYou could also look at the data for each of the V columns and plot them \\nagainst each other, but that’s more useful to help identify precise changes \\nin trends that will be more useful to know if you want to further train the \\nmodel to improve its performance on the new data. First of all, it’s possible \\nthat not every feature is very significant. So, if trends do shift, it does not \\nnecessarily mean that the model’s performance will be downgraded.\\nThorough analysis of the data helps data scientists get a much better \\nunderstanding of how the various data columns relate to each other and',\n",
       " 'Thorough analysis of the data helps data scientists get a much better \\nunderstanding of how the various data columns relate to each other and \\nlets them identify if trends are shifting over time. As data is continuously \\ncollected over time, data biases and trends are bound to shift. So perhaps \\na year from now, it’s the column V18 that shows profound differences \\nbetween anomalous points and normal points, and V17 now shows that \\nmost anomalous points are contained within the cluster of normal points.\\nFigure 1-31.  A scatterplot for the column Amount on the y-axis and \\nV17 on the x-axis. Just as with Figure\\xa0 1-30 , you can see a deviation \\nagain of fraudulent points from the normal point cluster. Once again, \\nthe majority of fraudulent points show this deviation, but you can also',\n",
       " 'again of fraudulent points from the normal point cluster. Once again, \\nthe majority of fraudulent points show this deviation, but you can also \\nsee some normal points that coincide with these anomalous pointsChapter 1  Gettin G Started: data analy SiS39 Summary\\nData analysis is a crucial step in the process of creating a machine learning \\nsolution. Not only does it determine the type of model and influence the \\nset of features that will be selected for the training process, but it also helps \\nidentify any changes in trends over time that may signify that the model \\nneeds to be further trained. You explored and analyzed the data in the \\ncredit card data set, generated many plots to get an idea of the relationship \\nbetween the two plotted variables, and identified some features that',\n",
       " 'credit card data set, generated many plots to get an idea of the relationship \\nbetween the two plotted variables, and identified some features that \\ndistinguish between normal points and anomalies. In the next chapter, you \\nwill process the data to create various subsets to help train several types of \\nmachine learning models.Chapter 1  Gettin G Started: data analy SiS41© Sridhar Alla, Suman Kalyan Adari 2021 \\nS. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \\nhttps://doi.org/10.1007/978-1-4842-6549-9_2CHAPTER 2\\nBuilding Models\\nIn this chapter, we will go over how to build a simple logistic regression \\nmodel in both scikit-learn and PySpark. We will also go over the process of \\nk-fold cross validation to tune a hyperparameter in scikit-learn.\\n Introduction',\n",
       " 'model in both scikit-learn and PySpark. We will also go over the process of \\nk-fold cross validation to tune a hyperparameter in scikit-learn.\\n Introduction\\nIn the previous chapter, you loaded the credit card data set and analyzed \\nthe distribution of its data. You also looked at the relationships between \\nthe features and got a general idea of how heavily they influence the labels.\\nNow that you’ve gained a better understanding of the data set, you \\nwill proceed with building the models themselves. You will be using the \\nsame credit card data set as in the previous chapter. In this chapter, you \\nwill look at two frameworks: scikit-learn , and PySpark . The models you \\nbuild in scikit-learn and in PySpark will stay relevant for the rest of the',\n",
       " 'will look at two frameworks: scikit-learn , and PySpark . The models you \\nbuild in scikit-learn and in PySpark will stay relevant for the rest of the \\nbook, as you will be using both of them later on when you host them on \\ncloud services to make predictions. You will keep it simple and construct \\nlogistic regression models in these two frameworks. Since the input data \\nformat is different for these two frameworks, you can’t just conduct the \\ndata processing in advance and use those train/test/validate sets for these \\ntwo frameworks. However, it is possible to do so for scikit-learn and Keras, \\nfor example, depending on how the last layer is constructed in the Keras \\nmodel.42You will be performing the validation step with the scikit-learn model',\n",
       " 'for example, depending on how the last layer is constructed in the Keras \\nmodel.42You will be performing the validation step with the scikit-learn model \\nto tune a hyperparameter. Hyperparameters  can be thought of as model-  \\nrelated parameters that influence the training process and result.\\nThat being said, let’s get started with scikit-learn and build a logistic \\nregression model. One thing to note is that we will provide a lot of \\ncommentary in the scikit-learn model that we may skip over in the PySpark \\nexample, so be sure to at least read through the process for scikit-learn to \\nget a general idea of how train-test-validate works.\\n Scikit-Learn\\nBefore we get started, here are the packages and their versions that you',\n",
       " 'get a general idea of how train-test-validate works.\\n Scikit-Learn\\nBefore we get started, here are the packages and their versions that you \\nwill need. We will provide an easy way for you to check the versions of your \\npackages within the code itself.\\nHere are the versions of our configuration:\\n• Python 3.6.5\\n• numpy 1.18.5\\n• pandas 1.1.0\\n• matplotlib 3.2.1\\n• seaborn 0.10.1\\n• sklearn 0.22.1.post1\\nIn the code below, you will find that some of the imports are \\nunnecessary, such as importing all of sklearn when you only use a bit of \\nits functionality. This is done for the purpose of displaying the version and \\nsuch statements have a # beside them.Chapter 2  Building Models43 Data Processing\\nSo now, let’s begin with the import  statements:\\nimport numpy as np\\nimport pandas as pd',\n",
       " 'such statements have a # beside them.Chapter 2  Building Models43 Data Processing\\nSo now, let’s begin with the import  statements:\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib #\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport sklearn #\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve, \\nconfusion_matrix\\nfrom sklearn.model_selection import KFold\\nprint(\"numpy: {}\".format(np.__version__))\\nprint(\"pandas: {}\".format(pd.__version__))\\nprint(\"matplotlib: {}\".format(matplotlib.__version__))\\nprint(\"seaborn: {}\".format(sns.__version__))\\nprint(\"sklearn: {}\".format(sklearn.__version__))',\n",
       " 'print(\"pandas: {}\".format(pd.__version__))\\nprint(\"matplotlib: {}\".format(matplotlib.__version__))\\nprint(\"seaborn: {}\".format(sns.__version__))\\nprint(\"sklearn: {}\".format(sklearn.__version__))\\nRefer to Figure\\xa0 2-1 to see the output.Chapter 2  Building Models44Now you can move on to loading the data. You will be using the same \\ncredit card dataset as from the previous chapter:\\ndata_path = \"data/creditcard.csv\"\\ndf = pd.read_csv(data_path)\\nRefer to Figure\\xa0 2-2 to see this code in a cell.\\nFigure 2-1.  The output showing the printed versions of the modules \\nyou will need. Some modules are imported for the sake of printing the \\nversions and have been marked with a # beside them to indicate that \\nthey are not necessary to run the code',\n",
       " 'you will need. Some modules are imported for the sake of printing the \\nversions and have been marked with a # beside them to indicate that \\nthey are not necessary to run the code\\nFigure 2-2.  Loading the data frame using pandas. The credit card \\ndata set is located in a folder called data , which is located in the same \\ndirectory as the notebook fileChapter 2  Building Models45There shouldn’t be any output from loading the data frame. To see the \\ndata frame you just loaded, call the following to ensure it has read the data \\ncorrectly:\\ndf.head()\\nYou should see something like in Figure\\xa0 2-3.\\nIf you remember from the previous chapter, there is a massive \\nimbalance in the distribution of data between the normal data and the',\n",
       " 'correctly:\\ndf.head()\\nYou should see something like in Figure\\xa0 2-3.\\nIf you remember from the previous chapter, there is a massive \\nimbalance in the distribution of data between the normal data and the \\nanomalies. Because of this, you are going to take a slightly alternative \\napproach in how you craft this data.\\nThis is where data analysis comes into play. Because you know that \\na massive disparity between the data counts in each class exists, you will \\nnow take care to specially craft the data sets so that it is ensured that a \\ngood amount of anomalies end up in each data set. If you simply select \\n100,000 data points from df, split it into your training/test/validate sets \\nand continue, it is entirely possible that very few or even no anomalies',\n",
       " '100,000 data points from df, split it into your training/test/validate sets \\nand continue, it is entirely possible that very few or even no anomalies \\nend up in one or more of those sets. At that point, you would have a lot of \\ntrouble in getting the model to properly learn this task.\\nThis is why you will be splitting up the anomalies and normal points to \\ncreate your training/test/validate sets.\\nWith that in mind, let’s create data frames for the normal points and for \\nthe fraudulent points:\\nFigure 2-3.  The output of the head() function. The data has loaded \\ncorrectly, and you can see the first five rows of the data frameChapter 2  Building Models46normal = df[df.Class == 0].sample(frac=0.5, random_state=2020).\\nreset_index(drop= True)\\nanomaly = df[df.Class == 1]',\n",
       " 'reset_index(drop= True)\\nanomaly = df[df.Class == 1]\\nYou have set the random_state  to a specific value so that the results of \\nthe random sampling should be the same no matter how many times you \\nrepeat it, helping with reproducibility. Unfortunately, given the nature of \\nhow models learn, you cannot expect to get the same results every time for \\nsomething like neural networks, for example.\\nIn the code, you filter out the respective values by class, and sample \\n50% of the entire data frame’s normal points to comprise the normal data \\nin this context.\\nRefer to Figure\\xa0 2-4 to see this code in a cell.\\nYou can add some code to check the shapes as well:\\nprint(f\"Normal: {normal.shape}\")\\nprint(f\"Anomaly: {anomaly.shape}\")\\nRefer to Figure\\xa0 2-5 for the output.',\n",
       " 'You can add some code to check the shapes as well:\\nprint(f\"Normal: {normal.shape}\")\\nprint(f\"Anomaly: {anomaly.shape}\")\\nRefer to Figure\\xa0 2-5 for the output.\\nFigure 2-5.  Printing the shapes of the normal and anomaly data \\nframes. There is a clear difference in the number of entries in the two \\ndata frames\\nFigure 2-4.  Filtering the data frame values by class to create the \\nnormal and anomaly data frames. The normal data frame contains \\n50% of all normal data points, randomly selected as determined by \\nthe seed (random_state)Chapter 2  Building Models47As you can see, there is still a big disparity between the normal points \\nand the anomalies. In the case of logistic regression, the model is still \\nable to learn how to distinguish between the two, but in the case of neural',\n",
       " 'and the anomalies. In the case of logistic regression, the model is still \\nable to learn how to distinguish between the two, but in the case of neural \\nnetworks, for example, this disparity means the model never really learns \\nhow to classify anomalies. However, as you will see later in this chapter, \\nyou can tell the model to weigh the anomalies far more in its learning \\nprocess compared to the normal points.\\nNow you can start creating the train/test/validate split. However, scikit-  \\nlearn provides functionality to create train/test splits only. To get around \\nthat, you will create train and test sets, and then split the train set again \\ninto train and validate sets.\\nFirst, you will split the data into train and test data, keeping the normal',\n",
       " 'that, you will create train and test sets, and then split the train set again \\ninto train and validate sets.\\nFirst, you will split the data into train and test data, keeping the normal \\npoints and anomalies separate. To do this, you will use the train_test_\\nsplit()  function from scikit-learn. Commonly passed parameters are\\n• x: The x set you want to split up\\n• y: The y set you want to split up corresponding to  \\nthe x set\\n• test_size : The proportion of data in x and y that you \\nwant to randomly sample for the test set.\\nAnd so, to split up x and y into your training and testing sets, you may \\nsee code like the following:\\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_\\nsize=0.2, random_state = 2020)\\nJust like earlier, random_state  is setting the random seed so that every',\n",
       " 'see code like the following:\\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_\\nsize=0.2, random_state = 2020)\\nJust like earlier, random_state  is setting the random seed so that every \\ntime you run it, the data will be split the same way.Chapter 2  Building Models48If you don’t pass in the y parameter, you simply get a split on the x \\ndata. And so, keeping that in mind, let’s split up your normal points and \\nanomalies into training and testing sets:\\nnormal_train, normal_test = train_test_split(normal, test_size \\n= 0.2, random_state = 2020)\\nanomaly_train, anomaly_test = train_test_split(anomaly,  \\ntest_size = 0.2, random_state = 2020)\\nThere should be no output but refer to Figure\\xa0 2-6 to see the code  \\nin a cell.',\n",
       " 'anomaly_train, anomaly_test = train_test_split(anomaly,  \\ntest_size = 0.2, random_state = 2020)\\nThere should be no output but refer to Figure\\xa0 2-6 to see the code  \\nin a cell.\\nNow, you can create your training and validation sets by calling the \\nsame function on the respective training sets. You don’t want to split it \\nby 20% again, though, since the training set is already 80% of the original \\ndata set. If you used a 20% split again, the validation set would be 16% of \\nthe original data, and the training set would be 64% of the original data. \\nYou will instead be doing a 60-20-20 split for the training, testing, and \\nvalidation data, respectively, and so you will be using a new test_size  \\nvalue of 0.25 to ensure these proportions hold (0.25 * 0.8 = 0.2).',\n",
       " 'validation data, respectively, and so you will be using a new test_size  \\nvalue of 0.25 to ensure these proportions hold (0.25 * 0.8 = 0.2).\\nWith that in mind, let’s create your training and validation splits:\\nnormal_train, normal_validate = train_test_split(normal_train, \\ntest_size = 0.25, random_state = 2020)\\nanomaly_train, anomaly_validate = train_test_split(anomaly_\\ntrain, test_size = 0.25, random_state = 2020)\\nRefer to Figure\\xa0 2-7 to see the code in a cell.\\nFigure 2-6.  Splitting the normal and anomaly data frames into train \\nand test subsets. The respective test sets comprise 20% of the original setsChapter 2  Building Models49To create your final training, testing, and validation sets, you have to \\nconcatenate the respective normal and anomaly data splits.',\n",
       " 'concatenate the respective normal and anomaly data splits.\\nFirst, you define x_train , x_test , and x_validate :\\nx_train = pd.concat((normal_train, anomaly_train))\\nx_test = pd.concat((normal_test, anomaly_test))\\nx_validate = pd.concat((normal_validate, anomaly_validate))\\nNext, you define y_train , y_test , and y_validate :\\ny_train = np.array(x_train[\"Class\"])\\ny_test = np.array(x_test[\"Class\"])\\ny_validate = np.array(x_validate[\"Class\"])\\nFinally, you have to drop the column Class  in the x sets since it would \\ndefeat the purpose of teaching the model how to learn what makes up a \\nnormal and a fraudulent transaction if you gave it the label directly:\\nx_train = x_train.drop(\"Class\", axis=1)\\nx_test = x_test.drop(\"Class\", axis=1)\\nx_validate = x_validate.drop(\"Class\", axis=1)',\n",
       " 'normal and a fraudulent transaction if you gave it the label directly:\\nx_train = x_train.drop(\"Class\", axis=1)\\nx_test = x_test.drop(\"Class\", axis=1)\\nx_validate = x_validate.drop(\"Class\", axis=1)\\nTo see all this code in a cell, refer to Figure\\xa0 2-8.\\nFigure 2-7.  You create train and validate splits from the training \\ndata. You have chosen to make the validation set comprise 25% of \\nthe respective original training sets. As these original training sets \\nthemselves comprise of 80% of the original normal and anomaly data \\nframes, the respective validation splits are 20% (0. 25 * 0.8) of their \\noriginal normal and anomaly data frames. And so, the final training',\n",
       " 'frames, the respective validation splits are 20% (0. 25 * 0.8) of their \\noriginal normal and anomaly data frames. And so, the final training \\nsplit also becomes 60% of the original, as 0.75 * 0.8 = 0.6Chapter 2  Building Models50Let’s get the shapes of the sets you just created:\\nprint(\"Training sets:\\\\nx_train: {} y_train: {}\".format(x_train.\\nshape, y_train.shape))\\nprint(\"\\\\nTesting sets:\\\\nx_test: {} y_test: {}\".format(x_test.\\nshape, y_test.shape))\\nprint(\"\\\\nValidation sets:\\\\nx_validate: {} y_validate:  \\n{}\".format(x_validate.shape, y_validate.shape))\\nRefer to Figure\\xa0 2-9 to see the output.\\nLooking at the data analysis, you can see that some of the values get \\nreally large. The fine details are beyond the scope of this book, but when',\n",
       " 'Refer to Figure\\xa0 2-9 to see the output.\\nLooking at the data analysis, you can see that some of the values get \\nreally large. The fine details are beyond the scope of this book, but when \\nsome features have a relatively small range but others have an extremely \\nlarge range (think of the range of V1 and Time  from the previous chapter), \\nthe model will have a much harder time learning.\\nFigure 2-8.  Creating the respective x and y splits of the training, \\ntesting, and validation sets. The x sets are the combinations of the \\nnormal and anomaly sets for each split (train, test, validate), while \\nthe y sets are simply the data in the Class columns of those x sets. You \\nthen drop the label column from the x sets\\nFigure 2-9.  Printing the output of the different sets. The three sets',\n",
       " 'the y sets are simply the data in the Class columns of those x sets. You \\nthen drop the label column from the x sets\\nFigure 2-9.  Printing the output of the different sets. The three sets \\nshould comprise 60%, 20%, and 20% of the original union of the \\nnormal and anomaly setsChapter 2  Building Models51In more detail, the model will have a hard time optimizing the cost \\nfunction and may take many more steps to converge, if it is able to do so at all.\\nAnd so it is better to scale everything down by normalizing the data. \\nYou will be using scikit-learn’s StandardScaler , which normalizes all of \\nthe data such that the mean is 0 and the standard deviation is 1.\\nHere is the code to standardize your data:\\nscaler = StandardScaler()\\nscaler.fit(pd.concat((normal, anomaly)).drop(\"Class\", axis=1))',\n",
       " 'the data such that the mean is 0 and the standard deviation is 1.\\nHere is the code to standardize your data:\\nscaler = StandardScaler()\\nscaler.fit(pd.concat((normal, anomaly)).drop(\"Class\", axis=1))\\nx_train = scaler.transform(x_train)\\nx_test = scaler.transform(x_test)\\nx_validate = scaler.transform(x_validate)\\nIt is important to note that you are fitting the scaler on the entire data \\nframe so that it standardizes all of your data in the same way. This is to \\nensure the best results since you don’t want to standardize x_train , x_test , \\nand x_validate  in their own ways since it would create discrepancies in \\nthe data and would be problematic for the model. Of course, once you’ve \\ndeployed the model and start receiving new data, you would still standardize',\n",
       " 'the data and would be problematic for the model. Of course, once you’ve \\ndeployed the model and start receiving new data, you would still standardize \\nit using the scaler from the training process, but this new data could possibly \\ncome from a slightly different distribution than your training data. This \\nwould especially be the case if trends start shifting - this new standardized \\ndata could possibly lead to a tougher time for the model since it wouldn’t fit \\nvery well in the distribution that the model trained on.\\nRefer to Figure\\xa0 2-10  to see the code in a cell.\\nFigure 2-10.  Fitting a standard scaler object on a concatenation of \\nthe normal and anomaly data frames. This is done so that each of \\nthe train, test, and validate subsets will be scaled according to the',\n",
       " \"the normal and anomaly data frames. This is done so that each of \\nthe train, test, and validate subsets will be scaled according to the \\nsame standards, ensuring that there are no discrepancies between the \\nscaling of the dataChapter 2  Building Models52 Model Training\\nFinally, you can now define your logistic regression model:\\nsk_model = LogisticRegression(random_state= None, max_iter=400, \\nsolver='newton-cg').fit(x_train, y_train)\\nRefer to Figure\\xa0 2-11  to see the code in a cell. There should not be any \\noutputs after execution if it all goes well. Any errors you might see could \\ninvolve a failure to converge. For that, changing the max_iter  parameter \\ncould help, and changing the solver  algorithm could help as well.\\nAfter the training process, either the evaluation step or validation\",\n",
       " 'could help, and changing the solver  algorithm could help as well.\\nAfter the training process, either the evaluation step or validation \\nstep can come next. As long as the testing set and the validation set come \\nfrom different distributions (the validation set is derived from the training \\nset, while the testing set is derived from the original data), the model is \\ntechnically seeing new data in the evaluation and in the validation processes.\\nThe context also matters. If you are using the validation process to \\nselect the best model out of a set of trained models, then the validation \\nprocess can come after the training process. You can still evaluate one or \\nall of your trained models, but it could be unnecessary because in this \\ncontext you’re trying to find the best model for the code.',\n",
       " 'all of your trained models, but it could be unnecessary because in this \\ncontext you’re trying to find the best model for the code.\\nIn the context where you’re trying to tune your hyperparameters for \\na model you are going to stick with, it doesn’t matter whether you do the \\nevaluation first or the validation first. Doing the evaluation first, as you will \\nbe doing shortly, can give you a good idea of how well the model is doing \\ncurrently before starting the validation step. The model will never learn \\nfrom the evaluation data, so there’s no harm in evaluating the model on \\nthis data.\\nFigure 2-11.  Defining the logistic regression model and training it on \\nthe training dataChapter 2  Building Models53In this example, you are looking at tuning the hyperparameter for class',\n",
       " 'Figure 2-11.  Defining the logistic regression model and training it on \\nthe training dataChapter 2  Building Models53In this example, you are looking at tuning the hyperparameter for class \\nweights (how much to weight a normal sample and how much to weight a \\nfraudulent sample).\\nBut first, let’s evaluate your model to get a deeper understanding of \\nhow everything works.\\n Model Evaluation\\nYou can now look at accuracy and AUC scores. First, you find the accuracy \\nusing the built-in score function of the model:\\neval_acc = sk_model.score(x_test, y_test)\\nNext, let’s get the list of predictions from the model to help calculate \\nthe AUC score. AUC is usually a better metric since it better explains \\nthe performance of the model. The general gist of it is that a model that',\n",
       " 'the AUC score. AUC is usually a better metric since it better explains \\nthe performance of the model. The general gist of it is that a model that \\nperfectly classifies every point correctly will have an AUC score of 100%.\\nThe problem with accuracy in this context is that if there are 100,000 \\nnormal points and perhaps around 100 anomalies, the model can classify \\nall of the normal points correctly and none of the anomalies and still get \\na really high accuracy above 99%. However, the AUC score would show \\na value much lower at around 0.5. An AUC of 0.5 means that the model \\nknows nothing and is practically just guessing randomly, but in this case, it \\nmeans the model only ever predicts “normal” for any point it sees. In other',\n",
       " 'knows nothing and is practically just guessing randomly, but in this case, it \\nmeans the model only ever predicts “normal” for any point it sees. In other \\nwords, it hasn’t actually learned much of anything if it doesn’t know how \\nto predict an anomaly.\\nIt’s also worth mentioning that AUC isn’t the sole metric by which one \\nshould base the worthiness of a model, since context matters. In this case, \\nnormal points far outnumber anomalies, so accuracy is a relatively poor \\nmetric to solely judge model performance on. AUC scores in this case \\nwould reflect the mode’s performance well, but it’s also possible to get \\nhigher AUC scores but lower accuracy scores. That just means you must Chapter 2  Building Models54look at the results carefully to understand exactly what’s happening. To',\n",
       " 'higher AUC scores but lower accuracy scores. That just means you must Chapter 2  Building Models54look at the results carefully to understand exactly what’s happening. To \\nhelp with this, you will look at a “confusion matrix” shortly.\\nNow, let’s get the predictions and calculate the AUC score:\\npreds = sk_model.predict(x_test)\\nauc_score = roc_auc_score(y_test, preds)\\nFinally, let’s print out the scores:\\nprint(f\"Auc Score: {auc_score:.3%}\")\\nprint(f\"Eval Accuracy: {eval_acc:.3%}\")\\nRefer to Figure\\xa0 2-12  to see all three of the cells above and the output \\nthat results.\\nIn this case, both the AUC score and the accuracy score are high. \\nBetween the two, the accuracy score is definitely inflated by the number of \\nnormal points that exist, but the AUC score indicates that the model does',\n",
       " \"Between the two, the accuracy score is definitely inflated by the number of \\nnormal points that exist, but the AUC score indicates that the model does \\na pretty good job at distinguishing between the anomalies and the normal \\npoints.\\nScikit-learn actually provides a function that lets you see the ROC \\ncurve—the figure from which the AUC score (or “area under curve”) is \\nderived from. Run the following:\\nroc_plot = plot_roc_curve(sk_model, x_test, y_test, \\nname='Scikit-learn ROC Curve')\\nRefer to Figure\\xa0 2-13  for the output.\\nFigure 2-12.  Printing out the AUC score and the accuracy for the \\nscikit-learn logistic regression modelChapter 2  Building Models55What’s basically happening is that scikit-learn takes in the model and\",\n",
       " 'scikit-learn logistic regression modelChapter 2  Building Models55What’s basically happening is that scikit-learn takes in the model and \\nthe evaluation set to dynamically generate the curve as it predicts on the \\ntest sets. The metrics you see on the axes are derived from how correctly \\nthe model predicts each of the values. The “true positive rate” and the \\n“false positive rate” are derived from the values on the confusion matrix \\nthat you will see below.\\nFrom that graph, the AUC score is generated. You can see that it differs \\nfrom the score that was calculated earlier, but this can be attributed to the \\ntwo functions calculating the scores slightly differently.\\nLet’s now build the confusion matrix and plot it using seaborn:\\nconf_matrix = confusion_matrix(y_test, preds)',\n",
       " \"two functions calculating the scores slightly differently.\\nLet’s now build the confusion matrix and plot it using seaborn:\\nconf_matrix = confusion_matrix(y_test, preds)\\nax = sns.heatmap(conf_matrix, annot= True,fmt='g')\\nax.invert_xaxis()\\nax.invert_yaxis()\\nplt.ylabel('Actual')\\nplt.xlabel('Predicted')\\nRefer to Figure\\xa0 2-14  for the output.\\nFigure 2-13.  The ROC curve generated for the logistic regression \\nmodel you just trained. An ROC curve starting with a true positive \\nvalue of 1.0 at a false positive value of 0.0 is the best possible curve in \\ntheory. From that point, it should keep going right while maintaining \\nits value as it hits 1.0 on the x-axis. This graph is quite close to that \\nideal, hence why the AUC score is so high at 0.98. The discrepancy in\",\n",
       " 'its value as it hits 1.0 on the x-axis. This graph is quite close to that \\nideal, hence why the AUC score is so high at 0.98. The discrepancy in \\nAUC score here compared to when you calculated it earlier has to do \\nwith how the value is actually calculatedChapter 2  Building Models56This is what a confusion matrix looks like. The y-axis consists of \\nthe true labels, while the x-axis consists of predicted labels. When the \\ntrue label is “0” and the model predicts “0, ” we call this a true negative . \\n“True” refers to the true label, and “negative” refers to the label the model \\npredicts.\\nWhat counts as “positive” and what counts as “negative” can differ. In \\ntasks such as disease detection, if a test finds someone to have the disease,',\n",
       " 'predicts.\\nWhat counts as “positive” and what counts as “negative” can differ. In \\ntasks such as disease detection, if a test finds someone to have the disease, \\nthey are said to “test positive. ” Otherwise, they “test negative. ” Anomaly \\ndetection is similar. When a model thinks that a point is an anomaly, it \\nflags it with the label “1. ” And so, a point is labeled “positive” if the model \\nthinks it is an anomaly, and “negative” if it doesn’t.\\nFigure 2-14.  The confusion matrix plot of the results of training. The \\naccuracy for the normal points is very good, but the accuracy for the \\nanomaly points is ok. There is still further room for improvement \\nlooking at these results, as you have not tuned the hyperparameters of',\n",
       " 'anomaly points is ok. There is still further room for improvement \\nlooking at these results, as you have not tuned the hyperparameters of \\nthe model yet, but it already does ok in detecting anomalies. The goal \\nnow is to keep the accuracy for the normal points as high as possible, \\nor at a high enough level that’s acceptable, while raising the accuracy \\nfor the anomaly points as high as possible. Based on this confusion \\nmatrix plot, you can now see that the lower AUC score is more \\naccurate at reflecting the true performance of the model. You can see \\nthat a non-negligible amount of anomalies were falsely classified as \\nnormal, hence an AUC score of 0.84 is a much better indicator of the',\n",
       " 'that a non-negligible amount of anomalies were falsely classified as \\nnormal, hence an AUC score of 0.84 is a much better indicator of the \\nmodel’s performance than the graph’s apparent score of 0.98Chapter 2  Building Models57You may notice that we have inverted the axes in the code. This is \\nsimply to get it in the format so that the top left of the matrix corresponds \\nto “true positives, ” the top right of the matrix corresponds to “false \\nnegatives, ” the bottom left of the matrix corresponds to “false positives, ” \\nand the bottom right of the matrix corresponds to “true negatives. ”\\nTo quickly recap these concepts:\\n• True positives  are values that the model predicts as \\npositive that actually are positive.\\n• False negatives  are values that the model predicts as',\n",
       " 'To quickly recap these concepts:\\n• True positives  are values that the model predicts as \\npositive that actually are positive.\\n• False negatives  are values that the model predicts as \\nnegative that actually are positive.\\n• False positives  are values that the model predicts as \\npositive that actually are negative.\\n• True negatives  are values that the model predicts as \\nnegative that actually are negative.\\nTo look at how well the model identifies anomalies, look at the 1 \\nrow on the y-axis. The sum of this row should equal the total number \\nof anomalies in the test set: 99 anomalies. The model predicted about \\n68.7% of the anomalies correctly (68/(68+31)) and predicted 99.98% of the \\nnormal points correctly (28425/(28425 + 7)) looking at the bottom row.',\n",
       " '68.7% of the anomalies correctly (68/(68+31)) and predicted 99.98% of the \\nnormal points correctly (28425/(28425 + 7)) looking at the bottom row.\\nAs you can see, the confusion matrix gives us a really good look at \\nthe true performance of the model. You now know that it does very well \\nin the task of predicting normal points but does an ok job at predicting \\nanomalies. That being said, the model can still predict a majority of \\nanomalies correctly. And so you can see that the AUC score of 0.84 was \\nmuch more accurate at indicating the performance of the model than the \\ngraph, which had an AUC of 0.98. With an AUC of 0.98, you can expect that \\nthere are very, very few instances of false negatives or false positives.Chapter 2  Building Models58 Model Validation',\n",
       " 'graph, which had an AUC of 0.98. With an AUC of 0.98, you can expect that \\nthere are very, very few instances of false negatives or false positives.Chapter 2  Building Models58 Model Validation\\nLet’s now look at how to use the process of k-fold cross-validation to \\ncompare several hyperparameter values. After the validation process has \\nended, you will compare the evaluation metrics to get a better idea of what \\nhyperparameter setting works best.\\nThe hyperparameter you want to tune is how much you want to weight \\nthe anomalies by compared to the normal data points. By default, both of \\nthem are weighted equally. Let’s define a list of weights to iterate over:\\nanomaly_weights = [1, 5, 10, 15]\\nNext, you define the number of folds and initialize your data fold \\ngenerator:\\nnum_folds = 5',\n",
       " 'them are weighted equally. Let’s define a list of weights to iterate over:\\nanomaly_weights = [1, 5, 10, 15]\\nNext, you define the number of folds and initialize your data fold \\ngenerator:\\nnum_folds = 5\\nkfold = KFold(n_splits=num_folds, shuffle= True,  \\nrandom_state=2020)\\nWhat this KFold()  function does is that it splits the data passed in into \\nnum_folds  different partitions. A single fold acts as a validation set at a \\ntime, while the rest of the folds are used for training. In this context, the \\n“validation fold” is basically what the model will be evaluating on. It is \\ncalled “validation” since it helps us get an idea of how the model is doing \\non data it has never seen before.\\nIf you have built deep learning models before, you may know that',\n",
       " 'called “validation” since it helps us get an idea of how the model is doing \\non data it has never seen before.\\nIf you have built deep learning models before, you may know that \\nduring the training process, you can split a small portion of the training \\nset aside as a validation set. This lets you know during training if you’re \\noverfitting or not, as decreasing training loss and increasing validation loss \\nwould indicate.\\nRefer to Figure\\xa0 2-15  to see the code above in cells.Chapter 2  Building Models59Now you define the validation script:\\nlogs = []\\nfor f in range(len(anomaly_weights)):\\n    fold = 1\\n    accuracies = []\\n    auc_scores= []\\n    for train, test in kfold.split(x_validate, y_validate):\\n        weight = anomaly_weights[f]\\n        class_weights= {\\n            0:1,',\n",
       " \"fold = 1\\n    accuracies = []\\n    auc_scores= []\\n    for train, test in kfold.split(x_validate, y_validate):\\n        weight = anomaly_weights[f]\\n        class_weights= {\\n            0:1,\\n            1: weight\\n        }\\n        sk_model = LogisticRegression(random_state= None,\\n                                      max_iter=400,\\n                                      solver='newton-cg',\\n                                       class_weight=class_\\nweights).fit(x_\\nvalidate[train],  \\ny_validate[train])\\nFigure 2-15.  Setting the different values for anomaly weights to test \\nwith the validation script and constructing the KFold data generator. \\nIn this case, you are using five folds, so the data passed in will be split\",\n",
       " 'with the validation script and constructing the KFold data generator. \\nIn this case, you are using five folds, so the data passed in will be split \\nfive waysChapter 2  Building Models60        for h in range(40): print(\\'-\\', end=\"\")\\n        print(f\"\\\\nfold {fold}\\\\nAnomaly Weight: {weight}\")\\n         eval_acc = sk_model.score(x_validate[test],  \\ny_validate[test])\\n        preds = sk_model.predict(x_validate[test])\\n        try:\\n            auc_score = roc_auc_score(y_validate[test], preds)\\n        except:\\n            auc_score = -1\\n         print(\"AUC: {}\\\\neval_acc: {}\".format(auc_score,  eval_acc))\\n        accuracies.append(eval_acc)\\n        auc_scores.append(auc_score)\\n         log = [sk_model, x_validate[test], y_validate[test], preds]\\n        logs.append(log)\\n        fold = fold + 1',\n",
       " 'accuracies.append(eval_acc)\\n        auc_scores.append(auc_score)\\n         log = [sk_model, x_validate[test], y_validate[test], preds]\\n        logs.append(log)\\n        fold = fold + 1\\n    print(\"\\\\nAverages: \")\\n    print(\"Accuracy: \", np.mean(accuracies))\\n    print(\"AUC: \", np.mean(auc_scores))\\n    print(\"Best: \")\\n    print(\"Accuracy: \", np.max(accuracies))\\n    print(\"AUC: \", np.max(auc_scores))\\nThat’s a lot to take in at once, so be sure to refer to Figure\\xa0 2-16  to make \\nsure your code is formatted correctly.Chapter 2  Building Models61Before you run the script, let’s go over what the code does, as that was a \\nlot of code thrown out at once.\\nThe first loop goes over each of the anomaly weights. You set the fold',\n",
       " 'lot of code thrown out at once.\\nThe first loop goes over each of the anomaly weights. You set the fold \\nnumber here equal to 1 and define empty lists to hold values for accuracy \\nand AUC scores for each run with the current weight parameter.\\nThe second loop goes over the five fold boundaries that the KFold()  \\nobject defines. You set the class_weights  dictionary and pass it into the \\nmodel as a hyperparameter. After the training process, you evaluate as \\nusual. There is a try-except block for the AUC score in the event that the \\nFigure 2-16.  The validation script in a cell. The script is quite long, so \\nbe sure it is formatted correctly because a single space misalignment',\n",
       " 'Figure 2-16.  The validation script in a cell. The script is quite long, so \\nbe sure it is formatted correctly because a single space misalignment \\ncan cause issuesChapter 2  Building Models62fold generated only has values of one class (so really if it only has normal \\ndata and no anomalies). If the AUC score is -1 for any fold, then you know \\nthere was a problem with one of the folds.\\nYou do save the model, the validation data, and the predictions so that \\nyou can examine the confusion matrix and plot the ROC curve for any run \\nyou like. After the end of the five folds, the script then displays averages \\nand the best scores.\\nThe output will be truncated when you run this, so don’t forget to \\nexpand it like in the previous chapter to look at all of the runs. Feel free to',\n",
       " 'and the best scores.\\nThe output will be truncated when you run this, so don’t forget to \\nexpand it like in the previous chapter to look at all of the runs. Feel free to \\nexplore the output or even change the number of folds but beware of the \\nresults because increasing the number of folds can mean that the number \\nof anomalies must be spread across even more partitions. In this specific \\ncontext, a lower number of folds is likely to be better because you have so \\nfew anomaly points.\\nWhen you sift through the output, you can see that the best results \\noccur when the anomaly weight is set to 10. This setting had the highest \\naverage AUC score and had the best AUC score as well, resulting in an \\noutput like what you see in Figure\\xa0 2-17 .Chapter 2  Building Models63',\n",
       " 'average AUC score and had the best AUC score as well, resulting in an \\noutput like what you see in Figure\\xa0 2-17 .Chapter 2  Building Models63\\nFigure 2-17.  Looking at the results of the best setup in the validation \\nscript output. The best setup turned out to be one where the anomalies \\nwere weighted as 10, as it had the best average AUC score and the best \\nAUC score with the other anomaly weight parameters. The true best \\nweight is likely around an anomaly weight of 10, though you must \\nperform another hyperparemter search with a more narrowed range \\nto find the absolute best setting. You can keep narrowing the search as \\nmuch as you’d like, but past a certain precision, you will find that you',\n",
       " 'to find the absolute best setting. You can keep narrowing the search as \\nmuch as you’d like, but past a certain precision, you will find that you \\nare getting diminishing returnsChapter 2  Building Models64Let’s examine the plots for this setup since it was the best performer of \\nall of them on average.\\nFirst, you load the correct log in the list of logs. Since the anomaly \\nweight was 10, and the second fold performed the best, you want to look at \\nthe twelfth index in the entries in logs. (The first five correspond to indices \\n0-4, and the next five are indices 5-9. With index 10, you begin the first fold \\nwith weight ten, so the second fold is at index 11.)\\nsk_model, x_val, y_val, preds = logs[11]\\nLet’s look at the ROC curve. Keep in mind that since there is so little',\n",
       " \"with weight ten, so the second fold is at index 11.)\\nsk_model, x_val, y_val, preds = logs[11]\\nLet’s look at the ROC curve. Keep in mind that since there is so little \\ndata in the validation set, the AUC score may not be so accurate. Here is \\nthe code:\\nroc_plot = plot_roc_curve(sk_model, x_val, y_val, name='Scikit-  \\nlearn ROC Curve')\\nRefer to Figure\\xa0 2-18  to see the output of the above two cells.\\nFigure 2-18.  Viewing the ROC curve for a specific validation fold. \\nAs you can see, the ROC curve is quite optimal. A perfect ROC curve \\nwould start as close as possible to 1.0 on the y-axis while maintaining \\nthat level right as it reaches 1.0 on the x-axis. An ROC graph like that \\nwould mean the AUC would be as close to 1.0 as possible. In this case,\",\n",
       " \"that level right as it reaches 1.0 on the x-axis. An ROC graph like that \\nwould mean the AUC would be as close to 1.0 as possible. In this case, \\nyou almost see the perfect AUC curve, and the AUC is stated to be 1.0. \\nThe confusion matrix in Figure\\xa0 2-19  will reveal a lot more about why \\nthe AUC score is so lowChapter 2  Building Models65This graph looks different compared to the ROC plot you saw earlier. In \\nfact, it almost seems perfect.\\nLet’s look at the confusion matrix to get a better idea of how the model \\nperformed on this fold:\\nconf_matrix = confusion_matrix(y_val, preds)\\nax = sns.heatmap(conf_matrix, annot= True,fmt='g')\\nax.invert_xaxis()\\nax.invert_yaxis()\\nplt.ylabel('Actual')\\nplt.xlabel('Predicted')\\nThe resulting confusion matrix can be seen in Figure\\xa0 2-19 .\",\n",
       " \"ax = sns.heatmap(conf_matrix, annot= True,fmt='g')\\nax.invert_xaxis()\\nax.invert_yaxis()\\nplt.ylabel('Actual')\\nplt.xlabel('Predicted')\\nThe resulting confusion matrix can be seen in Figure\\xa0 2-19 .\\nFigure 2-19.  The confusion matrix for a specific validation fold. It \\nhas very good accuracy in labeling normal data points and does very \\nwell with anomaly points. Additionally, you can see that there are \\nbarely any anomalies in this validation fold if you count the entries \\nin the top row: 21 anomalies to 5,685 normal points. It is no wonder, \\nthen, that having a higher weight on the anomaly helped the model \\nfactor in these anomalies in its learning process, resulting in better\",\n",
       " 'then, that having a higher weight on the anomaly helped the model \\nfactor in these anomalies in its learning process, resulting in better \\nperformance in anomaly detectionChapter 2  Building Models66The model did quite well on correctly classifying the anomalies, but \\nthe goal of validation in this case is just to help nudge the hyperparameter \\nsetting in the right direction. Based on the results of the validation process, \\nyou know that the optimal hyperparameter value lies within the values of \\n10 and 15 because those two settings produced the best results.\\nOf course, you can narrow the range further to include values between \\n10 and 15 for the anomaly weights and repeat this process again and again, \\nfurther reducing the range until a good, optimal value is found. After a',\n",
       " '10 and 15 for the anomaly weights and repeat this process again and again, \\nfurther reducing the range until a good, optimal value is found. After a \\ncertain precision, however, you will find that you are getting diminishing \\nreturns, and that the effort you put into hyperparameter tuning only \\nproduces near-negligible boosts in performance.\\nWith that, you now know how to train, evaluate, and validate a logistic \\nregression model in scikit-learn.\\n PySpark\\nWe have provided the versions of the modules we will be using. Installing \\nPySpark can be a little complicated as it’s not a matter of doing pip \\ninstall PySpark  depending on the version, so beware of that.\\nHere are the versions of our configuration:\\n• Python 3.6.5\\n• PySpark 3.0.0\\n• matplotlib 3.2.1\\n• seaborn 0.10.1',\n",
       " 'install PySpark  depending on the version, so beware of that.\\nHere are the versions of our configuration:\\n• Python 3.6.5\\n• PySpark 3.0.0\\n• matplotlib 3.2.1\\n• seaborn 0.10.1\\n• sklearn 0.22.1.post1\\nWith that, let’s begin. Again, we will not provide commentary as \\ndetailed as in the scikit-learn example, so be sure to review the whole \\nprocess in scikit-learn to get a good idea of how it will go. Additionally, we \\nwon’t be validating the model in PySpark in this example.Chapter 2  Building Models67 Data Processing\\nHere are the import  statements:\\nimport pyspark #\\nfrom pyspark.sql import SparkSession\\nfrom pyspark import SparkConf, SparkContext\\nfrom pyspark.sql.types import *\\nfrom pyspark.ml.feature import VectorAssembler\\nfrom pyspark.ml import Pipeline',\n",
       " 'from pyspark.sql import SparkSession\\nfrom pyspark import SparkConf, SparkContext\\nfrom pyspark.sql.types import *\\nfrom pyspark.ml.feature import VectorAssembler\\nfrom pyspark.ml import Pipeline\\nfrom pyspark.ml.classification import LogisticRegression as \\nLogisticRegressionPySpark\\nimport pyspark.sql.functions as F\\nimport os\\nimport seaborn as sns\\nimport sklearn #\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import roc_auc_score\\nimport matplotlib #\\nimport matplotlib.pyplot as plt\\nos.environ[\"SPARK_LOCAL_IP\"]=\\'127.0.0.1\\'\\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\\nspark.sparkContext._conf.getAll()\\nprint(\"pyspark: {}\".format(pyspark.__version__))\\nprint(\"matplotlib: {}\".format(matplotlib.__version__))\\nprint(\"seaborn: {}\".format(sns.__version__))',\n",
       " 'spark.sparkContext._conf.getAll()\\nprint(\"pyspark: {}\".format(pyspark.__version__))\\nprint(\"matplotlib: {}\".format(matplotlib.__version__))\\nprint(\"seaborn: {}\".format(sns.__version__))\\nprint(\"sklearn: {}\".format(sklearn.__version__))\\nThe output should look something like in Figure\\xa0 2-20 .Chapter 2  Building Models68You will notice that there is some additional code relating to PySpark \\nthat you have had to define. With PySpark, you must define a Spark context \\nand create a Spark session. What this really means is that you are creating \\na point of connection to the Spark engine, enabling the engine to run all of \\nthe code relating to Spark functionality.\\nLet’s now load the data set. PySpark has its own functionality for',\n",
       " 'a point of connection to the Spark engine, enabling the engine to run all of \\nthe code relating to Spark functionality.\\nLet’s now load the data set. PySpark has its own functionality for \\ncreating data frames, so you won’t be using pandas. Execute the following:\\ndata_path = \\'data/creditcard.csv\\'\\ndf = spark.read.csv(data_path, header = True, inferSchema = True)\\nlabelColumn = \"Class\"\\ncolumns = df.columns\\nnumericCols = columns\\nnumericCols.remove(labelColumn)\\nprint(numericCols)\\nFigure 2-20.  Importing the necessary modules and printing their \\nversions. Once again, modules imported solely for the purpose of \\ndisplaying versions are marked with a # so you may remove them and \\nthe print statements if desiredChapter 2  Building Models69You should see something like Figure\\xa0 2-21 .',\n",
       " 'displaying versions are marked with a # so you may remove them and \\nthe print statements if desiredChapter 2  Building Models69You should see something like Figure\\xa0 2-21 .\\nPrinting the columns is just to ensure that the label column has been \\nremoved successfully.\\nYou can look at the data frame now just to ensure that it has been \\nloaded properly. You will have to use built-in functionality to convert to a \\npandas data frame, because Spark data frames are not very clean to look at.\\nLook at the following two cells and their outputs:\\ndf.show(2)\\nRefer to Figure\\xa0 2-22 .\\nFigure 2-21.  Reading the credit card data set in PySpark and \\nremoving the Class column from the list of columns. This is done \\nbecause you don’t want the Class column to be included in the feature',\n",
       " 'Figure 2-21.  Reading the credit card data set in PySpark and \\nremoving the Class column from the list of columns. This is done \\nbecause you don’t want the Class column to be included in the feature \\nvector, as you will see in Figure\\xa0 2-22Chapter 2  Building Models70Now compare this to the following:\\ndf.toPandas().head()\\nRefer to Figure\\xa0 2-23 .\\nFigure 2-22.  The output of the Spark data frame. Since there are so \\nmany columns in the data frame, the output is very messy and very \\ndifficult to read. Fortunately, there is built-in functionality to convert \\nPySpark data frames into pandas data frames, making it much easier \\nto view the rows in the Spark data frameChapter 2  Building Models71So whenever you want to check a Spark data frame, make sure to',\n",
       " 'PySpark data frames into pandas data frames, making it much easier \\nto view the rows in the Spark data frameChapter 2  Building Models71So whenever you want to check a Spark data frame, make sure to \\nconvert it to pandas if it has a lot of columns.\\nThe data processing procedure for PySpark is slightly different than \\nin pandas. To train the model, you must pass in a vector called features . \\nTake a look at the following code:\\nstages = []\\nassemblerInputs = numericCols\\nassembler = VectorAssembler(inputCols=assemblerInputs, \\noutputCol=\"features\")\\nstages += [assembler]\\ndfFeatures = df.select(F.col(labelColumn).alias(\\'label\\'), \\n*numericCols )\\nThis defines the inputs to the assembler so that it knows what columns \\nto transform into the features  vector.',\n",
       " 'dfFeatures = df.select(F.col(labelColumn).alias(\\'label\\'), \\n*numericCols )\\nThis defines the inputs to the assembler so that it knows what columns \\nto transform into the features  vector.\\nFrom here, let’s add to the cell above and create the normal and \\nanomaly data splits as with the scikit-learn example.\\nnormal = dfFeatures.filter(\"Class == 0\").\\nsample(withReplacement= False, fraction=0.5, seed=2020)\\nanomaly = dfFeatures.filter(\"Class == 1\")\\nFigure 2-23.  Using PySpark’s built-in functionality to convert the \\nspark data frame into a pandas data frame for easier viewing. As seen \\nin Figure\\xa0 2-22 , it is extremely hard to read the direct output of a Spark \\ndata frameChapter 2  Building Models72normal_train, normal_test = normal.randomSplit([0.8, 0.2],  \\nseed = 2020)',\n",
       " 'in Figure\\xa0 2-22 , it is extremely hard to read the direct output of a Spark \\ndata frameChapter 2  Building Models72normal_train, normal_test = normal.randomSplit([0.8, 0.2],  \\nseed = 2020)\\nanomaly_train, anomaly_test = anomaly.randomSplit([0.8, 0.2], \\nseed = 2020)\\nThe cell should look like Figure\\xa0 2-24 .\\nJust like in the scikit-learn example, you combine the respective \\nnormal and anomaly splits to form your training and testing sets. This \\ntime, however, you won’t have a validation set, so you are looking at an \\n80- 20 s plit between the training and testing data.\\ntrain = normal_train.union(anomaly_train)\\ntest = normal_test.union(anomaly_test)\\nRefer to Figure\\xa0 2-25  to see the cell.\\nFigure 2-24.  Constructing the VectorAssembler that will be used later',\n",
       " 'train = normal_train.union(anomaly_train)\\ntest = normal_test.union(anomaly_test)\\nRefer to Figure\\xa0 2-25  to see the cell.\\nFigure 2-24.  Constructing the VectorAssembler that will be used later \\nto create a feature vector from the input data. You also create a normal \\nand anomaly data split similar to how it was done in scikit-learn, and \\nsplit it in a similar fashion into training and testing subsets\\nFigure 2-25.  Creating the training and testing sets in a similar \\nmanner to how you did it in scikit-learn, but with PySpark’s \\nfunctionalityChapter 2  Building Models73Let’s finish the rest of the pipeline and create the feature  vector:\\npipeline = Pipeline(stages = stages)\\npipelineModel = pipeline.fit(dfFeatures)\\ntrain = pipelineModel.transform(train)\\ntest = pipelineModel.transform(test)',\n",
       " 'pipeline = Pipeline(stages = stages)\\npipelineModel = pipeline.fit(dfFeatures)\\ntrain = pipelineModel.transform(train)\\ntest = pipelineModel.transform(test)\\nselectedCols = [\\'label\\', \\'features\\'] + numericCols\\ntrain = train.select(selectedCols)\\ntest = test.select(selectedCols)\\nprint(\"Training Dataset Count: \", train.count())\\nprint(\"Test Dataset Count: \", test.count())\\nRefer to Figure\\xa0 2-26  to see the output.\\n Model Training\\nYou can now define and train the model:\\nlr = LogisticRegressionPySpark(featuresCol = \\'features\\', \\nlabelCol = \\'label\\', maxIter=10)\\nlrModel = lr.fit(train)\\ntrainingSummary = lrModel.summary\\npyspark_auc_score = trainingSummary.areaUnderROC\\nFigure 2-26.  Using a pipeline to create a feature vector from the data',\n",
       " 'lrModel = lr.fit(train)\\ntrainingSummary = lrModel.summary\\npyspark_auc_score = trainingSummary.areaUnderROC\\nFigure 2-26.  Using a pipeline to create a feature vector from the data \\nframe. This feature vector is what the logistic regression model will \\ntrain onChapter 2  Building Models74Refer to Figure\\xa0 2-27  to see the above code in a cell.\\n Model Evaluation\\nOnce the model has finished training, run the evaluation code:\\npredictions = lrModel.transform(test)\\ny_true = predictions.select([\\'label\\']).collect()\\ny_pred = predictions.select([\\'prediction\\']).collect()\\nevaluations = lrModel.evaluate(test)\\naccuracy = evaluations.accuracy\\nAdd the following code as well to display the metrics:\\nprint(f\"AUC Score: {roc_auc_score(y_pred, y_true):.3%}\")\\nprint(f\"PySpark AUC Score: {pyspark_auc_score:.3%}\")',\n",
       " 'accuracy = evaluations.accuracy\\nAdd the following code as well to display the metrics:\\nprint(f\"AUC Score: {roc_auc_score(y_pred, y_true):.3%}\")\\nprint(f\"PySpark AUC Score: {pyspark_auc_score:.3%}\")\\nprint(f\"Accuracy Score: {accuracy:.3%}\")\\nRefer to Figure\\xa0 2-28  to see the output.\\nFigure 2-27.  Defining the PySpark logistic regression model, training \\nit, and finding the AUC score using the built-in function of the model\\nFigure 2-28.  The output metrics. The AUC score is calculated using \\nscikit-learn’s scoring algorithm, while the PySpark AUC score metric \\ncomes from the training summary of the PySpark model. Finally, the \\naccuracy score is also outputtedChapter 2  Building Models75You can see that the AUC score and the accuracy are quite high, so let’s \\nexamine the graphs.',\n",
       " \"accuracy score is also outputtedChapter 2  Building Models75You can see that the AUC score and the accuracy are quite high, so let’s \\nexamine the graphs.\\nFirst, let’s look at the ROC curve:\\npyspark_roc = trainingSummary.roc.toPandas()\\nplt.xlabel('False Positive Rate')\\nplt.ylabel('True Positive Rate')\\nplt.title('PySpark ROC Curve')\\nplt.plot(pyspark_roc['FPR'],pyspark_roc['TPR'])\\nTo see the graph, refer to Figure\\xa0 2-29 .\\nFigure 2-29.  The ROC curve for the PySpark logistic regression model \\nyou just trained. A perfect ROC curve would have the true positive \\nrate starting at 1.0, where it continues right to a false positive rate \\nvalue of 1.0. This curve is quite close to that, hence why its area (AUC) \\nis said to be around 0.97997 by PySpark, keeping in mind a perfect\",\n",
       " \"value of 1.0. This curve is quite close to that, hence why its area (AUC) \\nis said to be around 0.97997 by PySpark, keeping in mind a perfect \\nAUC score is 1.00Chapter 2  Building Models76The curve looks quite optimal. Let’s now look at the confusion matrix \\nto get a detailed idea of how the model performs:\\nconf_matrix = confusion_matrix(y_true, y_pred)\\nax = sns.heatmap(conf_matrix, annot= True,fmt='g')\\nax.invert_xaxis()\\nax.invert_yaxis()\\nplt.ylabel('Actual')\\nplt.xlabel('Predicted')\\nRefer to Figure\\xa0 2-30  to view the confusion matrix plot.\\nFigure 2-30.  The plotted confusion matrix of the PySpark logistic \\nregression model you just trained. The accuracy of correctly labeled \\npoints for the normal data is very high and is decent for the\",\n",
       " 'Figure 2-30.  The plotted confusion matrix of the PySpark logistic \\nregression model you just trained. The accuracy of correctly labeled \\npoints for the normal data is very high and is decent for the \\nanomalous dataChapter 2  Building Models77From this, you have a much more detailed account of how the model \\nperformed. Looking at just the anomalies, you see that the model has \\na 81.4% accuracy (70/(70+16)) in predicting anomalies. This is better \\nthan the model you trained in scikit-learn, though you haven’t tuned the \\nhyperparameter to attain maximum performance.\\nPySpark does have an option to weight your data, but this is done on \\na sample-by-sample basis. What this means is that instead of passing in a \\nweight dictionary for each class, you have to create a column in the data',\n",
       " 'a sample-by-sample basis. What this means is that instead of passing in a \\nweight dictionary for each class, you have to create a column in the data \\nframe with each anomaly being weighted a certain amount and each \\nnormal point being weighted as 1, for example. By default, everything \\nis weighted as 1, so that means the PySpark model may have a greater \\npotential in performance than the scikit-learn model.\\nMoving on to the normal points, you see a really good accuracy of \\n99.96% (28363/(28363+10)), so it is able to identify normal points very well.\\n Summary\\nWith the insights you gained from data analysis, you processed the data \\ninto training, testing, and validation sets in scikit-learn and PySpark \\n(you only did a train-test split in PySpark, but you could have split the',\n",
       " 'into training, testing, and validation sets in scikit-learn and PySpark \\n(you only did a train-test split in PySpark, but you could have split the \\ntraining data into training and validation sets just like in scikit-learn). \\nFrom there, you constructed logistic regression models in each framework \\nand trained and evaluated on them. You looked at accuracy and AUC \\nscores as metrics and looked at the ROC curve and confusion matrix to \\nget a better idea of how the model performed. For the scikit-learn model, \\nyou performed k-fold cross-validation to help tune the hyperparameter. \\nIn the next chapter, you will keep your experiences with data analysis \\nand model creation in mind as you learn about MLOps and how you can',\n",
       " 'In the next chapter, you will keep your experiences with data analysis \\nand model creation in mind as you learn about MLOps and how you can \\noperationalize your models.Chapter 2  Building Models79© Sridhar Alla, Suman Kalyan Adari 2021 \\nS. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \\nhttps://doi.org/10.1007/978-1-4842-6549-9_3CHAPTER 3\\nWhat Is MLOps?\\nIn this chapter, we will cover the concepts behind the term “MLOps” and \\ngo over what it is, why it’s useful, and how it’s implemented.\\n Introduction\\nCreating machine learning solutions to various problems can be quite \\nthe arduous task. Let’s imagine ourselves in the shoes of a team that is \\nattempting to solve a problem with machine learning. You may be familiar',\n",
       " 'the arduous task. Let’s imagine ourselves in the shoes of a team that is \\nattempting to solve a problem with machine learning. You may be familiar \\nwith this process if you read Chapter 1, but we will recap the entire process \\nonce again to establish the context. You may skip past this section if you \\nare already familiar with this. The entire process may look somewhat like \\nthe following:\\n• Colle ct and process raw data:  Raw data is rarely in \\na format that is easy to train a model on. Usually, it \\nrequires processing to remove aberrant data points \\nsuch as null values and faulty data values. Other \\ntimes, you might have to process the raw data to \\nextract only the information you need among all of \\nthe noise.80• Analyze the data:  This step involves looking at the',\n",
       " 'times, you might have to process the raw data to \\nextract only the information you need among all of \\nthe noise.80• Analyze the data:  This step involves looking at the \\ndata points and understanding their characteristics. \\nHow is it structured? What does the distribution of the \\ndata points look like? Are there any identifiable trends \\nor biases in the data? This step is crucial because it \\ndictates how you are going to approach the problem. \\nIf you already have a trained model you are looking \\nto update, it also tells you if there are any new trends \\nin the data that your model should be updated to \\nconsider. If you identified any “useless features” that \\ndon’t really influence the output, you might drop them \\nand train a new model to improve training speed while',\n",
       " 'consider. If you identified any “useless features” that \\ndon’t really influence the output, you might drop them \\nand train a new model to improve training speed while \\npossibly boosting performance.\\n• Process the data for training:  In this step, you could \\nbe scaling the data to a more appropriate range and \\nperhaps removing any outliers and/or anomalies that \\ncould interfere with model performance. Furthermore, \\nyou could also be applying feature engineering to \\ncreate new features from existing data points and \\nperhaps give your model more or a better context \\nduring training. This is also where you create training \\nand testing data sets, though optimal practice is to \\nmake training, testing, and validation data sets.\\n• Construct, train, and test the model:  In this step, you',\n",
       " 'and testing data sets, though optimal practice is to \\nmake training, testing, and validation data sets.\\n• Construct, train, and test the model:  In this step, you \\nare creating the model, setting hyperparameters, and \\ntraining the model. In the case of deep learning, you \\ncan also select a subsection of the training set to be a \\ndata validation set. The purpose of this set is to have \\nthe model be evaluated on it at the end of every epoch \\nor full forward pass of the data through the model. By \\ncomparing the model performance on data it’s seen Chapter 3  What Is MLOps?81many times over during training versus data it hasn’t \\nseen at all (or rather, data that has no effect on weight \\nadjustment), you can see if the model is truly learning \\nto generalize or if it’s just overfitting.',\n",
       " 'seen at all (or rather, data that has no effect on weight \\nadjustment), you can see if the model is truly learning \\nto generalize or if it’s just overfitting.\\n• Overfitting  is when a model performs significantly \\nbetter on a training set compared to data that it has \\nnever seen before. As just discussed, one way to \\ngive an early indication of overfitting is to set aside a \\nportion of the training set as validation data during the \\ntraining phase. This can give you an early indication \\nof overfitting without having to find out after the \\ntraining process has finished, which can take anywhere \\nfrom minutes to days depending on the depth of the \\nmodel and the equipment used. And so, it follows that \\noverfitting can also be observed when the model is',\n",
       " 'from minutes to days depending on the depth of the \\nmodel and the equipment used. And so, it follows that \\noverfitting can also be observed when the model is \\nevaluated on the testing data or validation data, and \\ndiscrepancies in model performance can be observed \\nbetween these sets and the training set.\\n• This phenomenon of overfitting could partially result \\nfrom the model not receiving enough data points \\nduring training to reflect the variety it is expected to \\nsee, so fixing the training set by introducing more \\nvariety or even increasing the number of data points \\ncan help. Additionally, including methods such as \\nregularization or dropout into the model’s architecture \\ncan also help combat overfitting in the case of deep',\n",
       " 'can help. Additionally, including methods such as \\nregularization or dropout into the model’s architecture \\ncan also help combat overfitting in the case of deep \\nlearning models.Chapter 3  What Is MLOps?82• An im portant thing to discuss is the purpose of the \\ntesting  and validation  sets. Testing sets are reserved \\nfor evaluating a model’s performance on data it’s never \\nseen before.\\n• Validation sets are reserved for helping select models, \\nselect model architecture, tune hyperparameters, or \\nsimply to give an indication of model performance on \\ndata it’s never seen during the training process.\\n• An exam ple of validation is k-fold cross-validation, \\nwhere it generates k random partitions of test-train \\ndata from validation data and can be used to train/',\n",
       " '• An exam ple of validation is k-fold cross-validation, \\nwhere it generates k random partitions of test-train \\ndata from validation data and can be used to train/\\nevaluate the model on all of them to give an idea \\nof the best performance it can attain with various \\nhyperparameter settings. Of course, we can also use \\nk-fold cross validation to perform the other functions \\nthat validation helps with. You looked at an example \\nusing this method of validation in Chapter 2, when you \\nused it to help tune the weighting of anomalies.\\n• Coupling this technique with a script that has a set of \\nhyperparameters can result in an optimal model with \\nproper hyperparameters. From there, the model can \\nbe retrained and evaluated again on the test set to get a \\nfinal performance benchmark.',\n",
       " 'hyperparameters can result in an optimal model with \\nproper hyperparameters. From there, the model can \\nbe retrained and evaluated again on the test set to get a \\nfinal performance benchmark.\\n• The specific order this is done in can differ, though. \\nFor example, trained models can also be evaluated \\nfirst and then validated, compared to the other way \\naround. This is because the training process is likely to \\nbe repeated with altered hyperparameters anyway after \\nthe evaluation stage reflects some form of performance Chapter 3  What Is MLOps?83discrepancy or if validation data during the training \\nprocess reveals that possible overfitting is occurring. \\nEither way, it really depends, but good practice is to at \\nleast incorporate both testing and validation data to \\nbest tune the model.',\n",
       " 'process reveals that possible overfitting is occurring. \\nEither way, it really depends, but good practice is to at \\nleast incorporate both testing and validation data to \\nbest tune the model.\\n• Validate and tune the model:  As previously \\ndiscussed, the validation set can be another “testing” \\nset that the model has never seen before, and can \\nbe used in any of the several ways described earlier \\nand in Chapter 1. Once your model has reached an \\nacceptable level of performance on the validation set \\nand is retrained and evaluated again, you can look at \\ndeploying the model.\\n• Deploy and monitor the model:  In this step, the \\nmodel has finally left the hands of the machine \\nlearning/data science team. It is now the job of \\nengineering and operational teams to integrate this',\n",
       " 'model has finally left the hands of the machine \\nlearning/data science team. It is now the job of \\nengineering and operational teams to integrate this \\nmodel into the application and put it into service. \\nOperational teams are in charge of constantly \\nmonitoring the performance of the model, with dips \\nin performance possibly indicating that this entire \\nprocess may need to be repeated to update the model \\nto understand new trends. Operational teams are also \\nresponsible for reporting any bugs and unexpected \\nmodel predictions to the data science team, feedback \\nthat also contributes to the start of this whole cycle as \\nthe model needs to be fixed.Chapter 3  What Is MLOps?84Hopefully, it’s clear just how work-intensive the entire process can',\n",
       " 'that also contributes to the start of this whole cycle as \\nthe model needs to be fixed.Chapter 3  What Is MLOps?84Hopefully, it’s clear just how work-intensive the entire process can \\nget, especially since it will most likely need to be repeated multiple times. \\nWhile it is possibly easier the second time around since you’re only \\nupdating the model on new data patterns and trends, it is still a problem \\nthat can take up hours of manual labor that can be better spent elsewhere. \\nAfter all, maintenance of applications in the software development \\nprocess is usually where most of the money and resources go, not the \\ninitial construction and release of the application. The same can apply \\nto machine learning models, worsening the overall maintenance costs',\n",
       " 'initial construction and release of the application. The same can apply \\nto machine learning models, worsening the overall maintenance costs \\nbecause the costs for deployed machine learning models are added on top \\nof the costs for the software application utilizing the services of the models.\\nImagine if you could simply automate this entire process away, \\nallowing you to take full advantage of high-performance machine \\nlearning models without all of that hassle. This is where MLOps comes in, \\nsomething that can be thought of as the intersection between machine \\nlearning and DevOps practices. DevOps , or developmental operations, \\nrefers to a set of practices that combines the work processes of software \\ndevelopers with those of operational teams to create a common set',\n",
       " 'refers to a set of practices that combines the work processes of software \\ndevelopers with those of operational teams to create a common set \\nof practices that functions as a hybrid of the two roles. As a result, the \\ndevelopmental cycle of software is expedited, and continuous delivery \\nof software products is ensured. Total costs also go down because \\nmaintenance costs are reduced as a result of the increase in efficiency of \\nthe workflow in maintaining the software applications. Refer to Figure\\xa0 3-1 \\nto see a graph representing the DevOps workflow.Chapter 3  What Is MLOps?85\\nFigure 3-1.  A graph depicting the workflow in a DevOps \\nenvironment. Software development teams typically adopt the Agile \\nmethodology of software development, which is summarized above',\n",
       " 'Figure 3-1.  A graph depicting the workflow in a DevOps \\nenvironment. Software development teams typically adopt the Agile \\nmethodology of software development, which is summarized above \\nthrough the planning, building, and testing stages. Operational teams \\nare in charge of deploying, maintaining, and collecting feedback in \\nthe form of bugs and user feedback and relaying this information to \\nthe development teams. From there, the development team enters the \\nmaintenance phase of the application, where they plan, build, test, \\nand push the next patch/update for the application. Furthermore, \\nautomating the process of testing and deploying allows for continuous \\nintegration and delivery of software products, something we will',\n",
       " 'automating the process of testing and deploying allows for continuous \\nintegration and delivery of software products, something we will \\nexpand upon later in this chapterChapter 3  What Is MLOps?86Similarly, MLOps adopts DevOps principles and applies them to \\nmachine learning models in place of software, uniting the developmental \\ncycles followed by data scientists and machine learning engineers \\nwith that of operational teams to help ensure continuous delivery of \\nhigh-performance machine learning models. The process of model \\ndevelopment in what’s called the experimental stage , something we will \\nlook at in detail later in the chapter, can lead to impressive performances \\nand can seem like very promising solutions. However, the reality is more',\n",
       " 'look at in detail later in the chapter, can lead to impressive performances \\nand can seem like very promising solutions. However, the reality is more \\nthat most models simply never make it past this experimental stage, \\nsince deploying them is a massive undertaking on its own. Unfortunately, \\nmaintaining models once deployed also drains resources, as every new \\nupdate requires reintegration into the application. This means that even \\nif the model is deployed, all teams have their work cut out for them. For \\nthese reasons, most models simply never make it past the prototype phase.\\nUntil the emergence of MLOps principles, deploying solutions created \\nusing the latest in machine learning technology served as a significant \\nchallenge to businesses due to the amount of resources that would be',\n",
       " 'using the latest in machine learning technology served as a significant \\nchallenge to businesses due to the amount of resources that would be \\nrequired. This is why MLOps is so crucial . It makes it significantly easier \\nto deploy and maintain your machine learning solutions by automating \\nmost of the hard parts for you, massively expediting the development and \\nmaintenance processes. With a fully automated setup, teams can keep up \\nwith the latest in machine learning technology and deploy new models \\nquickly. Services can maintain their high level of performance and perhaps \\neven improve on this front as teams can deploy newer, more promising \\nmodel architectures.\\nNow that you have a better idea of what MLOps is about and why it is',\n",
       " 'even improve on this front as teams can deploy newer, more promising \\nmodel architectures.\\nNow that you have a better idea of what MLOps is about and why it is \\nso important, let’s jump into the details and look at how an ideal MLOps \\nimplementation is set up.Chapter 3  What Is MLOps?87 MLOps Setups\\nBefore we look at any specific MLOps setups, let’s first establish three \\ndifferent setups representing the various stages of automation: manual \\nimplementation , continuous model delivery , and continuous \\nintegration/continuous delivery of pipelines .\\nManual implementation  refers to a setup where there are no MLOps \\nprinciples applied and everything is manually implemented. The steps \\ndiscussed above in the creation of a machine learning model are all',\n",
       " 'principles applied and everything is manually implemented. The steps \\ndiscussed above in the creation of a machine learning model are all \\nmanually performed. Software engineering teams must manually integrate \\nthe models into the application, and operational teams must help ensure \\nall functionality is preserved along with collecting data and performance \\nmetrics of the model.\\nContinuous model delivery  is a good middle ground between a \\nmanual setup and a fully automated one. Here, we see the emergence \\nof pipelines  to allow for automation of the machine learning side of the \\nprocess. Note that we will mention this term quite often in the sections \\nbelow. If you’d like to get a better idea about what a pipeline is, refer to the',\n",
       " 'process. Note that we will mention this term quite often in the sections \\nbelow. If you’d like to get a better idea about what a pipeline is, refer to the \\nsection titled “Pipelines and Automation” further down in this chapter. \\nFor now, a pipeline  is an infrastructure that contains a sequence of \\ncomponents manipulating information as it passes through the pipeline. \\nThe function of the pipeline can slightly differ within the setups, so be \\nsure to refer to the graphs and explanations to get a better idea of how the \\npipeline in the example functions.\\nThe main feature of this type of setup is that the deployed model \\nhas pipelines established to continuously train it on new data, even \\nafter deployment. Automation of the experimental stage, or the model',\n",
       " 'has pipelines established to continuously train it on new data, even \\nafter deployment. Automation of the experimental stage, or the model \\ndevelopment stage, also emerges along with modularization of code \\nto allow for further automation in the subsequent steps. In this setup, \\ncontinuous delivery  refers to expedited development and deployment of \\nnew machine learning models. With the barriers to rapid deployment lifted Chapter 3  What Is MLOps?88(the tediousness of manual work in the experimental stage) by automation, \\nmodels can now be created or updated at a much faster pace.\\nContinuous integration/continuous delivery of pipelines  refers to \\na setup where pipelines in the experimental stage are thoroughly tested \\nin an automated process to make sure all components work as intended.',\n",
       " 'a setup where pipelines in the experimental stage are thoroughly tested \\nin an automated process to make sure all components work as intended. \\nFrom there, pipelines are packaged and deployed, where deployment \\nteams deploy the pipeline to a test environment, handle additional testing \\nto ensure both compatibility and functionality, and then deploy it to the \\nproduction environment. In this setup, pipelines can now be created \\nand deployed at a quick pace, allowing for teams to continuously create \\nnew pipelines built around the latest in machine learning architectures \\nwithout any of the resource barriers associated with manual testing and \\nintegration.\\n Manual Implementation\\nNow that we’ve established three variations of setups, let’s look at the first',\n",
       " 'without any of the resource barriers associated with manual testing and \\nintegration.\\n Manual Implementation\\nNow that we’ve established three variations of setups, let’s look at the first \\nof the three deployment setups of machine learning models, which has no \\nMLOps principles integrated.\\nIn this case, there is a team of data scientists and machine learning \\nengineers, who will now be referred to as the “model development team, ” \\nmanually performing data analysis and building, training, testing, and \\nvalidating their models. Once their model has been finalized, they must \\ncreate a model class and push this to a code repository. Software engineers \\nextract this model class and integrate it into an existing application or',\n",
       " 'create a model class and push this to a code repository. Software engineers \\nextract this model class and integrate it into an existing application or \\nsystem, and operational teams are in charge of monitoring the application, \\nmaintaining functionality, and providing feedback to both the software \\nand model development teams.\\nEverything here is manual, meaning any new trends in the data lead \\nto the model development team having to update the model and repeat \\nthe entire process again. This is quite likely to happen considering the \\nhigh volume of users interacting with your model every day. Combined Chapter 3  What Is MLOps?89with performance metrics and user data collection, the information will \\nreveal a lot of aspects about your model as well as the user base the model',\n",
       " 'reveal a lot of aspects about your model as well as the user base the model \\nis servicing. Chances are high that you will have to update it to maintain \\nits performance on the new data. This is something to keep in mind as you \\nfollow through with the process on the graph.\\nRefer to Figure\\xa0 3-2 for a graphical representation of the setup.\\nLet’s go through this step by step. We can split the flow into roughly \\ntwo parts: the experimental stage , which involves the machine learning \\nside of the entire workflow, and the deployment stage,  which handles \\nintegration of the model into the application and maintaining operations.\\nFigure 3-2.  Graph depicting a possible deployment setup of a \\nmachine learning model without MLOps principles. The arrows with',\n",
       " 'Figure 3-2.  Graph depicting a possible deployment setup of a \\nmachine learning model without MLOps principles. The arrows with \\na dotted border mean that progression to the next step depends upon \\na condition in the current step. For example, in the model validation \\nstep, machine learning engineers must ensure that the model meets a \\nminimum benchmark in performance before pushing a model class to \\nthe repositoryChapter 3  What Is MLOps?90Experimental Stage:\\n 1. Data store:  The data store refers to wherever data \\nrelevant to data analysis and model development \\nis stored. An example of a data store could be using \\nHadoop to store large volumes of data, which can \\nbe used by multiple model development teams. In \\nthis example, data scientists can pull raw data from',\n",
       " 'Hadoop to store large volumes of data, which can \\nbe used by multiple model development teams. In \\nthis example, data scientists can pull raw data from \\nthis data store to start performing experiments and \\nconducting data analysis.\\n 2. Process raw data:  As previously mentioned, raw \\ndata must be processed in order to collect the \\nrelevant information. From there, it must also \\nbe purged of faults and corrupted data. When a \\ncompany collects massive volumes of data every \\nday, some of it is bound to be corrupted or faulty \\nin some way eventually, and it’s important to get \\nrid of these points because they can harm the \\ndata analysis and model development processes. \\nFor example, one null value entry can completely \\ndestroy the training process of a neural network',\n",
       " 'rid of these points because they can harm the \\ndata analysis and model development processes. \\nFor example, one null value entry can completely \\ndestroy the training process of a neural network \\nused for a regression (value prediction) task.\\n 3. Data analysis:  This step involves analyzing all \\naspects of the data. The general gist of it was \\ndiscussed earlier, but in the context of updating \\nthe model, data scientists want to see if there are \\nany new trends or variety in data that they think \\nthe model should be updated on. Since the initial \\ntraining process can be thought of as a small \\nrepresentation of the real-world setting, there is a \\nfair chance that the model will need to be updated Chapter 3  What Is MLOps?91soon after the initial deployment. This does depend',\n",
       " 'representation of the real-world setting, there is a \\nfair chance that the model will need to be updated Chapter 3  What Is MLOps?91soon after the initial deployment. This does depend \\non how many characteristics of the true user base \\nthe original training set captured however, but user \\nbases change over time, and so must the models. By \\n“user base, ” we refer to the actual customers using \\nthe prediction services of the model.\\n 4. Model building stage:  This stage is more or less the \\nsame as what we discussed earlier. The second time \\naround, when updating the model, it could turn out \\nthat slight adjustments to the model layers may be \\nneeded. In some of the worst cases, the current model \\narchitecture being used cannot achieve a high enough',\n",
       " 'that slight adjustments to the model layers may be \\nneeded. In some of the worst cases, the current model \\narchitecture being used cannot achieve a high enough \\nperformance even with new data or architectural \\ntweaks. An entirely new model may have to be built, \\ntrained, and validated. If there are no such issues, \\nthen the model would just be further trained, tested, \\nvalidated, and pushed to the code repository upon \\nmeeting some performance criteria.\\n• An important thing to note about this experimental \\nstage is that it is quite popular for experiments \\nto be conducted using Jupyter notebook. When \\nmodel development teams reach a target level \\nof performance, they must work on building \\na workable model that can be called by other \\ncode. For example, this can be done by creating',\n",
       " 'model development teams reach a target level \\nof performance, they must work on building \\na workable model that can be called by other \\ncode. For example, this can be done by creating \\na model class with various functions that provide \\nfunctionality such as load_weights , predict , and \\nperhaps even evaluate  to allow for easier gathering \\nof performance metrics. Since the true label can’t be \\nknown in real-time settings, evaluation metrics can \\nsimply be something like a root-mean-squared error.Chapter 3  What Is MLOps?92Deployment Stage:\\n 5. Model deployment:  In this case, this step is where \\nsoftware engineers must manually integrate \\nthe model into the system/application they are \\ndeveloping. Whenever the model development \\nteam finishes with their experiments, builds',\n",
       " 'software engineers must manually integrate \\nthe model into the system/application they are \\ndeveloping. Whenever the model development \\nteam finishes with their experiments, builds \\na workable model, and pushes it to the code \\nrepository, the engineering team must manually \\nintegrate it again. Although the process may not be \\nthat bad the second time around, there is still the \\nissue of fixing any potential bugs that may arise from \\nthe new model. Additionally, engineering teams \\nmust also handle testing of not only the model once \\nit is integrated into the application, but also of the \\nrest of the application.\\n 6. Model services:  This step is where the model is \\nfinally deployed and is interacting with the user \\nbase in real time. This is also where the operational',\n",
       " 'rest of the application.\\n 6. Model services:  This step is where the model is \\nfinally deployed and is interacting with the user \\nbase in real time. This is also where the operational \\nteam steps in to help maintain the functionality of \\nthe software. For example, if there are any issues \\nwith some aspect of the model functionality, the \\noperational team must record the bug and forward it \\nto the model development team.\\n 7. Data collection:  The operational team can also \\ncollect raw data and performance metrics. This \\ndata is crucial for the company to operate since \\nthat is how it makes its decisions. For example, the \\ncompany might want to know what service is most \\npopular with the user base, or how well the machine',\n",
       " 'that is how it makes its decisions. For example, the \\ncompany might want to know what service is most \\npopular with the user base, or how well the machine \\nlearning models are performing so far. This job can Chapter 3  What Is MLOps?93be performed by the application as well, storing all \\nthe relevant data in some specific data store related \\nto the application.\\n 8. Data forwarded to data store:  This step is where \\nthe operational team sends the data to the data \\nstore. Because there could be massive volumes of \\ndata collected, it’s fair to assume some degree of \\nautomation on behalf of the operational team on \\nthis end. Additionally, the application itself could \\nalso be in charge of forwarding data it collects to the \\nrelevant data store.\\n Reflection on\\xa0the\\xa0Setup',\n",
       " 'this end. Additionally, the application itself could \\nalso be in charge of forwarding data it collects to the \\nrelevant data store.\\n Reflection on\\xa0the\\xa0Setup\\nRight away, you can notice some problems that may arise from such an \\nimplementation. The first thing to realize is that the entire experimental \\nstage is manual, meaning data scientists and machine learning engineers \\nmust repeat those steps every time. When models are constantly exposed \\nto new data that is more than likely not captured in the original training \\nset, models must frequently be retrained so that they are always up to date \\nwith current trends in user data. Unfortunately, when the entire process of \\nanalyzing new trends, training, testing, and validating data is manual, this',\n",
       " 'with current trends in user data. Unfortunately, when the entire process of \\nanalyzing new trends, training, testing, and validating data is manual, this \\nmay require significant resources over time, which may become unfeasible \\nfor a company without the resources to spare. Additionally, trends in data \\ncan change over time. For example, perhaps the age group with the largest \\nnumber of users logging into the site is comprised of people in their early \\ntwenties. A year later, perhaps the dominant age group is now teenagers. \\nWhat was normal back then isn’t normal now, and this could lead to losses \\nin ad revenues, for example, if that’s the service (targeted advertising) the',\n",
       " 'What was normal back then isn’t normal now, and this could lead to losses \\nin ad revenues, for example, if that’s the service (targeted advertising) the \\nmodel in this case provides.Chapter 3  What Is MLOps?94Another issue is that tools such as Jupyter notebook are very popular \\nfor prototyping and experimenting machine learning and deep learning \\nmodels. Even if the experiments aren’t carried out on notebooks, it’s likely \\nthat work must be done in order to push the model to the source repo. \\nFor example, constructing a model class with some important functions \\nsuch as load_weights , predict , and evaluate  would be ideal for a model \\nclass. Some external code may call upon load_weights()  to set the model \\nweights from different training instances (so if the model has been further',\n",
       " 'class. Some external code may call upon load_weights()  to set the model \\nweights from different training instances (so if the model has been further \\ntrained and updated, simply call this function to get the new model). The \\nfunction predict()  would then be called to make predictions based on \\nsome input data and provide the services the application requires, and \\nthe function evaluate()  would be useful in keeping performance metrics. \\nLive data will almost never have truth labels on it (unless the user provides \\ninstant feedback, like Google’s captchas where you select the correct \\nimages), so a score metric like a root-mean-squared error can be useful \\nwhen keeping track of performance.\\nOnce the model class is completed and pushed, software engineering',\n",
       " 'images), so a score metric like a root-mean-squared error can be useful \\nwhen keeping track of performance.\\nOnce the model class is completed and pushed, software engineering \\nteams must integrate the model class into the overall application/system. \\nThis could prove difficult the first time around, but once the integration \\nhas been completed, updates to the model can be as simple as loading new \\nweights. Unfortunately, model architectures are likely to change, so the \\nsoftware teams must reintegrate new model classes into the application.\\nFurthermore, deep learning is a complicated and rapidly evolving \\nfield. Models that were cutting-edge several years ago can be far surpassed \\nby the current state-of-the-art models, so it’s important to keep updating',\n",
       " 'field. Models that were cutting-edge several years ago can be far surpassed \\nby the current state-of-the-art models, so it’s important to keep updating \\nyour model architectures and to make full use of the new developments in \\nthe field. This means teams must continuously repeat the model-building \\nprocess in order to keep up with developments in the field.\\nHopefully it is more clear that this implementation is quite flawed in \\nhow much work is required to not only create and deploy the model in the \\nfirst place, but also to continuously maintain it and keep it up to par.Chapter 3  What Is MLOps?95Alright, so how would we go about improving it? Where does this \\nMLOps come into play? To answer these questions, let’s look at the second \\nsetup of the three defined earlier.',\n",
       " 'MLOps come into play? To answer these questions, let’s look at the second \\nsetup of the three defined earlier.\\n Continuous Model Delivery\\nThis setup contains pipelines  for automatic training of the deployed \\nmodel as well as for speeding up the experimental process. Refer to \\nFigure\\xa0 3-3 for a graphical representation of this setup.\\nThis is a lot to take in at once, so let’s break it down and follow it \\naccording to the numbers on the graph.\\n 1. Feature store:  This is a data storage bin that \\ntakes the place of the data store in the previous \\nexample. The reason for this is that all data can now \\nbe standardized to a common definition that all \\nFigure 3-3.  Graph depicting a possible deployment setup of a',\n",
       " 'example. The reason for this is that all data can now \\nbe standardized to a common definition that all \\nFigure 3-3.  Graph depicting a possible deployment setup of a \\nmachine learning model with automation via pipelines Chapter 3  What Is MLOps?96processes can use in this instance. For example, the \\nprocesses in the experimental stage will be using the \\nsame input data as the deployed training pipeline \\nbecause all of the data is held to the same definition. \\nWhat is meant by common definition  is that raw \\ndata is cleansed and processed in a procedural \\nway that applies to all relevant raw data. These \\nprocessed features are then held in the feature store \\nfor pipelines to draw from, and it is ensured that \\nevery pipeline uses features processed according to',\n",
       " 'processed features are then held in the feature store \\nfor pipelines to draw from, and it is ensured that \\nevery pipeline uses features processed according to \\nthis standard. This way, any perceived differences \\nin trends between two different pipelines won’t be \\nattributed to deviances in processing procedures.\\nPresume for an instance that you are trying to provide \\nan object detection service that detects and identifies \\nvarious animals in a national park. All video feed \\nfrom the trail cameras (a video can be thought of as \\na sequence of frames) can be stored as raw data, but \\nit can be possible that different trail cameras have \\ndifferent resolutions. Instead of repeating the same \\ndata processing procedure, you can simply apply the \\nsame procedure (normalizing, scaling, and batching',\n",
       " 'different resolutions. Instead of repeating the same \\ndata processing procedure, you can simply apply the \\nsame procedure (normalizing, scaling, and batching \\nthe frames, for example) to the raw videos and store \\nthe features that you know all pipelines will use.\\n 2. Data analysis:  In this step, data analysis is still \\nperformed to give data scientists and machine \\nlearning engineers an idea of what the data looks \\nlike, how it’s distributed, and so on, just like in the \\nmanual setup. Similarly, this step can determine \\nwhether or not to proceed with construction of a \\nnew model or just update the current model.Chapter 3  What Is MLOps?97 3. Automated model building and analysis:  In this \\nstep, data scientists and machine learning engineers',\n",
       " 'new model or just update the current model.Chapter 3  What Is MLOps?97 3. Automated model building and analysis:  In this \\nstep, data scientists and machine learning engineers \\ncan select a model, set any specific hyperparameters, \\nand let the pipeline automate the entire process. \\nThe pipeline will automatically process the data \\naccording to the specifications of this model (take \\nthe case where the features are 331x331x3 images \\nbut this particular model only accepts images that \\nare 224x224x3), build the model, train it, evaluate \\nit, and validate it. During validation, the pipeline \\nmay automatically tune the hyperparameters \\nas well optimize performance. It is possible that \\nmanual intervention may be required in some \\ncases (debugging, for example, when the model is',\n",
       " 'may automatically tune the hyperparameters \\nas well optimize performance. It is possible that \\nmanual intervention may be required in some \\ncases (debugging, for example, when the model is \\nparticularly large and complex, or if the model has a \\nnovel architecture), but automation should otherwise \\ntake care of producing an optimal model. Once this \\noccurs, modularized code is automatically created so \\nthat this pipeline can be easily deployed.\\nEverything in this stage is set up so that the \\nexperimental stage goes very smoothly, requiring \\nonly that the model is built. Depending on the level of \\nautomation implemented, perhaps all that is required \\nis that the model architecture is selected with some \\nhyperparameters specified, and the automation takes',\n",
       " 'automation implemented, perhaps all that is required \\nis that the model architecture is selected with some \\nhyperparameters specified, and the automation takes \\ncare of the rest. Either way, the development process \\nin the experimental stage is sped up massively. With \\nthis stage going faster, more experiments can be \\nperformed too, leading to possible boosts in overall \\nefficiency as productivity is increased and optimal \\nsolutions can be found quicker.Chapter 3  What Is MLOps?98 4. Modularized code:  The experimental stage is set \\nup so that the pipeline and its components are \\nmodularized. In this specific context, the data \\nscientist/machine learning engineer defines and \\nbuilds some model, and the data is standardized to \\nsome definition. Basically, the pipeline should be',\n",
       " 'scientist/machine learning engineer defines and \\nbuilds some model, and the data is standardized to \\nsome definition. Basically, the pipeline should be \\nable to accept any constructed model and perform \\nthe corresponding steps given some data without \\nhardcoding anything. (Meaning there isn’t any code \\nthat will only work for a specific model and specific \\ndata. The code works with generalized cases of \\nmodels and data.)\\nThis is modularization , when the whole system \\nis divided into individual components that each \\nhave their own function, and these components \\ncan be switched out depending on variable inputs. \\nThanks to the modularized code, when the pipeline \\nis deployed, it will be able to accept any new feature \\ndata as needed in order to update the deployed',\n",
       " 'Thanks to the modularized code, when the pipeline \\nis deployed, it will be able to accept any new feature \\ndata as needed in order to update the deployed \\nmodel. Furthermore, this structure also lets it \\nswap out models as needed, so there’s no need to \\nconstruct the entire pipeline for every new model \\narchitecture.\\nThink of it this way: the pipeline is a puzzle piece, \\nand the models along with their feature data are \\nvarious puzzle pieces that can all fit within the \\npipeline. They all have their own “image” on the \\npiece and the other sides can have variable shapes, \\nbut what is important is that they fit with the \\npipeline and can easily be swapped out for others.Chapter 3  What Is MLOps?99 5. Deploy pipeline:  In this step, the pipeline is',\n",
       " 'but what is important is that they fit with the \\npipeline and can easily be swapped out for others.Chapter 3  What Is MLOps?99 5. Deploy pipeline:  In this step, the pipeline is \\nmanually deployed and is retrieved from the \\nsource code. Thanks to its modularization, the \\npipeline setup is able to operate independently \\nand automatically train the deployed model on \\nany new data if needed, and the application is \\nbuilt around the code structure of the pipeline \\nso all components will work with each other \\ncorrespondingly. The engineering team has to build \\nparts of the application around the pipeline and its \\nmodularized components the first time around, but \\nafter that, the pipelines should work seamlessly with \\nthe applications so as long as the structure remains',\n",
       " 'modularized components the first time around, but \\nafter that, the pipelines should work seamlessly with \\nthe applications so as long as the structure remains \\nthe same. Models are simply swapped, unlike before \\nwhen the model had to be manually integrated into \\nthe application. This time, the pipeline must be \\nintegrated into the application, and the models are \\nsimply swapped out.\\nHowever, it is important to mention that pipeline \\nstructures can change depending on the model. The \\nmain takeaway here is that pipelines should be able \\nto handle many more models before having to be \\nrestructured compared to the setup before where \\n“swapping” models meant you only loaded updated \\nweights. Now, if several architectures all have \\ncommon training, testing, and validation code, they',\n",
       " 'restructured compared to the setup before where \\n“swapping” models meant you only loaded updated \\nweights. Now, if several architectures all have \\ncommon training, testing, and validation code, they \\ncan all be used under the same pipeline.\\n 6. Automated training pipeline:  This pipeline \\ncontains the model that provides its services and \\nis set up to automatically fetch new features upon \\nactivation of the trigger. The conditions for trigger Chapter 3  What Is MLOps?100activation will be discussed in item 10. When the \\npipeline finishes updating a trained model, the \\nmodel is saved to a model registry, a type of storage \\nunit that holds trained models for ease of access.\\n 7. Model registry:  This is a storage unit that \\nspecifically holds model classes and/or weights. The',\n",
       " 'unit that holds trained models for ease of access.\\n 7. Model registry:  This is a storage unit that \\nspecifically holds model classes and/or weights. The \\npurpose of this unit is to hold trained models for \\neasy retrieval by an application, for example, and it \\nis a good component to add to an automation setup. \\nWithout the model registry, the model classes and \\nweights would just be saved to whatever source code \\nrepository is established, but this way, we make the \\nprocess simpler by providing a centralized area of \\nstorage for these models. It also serves to bridge the \\ngap between model development teams, software \\ndevelopment teams, and operational teams since it \\nis accessible by everyone, which is ultimately what \\nwe want in an ideal automation setup.',\n",
       " 'gap between model development teams, software \\ndevelopment teams, and operational teams since it \\nis accessible by everyone, which is ultimately what \\nwe want in an ideal automation setup.\\nThis registry along with the automated training \\npipeline assures continuous delivery of model \\nservices  since models can frequently be updated, \\npushed to this registry, and deployed without having \\nto go through the entire experimental stage.\\n 8. Model services:  Here the application pulls the \\nlatest, best performing model from the model \\nregistry and makes use of its prediction services. \\nThis action then goes on to provide the desired \\nfunctionality in the application.Chapter 3  What Is MLOps?101 9. Performance and user data collection:  New \\ndata is collected as usual along with performance',\n",
       " 'functionality in the application.Chapter 3  What Is MLOps?101 9. Performance and user data collection:  New \\ndata is collected as usual along with performance \\nmetrics related to the model. This data goes to \\nthe feature store, where the new data is processed \\nand standardized so that it can be used in both the \\nexperimental stage and the deployment stage and \\nthere are no discrepancies between the data used by \\neither stage. Performance data is stored so that data \\nscientists can tell how the model is performing once \\ndeployed. Based on that data, important decisions \\nsuch as whether or not to build a new model with a \\nnew architecture can be made.\\n 10. Training pipeline trigger:  This trigger, upon \\nactivation, initiates the automated training pipeline',\n",
       " 'such as whether or not to build a new model with a \\nnew architecture can be made.\\n 10. Training pipeline trigger:  This trigger, upon \\nactivation, initiates the automated training pipeline \\nfor the deployed model and allows for feature \\nretrieval by the pipeline from the feature store. The \\ntrigger can have any of the following conditions, \\nalthough it is not limited to them:\\n• Manual trigger:  Perhaps the model is to be trained \\nonly if the process is manually initiated. For \\nexample, data science teams can choose to start \\nthis process after reviewing performance and data \\nand concluding that the deployed model needs to \\ntrain on fresh batches of data.\\n• Scheduled training:  Perhaps the model is set to \\ntrain on a specific schedule. This can be a certain',\n",
       " 'and concluding that the deployed model needs to \\ntrain on fresh batches of data.\\n• Scheduled training:  Perhaps the model is set to \\ntrain on a specific schedule. This can be a certain \\ntime on the weekend, every night during hours of \\nlowest traffic, every month, and so on.Chapter 3  What Is MLOps?102• Performance issues:  Perhaps performance data \\nindicates that the model’s performance has dipped \\nbelow a certain benchmark. This can automatically \\nactivate the training process to attempt to get the \\nperformance back up to par. If this is not possible \\nor is taking too many resources, data scientists and \\nmachine learning engineers can choose to build \\nand deploy a new model.\\n• Changes in  data patterns:  Perhaps changes in \\nthe trends of the data have been noticed while',\n",
       " 'machine learning engineers can choose to build \\nand deploy a new model.\\n• Changes in  data patterns:  Perhaps changes in \\nthe trends of the data have been noticed while \\ncreating the features in the feature store. Of course, \\nthe feature store isn’t the only possible place that \\ncan analyze data and identify any new trends \\nor changes in the data. There can be a separate \\nprocess/program dedicated to this task, which can \\ndecide whether or not to activate the trigger.\\nThis would also be a good condition to begin the \\ntraining process, since the new trends in the data \\nare likely to lead to performance degradation. \\nInstead of waiting for the performance hit to \\nactivate the trigger, the model can begin training \\non new data immediately upon sufficient',\n",
       " 'are likely to lead to performance degradation. \\nInstead of waiting for the performance hit to \\nactivate the trigger, the model can begin training \\non new data immediately upon sufficient \\ndetection of such changes in the data, allowing \\nfor the company to minimize any potential losses \\nfrom such a scenario.\\n Reflection on\\xa0the\\xa0Setup\\nThis implementation fixes many of the issues from the previous setup. \\nThanks to the integration of pipelines in the experimental stage, the \\nprevious problem of having the entire stage be composed of manual Chapter 3  What Is MLOps?103processes is no longer a concern. The pipeline automates the whole \\nprocess of training, evaluating, and validating a model. The model \\ndevelopment team now only needs to build the model and reuse any',\n",
       " 'process of training, evaluating, and validating a model. The model \\ndevelopment team now only needs to build the model and reuse any \\ncommon training, evaluation, and validation procedures that are still \\napplicable to this model. At the end of the model development pipeline, \\nrelevant model metrics are collected and displayed to the operator. These \\nmetrics can help the model development team to prototype quickly and \\narrive at optimal solutions even faster than they would have without the \\nautomation since they can run multiple pipelines on different models and \\ncompare all of them at once.\\nAutomated model creation pipelines in the experimental stage \\nallow for teams to respond faster to any significant changes in the data',\n",
       " 'compare all of them at once.\\nAutomated model creation pipelines in the experimental stage \\nallow for teams to respond faster to any significant changes in the data \\nor any issues with the deployed model that need to be resolved. Unlike \\nbefore, where the only model swapping was the result of loading updated \\nweights for the same model, these pipelines are structured to allow for \\nvarious models with different architectures as long as they all use the \\nsame training, evaluation, and validation procedures. Thanks to the \\nmodularized code, the pipeline can simply swap out model classes and \\ntheir respective weights once deployed. The modularization allows for \\neasier deployment of the pipeline and lets models be swapped out easily to',\n",
       " 'their respective weights once deployed. The modularization allows for \\neasier deployment of the pipeline and lets models be swapped out easily to \\nallow for further training of any model during deployment. Should a model \\nrequire special attention from the model development team, it can simply \\nbe trained further by the team and swapped back in once it is ready. Now \\nteams can respond much more quickly by being able to swap models in \\nand out in such a manner.\\nThe pipelines also make it much easier for software engineering \\nteams and operational teams to deploy the pipelines and models. Because \\neverything is modularized, teams do not have to work on integrating \\nnew model classes into the application every time. Everyone benefits,',\n",
       " 'everything is modularized, teams do not have to work on integrating \\nnew model classes into the application every time. Everyone benefits, \\nand model development teams do not have to be as hesitant about \\nimplementing new architectures so as long as the new model still uses the \\nsame training, evaluation, and validation code as in the existing pipeline.Chapter 3  What Is MLOps?104While this setup solves most of the issues that plagued the original \\nsetup, there are still some important problems that remain. Firstly, there \\nare no mechanisms in place to test and debug the pipelines, so this must \\nall be done manually before it is pushed to a source repository. This can \\nbecome a problem when you’re trying to push many iterations of pipelines,',\n",
       " 'all be done manually before it is pushed to a source repository. This can \\nbecome a problem when you’re trying to push many iterations of pipelines, \\nsuch as when you’re building different models with architectures that \\ndiffer in how they must be trained, tested, and validated. Perhaps the latest \\nmodels are showing a vast improvement over the old state-of-the art, and \\nyour team wants to implement these new solutions as soon as possible. In \\nsituations like this, teams will frequently need to debug and test pipelines \\nbefore pushing them to source code for deployment. In this case, there is \\nstill some automation left to be done to avoid manual work.\\nPipelines are also manually deployed, so if the structure in the code',\n",
       " 'still some automation left to be done to avoid manual work.\\nPipelines are also manually deployed, so if the structure in the code \\nchanges, the engineering teams must rebuild parts of the application to \\nwork with the new pipeline and its modularized code. Modularization \\nworks smoothly when all components know what to expect from \\neach other, but if the code of one of the components changes so that \\nit isn’t compatible anymore, either the application must be rebuilt to \\naccommodate the new changes or the component must be rewritten to \\nwork with the original pipeline. Unfortunately, new model architectures \\nmay require that part of the pipeline itself be rewritten, so it is likely \\nthat the application itself must be worked on to accommodate the new \\npipeline.',\n",
       " 'may require that part of the pipeline itself be rewritten, so it is likely \\nthat the application itself must be worked on to accommodate the new \\npipeline.\\nHopefully you begin to see the vast improvements that automation \\nhas made in this setup, but also the issues that remain to be solved. The \\nautomation has solved the issue of building and creating new models, but \\nthe problem of building and creating new pipelines still remains.\\nTo find an answer to that problem, let’s take a look at the last of the \\nthree setups defined earlier.Chapter 3  What Is MLOps?105 Continuous Integration/Continuous Delivery \\nof\\xa0Pipelines\\nIn this setup, we will be introducing a system to thoroughly test pipeline \\ncomponents before they are packaged and ready to deploy. This will ensure',\n",
       " 'of\\xa0Pipelines\\nIn this setup, we will be introducing a system to thoroughly test pipeline \\ncomponents before they are packaged and ready to deploy. This will ensure \\ncontinuous integration of pipeline code  along with continuous delivery of \\npipelines , crucial elements of the automation process that the previous setup \\nwas missing. Refer to Figure\\xa0 3-4 for a graphical representation of such a setup.\\nThough this is mostly the same setup, we will go through it again step \\nby step with an emphasis on the newly introduced elements.\\n 1. Feature store:  The feature store contains \\nstandardized data processed into features. Features \\ncan be pulled by data scientists for offline data \\nanalysis. Upon activation of the trigger, features can \\nalso be sent to the automated training pipeline to',\n",
       " 'can be pulled by data scientists for offline data \\nanalysis. Upon activation of the trigger, features can \\nalso be sent to the automated training pipeline to \\nfurther train the deployed model.\\nFigure 3-4.  Graph depicting added testing systems and a package \\nstore to the automation setup in Figure\\xa0 3-2Chapter 3  What Is MLOps?106 2. Data analysis:  This step is performed by data \\nscientists on features pulled from the feature store. \\nThe results from the analysis can determine whether \\nor not to build a new model or adjust the architecture \\nof an existing model and retrain it from there.\\n 3. Automated model building and analysis:  This \\nstep is performed by the model development team. \\nModels can be built by the team and passed into the',\n",
       " 'of an existing model and retrain it from there.\\n 3. Automated model building and analysis:  This \\nstep is performed by the model development team. \\nModels can be built by the team and passed into the \\npipeline, assuming that they are compatible with the \\ntraining, testing, and validation code, and the entire \\nprocess is automatically conducted with a model \\nanalysis report generated at the end. In the case \\nwhere the team wants to implement some of the \\nlatest machine learning architectures, models will \\nhave to be created from scratch with integration into \\npipelines in mind to maintain modularity. Parts of \\nthe pipeline code may have to change as well, which \\nis acceptable because the new components of this \\nsetup can handle this automatically.',\n",
       " 'pipelines in mind to maintain modularity. Parts of \\nthe pipeline code may have to change as well, which \\nis acceptable because the new components of this \\nsetup can handle this automatically.\\n 4. Modularized code:  Once the model reaches a \\nminimum level of performance in the validation \\nstep, the pipeline, its components, and the model \\nare all ready to be modularized and stored in a \\nsource repository.\\n 5. Source repository:  The source repository holds \\nall of the packaged pipeline and model code for \\ndifferent pipelines and different models. Teams can \\ncreate multiples at once for different purposes and \\nstore them all here. In the old setup, pipelines and \\nmodels would be pulled from here and manually Chapter 3  What Is MLOps?107integrated and deployed by software engineering',\n",
       " 'store them all here. In the old setup, pipelines and \\nmodels would be pulled from here and manually Chapter 3  What Is MLOps?107integrated and deployed by software engineering \\nteams. In this setup, the modularized code must \\nnow be tested to make sure all of the components \\nwill work correctly.\\n 6. Testing:  This step is crucial in achieving continuous \\nintegration , or a result of automation where \\nnew components and elements are continuously \\ndesigned, built, and deployed in the new \\nenvironment.\\nPipelines and their components, including the \\nmodel, must be thoroughly tested to ensure that \\nall outputs are correct. Furthermore, the pipelines \\nthemselves must be tested so that they are \\nguaranteed to work with the application and how it',\n",
       " 'model, must be thoroughly tested to ensure that \\nall outputs are correct. Furthermore, the pipelines \\nthemselves must be tested so that they are \\nguaranteed to work with the application and how it \\nis designed. There shouldn’t be bugs in the pipeline, \\nfor example, that would break its compatibility with \\nthe application. The application is programmed to \\nexpect a specific behavior from the pipeline, and the \\npipeline must behave correspondingly.\\nIf you are familiar with software development, the \\ntesting of pipeline components and the models is \\nsimilar to the automated testing that developers \\nwrite to check various parts of an application’s \\nfunctionality. A simple example is automated testing \\nto ensure data of various types are successfully',\n",
       " 'write to check various parts of an application’s \\nfunctionality. A simple example is automated testing \\nto ensure data of various types are successfully \\nreceived by the server and are added to the correct \\ndatabases.Chapter 3  What Is MLOps?108With pipelines and machine learning models, some \\nexamples of testing include:\\n• Does the validation testing procedure lead to \\ncorrect tuning of the hyperparameters?\\n• Does each pipeline component work correctly? \\nDoes it output the expected element? For example, \\nafter model evaluation, does it correctly begin the \\nvalidation step? (Alternatively, if model evaluation \\ngoes after model validation, does the evaluation \\nstep correctly initiate?)\\n• Is the data processing performed correctly? Are \\nthere any issues with the data post-processing',\n",
       " 'goes after model validation, does the evaluation \\nstep correctly initiate?)\\n• Is the data processing performed correctly? Are \\nthere any issues with the data post-processing \\nthat would lead to poor model performance? \\nAvoiding this outcome is for the best since it would \\nwaste resources having to fix the data processing \\ncomponent. If the business relies on rapid pipeline \\ndeployment, then avoiding this type of scenario is \\neven more crucial.\\n• Does the data processing component correctly \\nperform data scaling? Does it correctly perform \\nfeature engineering? Does it correctly transform \\nimages?\\n• Does the model analysis work correctly? You \\nwant to make sure that you’re basing decisions \\non accurate data. If the model truly performs well \\nbut faults in the model analysis component of the',\n",
       " '• Does the model analysis work correctly? You \\nwant to make sure that you’re basing decisions \\non accurate data. If the model truly performs well \\nbut faults in the model analysis component of the \\npipeline lead the data scientist/machine learning \\nengineer to believe the model isn’t performing that \\nwell, then it could lead to issues where pipeline \\ndeployment is slowed down. Likewise, you don’t Chapter 3  What Is MLOps?109want the model analysis to be displaying the wrong \\ninformation, even if it mistakenly displays precision \\nfor accuracy.\\nThe more thorough the automated testing, the \\nbetter the guarantee that the pipeline will operate \\nwithin the application without issues. (This doesn’t \\nnecessarily include model performance as that has',\n",
       " 'better the guarantee that the pipeline will operate \\nwithin the application without issues. (This doesn’t \\nnecessarily include model performance as that has \\nto do more with the model architecture, how the \\nmodel is developed, and what it is capable of.)\\nOnce the pipeline passes all the tests, it is then \\nautomatically packaged and sent to a package store. \\nContinuous integration of pipelines is now achieved \\nsince teams can build modularized and tested \\npipelines much more quickly and have them ready \\nfor deployment.\\n 7. Package store:  The package store is a containment \\nunit that holds various packaged pipelines. It is \\noptional but included in this setup so that there \\nis a centralized area where all teams can access \\npackaged pipelines that are ready for deployment.',\n",
       " 'optional but included in this setup so that there \\nis a centralized area where all teams can access \\npackaged pipelines that are ready for deployment. \\nModel development teams push to this package \\nstore, and software engineers and operational \\nteams can retrieve a packaged pipeline and deploy \\nit. In this way, it is similar to the model registry in \\nthat both help achieve continuous delivery . The \\npackage store helps achieve continuous delivery of \\npipelines just as the model registry helps achieve \\ncontinuous delivery of models and model services.\\nThanks to automated testing providing continuous \\nintegration of pipelines and continuous delivery of Chapter 3  What Is MLOps?110pipelines via the package store, pipelines can also be \\ndeployed rapidly by operational teams and software',\n",
       " 'integration of pipelines and continuous delivery of Chapter 3  What Is MLOps?110pipelines via the package store, pipelines can also be \\ndeployed rapidly by operational teams and software \\nengineers. With this, businesses can easily keep \\nup with the latest trends and advances in machine \\nlearning architectures, allowing for better and better \\nperformance and more involved services.\\n 8. Deploy pipeline:  Pipelines can be retrieved \\nfrom the package store and deployed in this step. \\nSoftware engineering and operational teams must \\nensure that the pipeline will integrate without \\nincident into the application. Because of that, \\nthere can be more testing on the part of software \\nengineering teams to ensure proper integration of \\nthe pipeline. For example, one test can be to ensure',\n",
       " 'there can be more testing on the part of software \\nengineering teams to ensure proper integration of \\nthe pipeline. For example, one test can be to ensure \\nthe dependencies of the pipeline are considered \\nin the application (if, for example, TensorFlow has \\nupdated and contains new functionality the pipeline \\nnow uses, the application should update its version \\nof TensorFlow as well).\\nTeams usually want to deploy the pipelines into \\na test environment where it will be subjected \\nto further automated testing to ensure full \\ncompatibility with the application. This can be \\ndone automatically, where the pipelines go from \\nthe package store into the test environment, or \\nmanually, where teams decide to deploy the \\npipeline into the test environment. After the',\n",
       " 'done automatically, where the pipelines go from \\nthe package store into the test environment, or \\nmanually, where teams decide to deploy the \\npipeline into the test environment. After the \\npipeline passes all the tests, teams can choose to \\nmanually deploy the pipeline into the production \\nenvironment or have it automatically done.Chapter 3  What Is MLOps?111Either way, pipeline creation and deployment is a \\nmuch faster process now especially since teams do \\nnot have to manually test the pipelines and they do \\nnot have to build or modify the application to work \\nwith the pipeline every time.\\n 9. Automated training pipeline:  The automated \\ntraining pipeline, once deployed, exists to further \\ntrain models upon activation of the trigger. This',\n",
       " 'with the pipeline every time.\\n 9. Automated training pipeline:  The automated \\ntraining pipeline, once deployed, exists to further \\ntrain models upon activation of the trigger. This \\nhelps keep models as up to date as possible on new \\ntrends in data and maintain high performance for \\nlonger. Upon validation of the model, models are \\nsent to the model registry where they are held until \\nthey are needed for services.\\n 10. Model registry:  The model registry holds trained \\nmodels until they are needed for their services. \\nOnce again, continuous delivery of model services \\nis achieved as the automated training pipeline \\ncontinuously provides the model registry with high-\\nperformance machine learning models to be used to \\nperform various services.',\n",
       " 'is achieved as the automated training pipeline \\ncontinuously provides the model registry with high-\\nperformance machine learning models to be used to \\nperform various services.\\n 11. Model services:  The best models are pulled from \\nthe model registry to perform various services for \\nthe application.\\n 12. Performance and user data collection:  Model \\nperformance data and user data is collected to be \\nsent to model development teams and the feature \\nstore, respectively. Teams can use the model \\nperformance metrics along with the results from \\nthe data analysis to help decide their next course of \\naction.Chapter 3  What Is MLOps?112 13. Training pipeline trigger:  This step involves some \\ncondition being met (refer to the previous setup, \\ncontinuous model delivery ) to initiate the training',\n",
       " 'action.Chapter 3  What Is MLOps?112 13. Training pipeline trigger:  This step involves some \\ncondition being met (refer to the previous setup, \\ncontinuous model delivery ) to initiate the training \\nprocess of the deployed pipeline and feed it with \\nnew feature data pulled from the feature store.\\n Reflection on\\xa0the\\xa0Setup\\nThe main issue of the previous setup that this one fixes is that of pipeline \\ndeployment. Previously, pipelines had to be manually tested by machine \\nlearning teams and operational teams to ensure that the pipeline and \\nits components worked, and that the pipeline and its components \\nwere compatible with the application. However, in this setup, testing is \\nautomated, allowing for teams to much more easily build and deploy',\n",
       " 'were compatible with the application. However, in this setup, testing is \\nautomated, allowing for teams to much more easily build and deploy \\npipelines than before. The biggest advantage to this is that businesses can \\nnow keep up with significant changes in the data requiring the creation \\nof new models and new pipelines, and can also capitalize on the latest \\nmachine learning trends and architectures all thanks to rapid pipeline \\ncreation and deployment combined with continuous delivery of model \\nservices from the previous setup.\\nThe important thing to understand from all these examples is that \\nautomation is the way to go. Machine learning technology has progressed \\nincredibly far within the last decade alone, but finally, the infrastructure to',\n",
       " 'automation is the way to go. Machine learning technology has progressed \\nincredibly far within the last decade alone, but finally, the infrastructure to \\nallow you to capitalize on these advancements is catching up.\\nHopefully, after seeing the three possible MLOps setups, you \\nunderstand more about MLOps and how implementations of MLOps \\nprinciples might look. You might have noticed that pipelines have been \\nmentioned quite often throughout the descriptions of the setups, and you \\nmight be wondering, “What are pipelines, and why are they so crucial for \\nautomation?”\\nTo answer that question, let’s take a look at what a “pipeline” really is.Chapter 3  What Is MLOps?113 Pipelines and\\xa0Automation\\nPipelines are an important part of automation setups employing DevOps',\n",
       " 'To answer that question, let’s take a look at what a “pipeline” really is.Chapter 3  What Is MLOps?113 Pipelines and\\xa0Automation\\nPipelines are an important part of automation setups employing DevOps \\nprinciples. One way to think about a pipeline  is that it is a specific, often \\nsequential procedure that dictates the flow of information as it passes \\nthrough the pipeline. To see an example of a testing pipeline in a software \\ndevelopment setting, refer to Figure\\xa0 3-5.\\nIn the MLOps setups above, you’ve seen pipelines for automating the \\nprocess of training a deployed model and for building, testing, and packing \\npipelines as well as for testing integration of packaged pipelines before \\ndeploying them to the production environment.',\n",
       " 'process of training a deployed model and for building, testing, and packing \\npipelines as well as for testing integration of packaged pipelines before \\ndeploying them to the production environment.\\nSo, what does all that really mean? To get a better idea of what exactly \\ngoes on in a pipeline, let’s follow the flow of data through a pipeline in the \\nexperimental stage. Even if you understand how pipelines work, it may \\nbe worth following the example anyway as we now look at this pipeline \\nthrough the context of using MLOps APIs.\\nFigure 3-5.  A tes ting pipeline in a software development setting. \\nThe pipeline for testing packaged model pipelines in the optimal \\nsetup above is similar in that individual components must be tested,',\n",
       " 'The pipeline for testing packaged model pipelines in the optimal \\nsetup above is similar in that individual components must be tested, \\ncomponents must be tested in groups, and in the case where pipelines \\nare deployed to a test environment first where further tests are \\nperformed before they are deployed to the production environmentChapter 3  What Is MLOps?114 Journey Through a\\xa0Pipeline\\nWe will be looking at the model development pipeline in the experimental \\nstage. Before we begin, it is important to mention that we will be \\nreferencing API calls in this pipeline. This is because some APIs can be \\ncalled while executing scripts or even Jupyter cells at key points in the \\nmodel’s development, giving MLOps monitoring software information on',\n",
       " 'called while executing scripts or even Jupyter cells at key points in the \\nmodel’s development, giving MLOps monitoring software information on \\nmodel training, model evaluation, and model validation. At the end of the \\npipeline, the MLOps software would also ready the model for deployment \\nvia functionality provided by the API.\\nYou will read more about this API in the next chapter, Chapter 4, but \\nfor now, you may assume that the API will take care of automation as you \\nfollow along through the example.\\n Model Selection\\nAs seen in Figure\\xa0 3-4, the experimental pipeline begins with the selection \\nof a model. This is up to the operator, who must now choose and build a \\nmodel. Some APIs allow you to call their functionality while building the',\n",
       " 'of a model. This is up to the operator, who must now choose and build a \\nmodel. Some APIs allow you to call their functionality while building the \\nmodel to connect with MLOps software as the rest of the process goes \\non. This software then keeps track of all relevant metrics related to the \\nmodel’s development along with the model itself in order to initiate the \\ndeployment process.\\nIn this case, the operator has chosen to use a logistic regression. Refer \\nto Figure\\xa0 3-6.Chapter 3  What Is MLOps?115 Data Preprocessing\\nWith the model now selected and built, and with feature data supplied by \\nthe feature store, the process can now move forward to the next stage in \\nthe pipeline: data preprocessing. Refer to Figure\\xa0 3-7.\\nFigure 3-6.  A graphical representation of a pipeline where the',\n",
       " 'the feature store, the process can now move forward to the next stage in \\nthe pipeline: data preprocessing. Refer to Figure\\xa0 3-7.\\nFigure 3-6.  A graphical representation of a pipeline where the \\noperator has selected a logistic regression model. The rest of the steps \\nhave been hidden for now and will appear as we gradually move \\nthrough the pipelineChapter 3  What Is MLOps?116The data preprocessing can be done manually or automatically. In \\nthis case, the data preprocessing only involves normalization and resizing \\nof image feature data, so the operator can implement this manually. \\nDepending on the level of automation, the operator can also call some \\nfunction that takes in data and automatically processes it depending on \\nthe type of data and any other parameters provided.',\n",
       " 'Depending on the level of automation, the operator can also call some \\nfunction that takes in data and automatically processes it depending on \\nthe type of data and any other parameters provided.\\nEither way, the end of the processing stage will result in the data being \\nbroken up into subsets. In this example, the operator chose to create a \\ntraining set, a testing set, and a validation set. Now, the operator can begin \\nthe training process.\\n Training Process\\nDepending on the framework being used, the operator can further split \\nup the training data into a training set and a data validation set and use \\nboth in the training process. The data validation set exists totally separate \\nFigure 3-7.  The operator has chosen to normalize and resize the',\n",
       " 'both in the training process. The data validation set exists totally separate \\nFigure 3-7.  The operator has chosen to normalize and resize the \\nimage data. The process creates a training set, a testing set, and a \\nvalidation setChapter 3  What Is MLOps?117from the training set (although it is derived from it) since the model never \\nsees it during training. Its purpose is to periodically evaluate the model’s \\nperformance on a data set that it has never seen before. Refer to Figure\\xa0 3-8.\\nIn the context of deep learning, for example, the model can evaluate \\non the validation set at the end of each epoch, generating some metric \\ndata for the operator to see. Based on this, the operator can judge how \\nthe model is doing and whether or not it could be overfitting and adjust',\n",
       " 'data for the operator to see. Based on this, the operator can judge how \\nthe model is doing and whether or not it could be overfitting and adjust \\nhyperparameters or model structure if needed.\\nThe API can also be told what script to run in order to initiate this \\nentire pipeline process. The script can contain the training, evaluation, \\nand validation code all at once so the API can run this entire pipeline when \\nneeded.\\nOnce the training process is done, the process moves to the evaluation \\nstage.\\nFigure 3-8.  The model training process beginsChapter 3  What Is MLOps?118 Model Evaluation\\nIn the evaluation stage, the model’s performance is measured on a test \\ndata set that it has never seen. This performance will indicate to the',\n",
       " 'In the evaluation stage, the model’s performance is measured on a test \\ndata set that it has never seen. This performance will indicate to the \\noperator whether or not the model is overfitting, especially if it performed \\nextremely well in training but has trouble replicating those results in this \\nstage. That is part of why the training data can be split to include some \\nvalidation data, as it can be an early indicator of overfitting. This can be \\ncrucial especially if the model takes a significant amount of time to run. \\nYou would rather know earlier, partway through training, if the model \\nis overfitting, rather than after it ran all night and is evaluated the next \\nmorning. Refer to Figure\\xa0 3-9.\\nFigure 3-9.  Training results are stored in a common area (for',\n",
       " 'is overfitting, rather than after it ran all night and is evaluated the next \\nmorning. Refer to Figure\\xa0 3-9.\\nFigure 3-9.  Training results are stored in a common area (for \\nexample, the API could be called to monitor these results) for the \\nmetrics of the current model. Model evaluation begins on the trained \\nmodel using the testing setChapter 3  What Is MLOps?119Another thing to note again is that the validation stage could come \\nbefore the evaluation stage, but in this case, the trained model will be \\nevaluated first on a test data set before the validation stage begins. This \\nis just to get a sense of how the model does on the testing set before \\nhyperparameter tuning begins. Of course, hyperparameter tuning via the',\n",
       " 'is just to get a sense of how the model does on the testing set before \\nhyperparameter tuning begins. Of course, hyperparameter tuning via the \\nvalidation step could be performed first before the final model evaluation, \\nbut in some frameworks, model evaluation would come first. An example \\nof this is a validation process like scikit-learn’s cross-validation. Of course, \\nyou can evaluate the tuned model on the test set once again to get a final \\nperformance evaluation.\\nOnce the evaluation finishes, metrics are stored by the API or by some \\nother mechanism that the team has implemented, and the process moves \\non to the validation stage.\\n Model Validation\\nIn this stage, the model begins the validation process, which attempts',\n",
       " 'other mechanism that the team has implemented, and the process moves \\non to the validation stage.\\n Model Validation\\nIn this stage, the model begins the validation process, which attempts \\nto seek the best hyperparameters. You could combine the use of a script \\nto iterate through various configurations of hyperparameter values \\nand utilize k-fold cross-validation, for example, to help decide the best \\nhyperparameters. Refer to Figure\\xa0 3-10 .Chapter 3  What Is MLOps?120In any case, the point of a validation set is to help tune the model’s \\nhyperparameters. The team could even automate this process entirely \\nif they tend to train a lot of models of the same few types, saving \\ntime and resources in the long run by automating the validation and',\n",
       " 'if they tend to train a lot of models of the same few types, saving \\ntime and resources in the long run by automating the validation and \\nhyperparameter tuning process for that set of models.\\nFinally, once the model achieves a good level of performance and \\nfinishes the validation stage, the validation results are stored, and all \\nrelevant data is displayed as a summary to the operator. Again, depending \\non the level of automation, perhaps the model is retrained and evaluated \\non the best hyperparameter setup discovered in the validation stage. The \\nAPI simply needs to be told what metrics to track and it will automatically \\ndo so.\\nFigure 3-10.  Evaluation metrics are stored along with the training',\n",
       " 'API simply needs to be told what metrics to track and it will automatically \\ndo so.\\nFigure 3-10.  Evaluation metrics are stored along with the training \\nmetrics by the API, and the validation process beginsChapter 3  What Is MLOps?121 Model Summary\\nAt this point, the operator can compare the outcome of this experiment \\nwith that of other models, using the metrics as baselines for comparison. \\nThe API can track the relevant metrics for different model runs and can \\ncompare them all at the same time. Should the operator decide to move \\nforward with this particular model, the API and the MLOps software \\ncan allow for deployment on a simple click of a button. Usually, the \\ndeployment is to a staging environment first, where the functionality can',\n",
       " 'can allow for deployment on a simple click of a button. Usually, the \\ndeployment is to a staging environment first, where the functionality can \\nbe tested further before moving directly into the production environment. \\nEverything is configurable, and the API can adapt to the needs of the \\nbusiness and its workflow. If the developers want to deploy straight to \\nproduction, sure, though that could potentially be unwise considering the \\ncase of failure. Refer to Figure\\xa0 3-11 .\\nFigure 3-11.  Validation is complete, and all metrics are displayed to \\nthe operatorChapter 3  What Is MLOps?122After the model passes the tests in the staging environment, it can \\nthen be deployed to the production environment, where it can be further \\nmonitored by the software.',\n",
       " 'then be deployed to the production environment, where it can be further \\nmonitored by the software.\\nHopefully now you have a better understanding of what a pipeline \\nreally is. The pipelines for models and pipeline integration testing are \\nsimilar, except they are assisted by MLOps software and APIs such as \\nDatabricks and MLFlow, for example. Let’s now look at how you can go \\nabout using those APIs and software to help you implement MLOps.\\n How to\\xa0Implement MLOps\\nMLOps sounds great. It helps you deploy machine learning models rapidly \\nand helps maintain them once they’re deployed. However, the biggest \\nproblem now seems to be the question of how to get there. The level of \\nautomation described in the setups requires significant work from both the',\n",
       " 'problem now seems to be the question of how to get there. The level of \\nautomation described in the setups requires significant work from both the \\n“ML ” and “Ops” sides of the workflow to achieve it. It almost seems better \\nin the short run to build and deploy the models manually rather than \\ndevote resources to setting up the entire infrastructure, but this is simply \\nunsustainable in the long run.\\nAlso, Jupyter is great for performing experiments, so is there a way to \\ntrack them as well? This sort of functionality would be extremely useful \\nespecially when teams are implementing advanced machine learning \\narchitectures from scratch, as it would let them compare the new models \\nacross all of the relevant metrics with deployed models or current',\n",
       " 'architectures from scratch, as it would let them compare the new models \\nacross all of the relevant metrics with deployed models or current \\narchitectures. Tasks like these are more convenient to do in a notebook \\nand having to convert everything to a proper model file is simply further \\nwork.\\nThe takeaway here is that accounting for these factors and more would \\nrequire significant resources to plan, develop, and test. For smaller-scale \\nbusinesses, this is an undertaking that’s possibly beyond their reach. So, \\nwhat now?Chapter 3  What Is MLOps?123The good news is that are a great assortment of tools available to use \\nnow that essentially implement all of the automation for you, such as \\nthe API we looked at in the pipeline example earlier. Several examples of',\n",
       " 'now that essentially implement all of the automation for you, such as \\nthe API we looked at in the pipeline example earlier. Several examples of \\nsuch tools that we will explore in later chapters are MLFlow, Databricks , \\nAWS SageMaker , Microsoft Azure , Google Cloud , and Datarobots . With \\nthese tools, implementing MLOps principles into your workflow will be \\nsignificantly easier.\\nIn the case of MLFlow, integrating it into code is extremely simple. \\nYou only have to write a couple lines of code to track all of the metrics \\nyou need. The functionality of the API we looked at earlier in the pipeline \\nexample is all provided by MLFlow. Furthermore, MLFlow also saves the \\nmodel for you, allowing for model serving functionality where given some \\ndata, the model returns its predictions.',\n",
       " 'example is all provided by MLFlow. Furthermore, MLFlow also saves the \\nmodel for you, allowing for model serving functionality where given some \\ndata, the model returns its predictions.\\nMLFlow also integrates into Databricks, AWS SageMaker, Microsoft \\nAzure, and can be deployed to Google Cloud as well, all of which are \\ntools that help manage your MLOps setup and serve as platforms to \\ndeploy your models on. While the cloud platforms do provide some \\nMLOps functionality, with the extent of this varying for each platform, the \\nadvantage of using MLFlow is that it lets you have the freedom of choice \\nwhen it comes to one platform to commit to. Furthermore, it gives you a \\ngreater degree of freedom, as you can perform all the experiments locally',\n",
       " 'when it comes to one platform to commit to. Furthermore, it gives you a \\ngreater degree of freedom, as you can perform all the experiments locally \\nand offline, and you can support models from many different frameworks. \\nMLFlow also provides functionality to help you modularize any custom-  \\nbuilt models or models made from other frameworks not explicitly \\nsupported.\\nAnd so, to really answer the question of how to implement MLOps, \\nyou will get familiar with MLFlow and explore each of those tools. The goal \\nis to take the model we built in Chapter 2 all the way to deployment and \\nbeyond.Chapter 3  What Is MLOps?124 Summary\\nMLOps is a set of principles and practices adopted from DevOps and \\napplied to machine learning. You explored three different types of MLOps',\n",
       " 'beyond.Chapter 3  What Is MLOps?124 Summary\\nMLOps is a set of principles and practices adopted from DevOps and \\napplied to machine learning. You explored three different types of MLOps \\nsetups with varying degrees of automation: manual implementation , \\ncontinuous model delivery , and continuous integration/continuous \\ndelivery of pipelines . You identified that the manual implementation was \\nriddled with issues regarding scalability and efficiency and you explored \\na setup that ensured continuous model delivery. Although this setup \\nfixed many of the issues found in the manual setup, there were still some \\nproblems with pipeline integration testing to be solved. The final setup \\nsolved this issue too and ensured continuous integration and delivery of',\n",
       " 'problems with pipeline integration testing to be solved. The final setup \\nsolved this issue too and ensured continuous integration and delivery of \\npipelines, completing the total automation setup.\\nYou also looked into what a pipeline really is so that you can \\nunderstand why they are so crucial to the automation setup. Finally, you \\nlearned about some tools that can help you implement MLOps into your \\nworkspace, avoiding the trouble of implementing all the automation from \\nscratch. In the next chapter, you will look at MLFlow, an excellent API that \\nlets you implement your own MLOps setups and is compatible with many \\nplatforms and frameworks.Chapter 3  What Is MLOps?125© Sridhar Alla, Suman Kalyan Adari 2021 \\nS. Alla and S. K. Adari, Beginning MLOps with MLFlow ,',\n",
       " 'platforms and frameworks.Chapter 3  What Is MLOps?125© Sridhar Alla, Suman Kalyan Adari 2021 \\nS. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \\nhttps://doi.org/10.1007/978-1-4842-6549-9_4CHAPTER 4\\nIntroduction \\nto\\xa0MLFlow\\nIn this chapter, we will cover what MLFlow is, what it does, and how \\nyou can implement MLOps setups into your existing projects. More \\nspecifically, we will cover how you can integrate MLFlow with scikit-learn, \\nTensorFlow 2.0+/Keras, PyTorch, and PySpark. We will go over experiment \\ncreation; metric, parameter, and artifact logging; model logging; and how \\nyou can deploy models on a local server and query them for predictions.\\n Introduction\\nIn the previous chapter, we went over what an optimal MLOps setup',\n",
       " 'you can deploy models on a local server and query them for predictions.\\n Introduction\\nIn the previous chapter, we went over what an optimal MLOps setup \\nlooks like. However, the level of automation presented would require \\nan immense amount of resources dedicated to the project. Fortunately, \\nthere are APIs that do this for you, such as MLFlow . MLFlow is an API that \\nallows you to integrate MLOps principles into your projects with minimal \\nchanges made to existing code. With just a couple lines of code here and \\nthere, you can track all of the details relevant to the project that you want. \\nFurthermore, you can even save the model for future use in deployment, \\nfor example, and you can compare all of the metrics between individual',\n",
       " 'Furthermore, you can even save the model for future use in deployment, \\nfor example, and you can compare all of the metrics between individual \\nmodels to help you select the best model.126The great thing about MLFlow is that it abstracts everything for you. \\nIt packages and modularizes the models for you so that when you deploy \\nthe model and want to make predictions, all you need to do is simply pass \\nin the input data in a certain format. All of the modularization that we \\ndiscussed in the previous chapter with the pipelines is taken care of by \\nMLFlow. MLFlow also allows you to create a wrapper around your model \\nif your model prediction code needs to be different. We will look at this \\nfunctionality in detail in the next chapter, when you deploy your models to',\n",
       " 'if your model prediction code needs to be different. We will look at this \\nfunctionality in detail in the next chapter, when you deploy your models to \\nAmazon SageMaker. Even with custom code, MLFlow will modularize it so \\nthat it will still work the same way as any other model once it is deployed \\nand ready to make predictions.\\nIn detail, we will go over the following in this chapter:\\n• Creating experiments:  Experiments in MLFlow \\nessentially allow you to group your models and any \\nrelevant metrics. For example, you can compare \\nmodels that you’ve built in TensorFlow and in PyTorch \\nand name this experiment something like  \\npytorch_tensorflow . In the context of anomaly \\ndetection, you can create an experiment called  \\nmodel_prototyping  and group all of the models that',\n",
       " 'and name this experiment something like  \\npytorch_tensorflow . In the context of anomaly \\ndetection, you can create an experiment called  \\nmodel_prototyping  and group all of the models that \\nyou want to test by running the training pipelines after \\nsetting model_prototyping  as the experiment name.\\nAs you’ll see shortly, grouping model training \\nsessions by experiment can really help organize \\nyour workspace because you’ll get a clear idea of the \\ncontext behind trained models.\\n• Model and metric logging:  MLFlow allows you to \\nsave a model in a modularized form and log all of the \\nmetrics related to the model run . A model run can be \\nthought of as the model training, testing, and validation Chapter 4  Introdu CtIon to\\xa0MLF Low127pipeline from the previous chapter. In MLFlow, you',\n",
       " 'thought of as the model training, testing, and validation Chapter 4  Introdu CtIon to\\xa0MLF Low127pipeline from the previous chapter. In MLFlow, you \\ncan mark the start and the end of each run and decide \\nwhich metrics you want to save. Additionally, you can \\nsave graphs, so you can also view plots like confusion \\nmatrices and ROC curves. A model run  is basically the \\ninstance in which MLFlow executes the code that you \\ntell it to, so if you want, you can only train the model \\nand leave it at that.\\nIt is possible for you to train, evaluate, and even \\nvalidate your model, logging all of the metrics for \\neach respective step in the whole process. MLFlow \\ngives you a lot of flexibility in how you define \\na model run. You can end the run after simply',\n",
       " 'validate your model, logging all of the metrics for \\neach respective step in the whole process. MLFlow \\ngives you a lot of flexibility in how you define \\na model run. You can end the run after simply \\ntraining it, or you can end the run after training \\nand evaluating it. If you wish, you can even set up \\nan entire validation script to log the entire process \\nfor you, allowing you to much more easily compare \\ndifferent hyperparameter setups all at once in \\nMLFlow. We will explore how to perform model \\nvalidation with MLFlow shortly when we revisit the \\nscikit-learn experiment from Chapter 2.\\n• Comp aring model metrics:  MLFlow also allows you \\nto compare different models and their metrics all at \\nonce. And so, when performing validation to help',\n",
       " 'scikit-learn experiment from Chapter 2.\\n• Comp aring model metrics:  MLFlow also allows you \\nto compare different models and their metrics all at \\nonce. And so, when performing validation to help \\ntune a model’s hyperparameters, you can compare all \\nof the selected metrics together in MLFlow using its \\nuser interface. In the previous chapter, you printed out \\neverything, making the cell output possibly very large \\nif the script is quite involved in its hyperparameter \\nsetups.Chapter 4  Introdu CtIon to\\xa0MLF Low128• Model Registry:  MLFlow also adds functionality to \\nallow you to implement a model registry, allowing you \\nto define what stage a particular model is in. Databricks \\nintegrates quite well with MLFlow, providing built-  \\nin model registry functionality. You will explore how',\n",
       " 'to define what stage a particular model is in. Databricks \\nintegrates quite well with MLFlow, providing built-  \\nin model registry functionality. You will explore how \\nto use the MLFlow Model Registry when you look at \\nDatabricks in Appendix.\\n• Local deployment:  MLFlow also allows you to \\ndeploy on a local server, allowing you to test model \\ninference . Model inference is basically the prediction \\nprocess of a model. Data is sent to the model in one of \\nseveral standardized formats, and MLFlow returns the \\npredictions made by the model.\\nSuch a setup can easily be converted to work on \\na hosted server as well. As you will see in the next \\nseveral chapters, MLFlow also allows you to deploy \\nyour models on popular cloud services such as \\nAmazon SageMaker, Microsoft Azure, Google Cloud,',\n",
       " 'several chapters, MLFlow also allows you to deploy \\nyour models on popular cloud services such as \\nAmazon SageMaker, Microsoft Azure, Google Cloud, \\nand Databricks. The process at its core remains \\nsimilar to how you will perform local model serving. \\nThe only difference comes with where you host the \\nmodel and the particular procedure for querying it.\\nWith that being said, let’s get started by revisiting the scikit-learn \\nlogistic regression model and integrating MLFlow into it.Chapter 4  Introdu CtIon to\\xa0MLF Low129 MLFlow with\\xa0Scikit-Learn\\nBefore we begin, here are the versions of Python and the packages that \\nwere used:\\n• Python : 3.6.5\\n• numpy : 1.18.5\\n• scikit-learn : 0.22.2.post1\\n• pandas : 1.1.0\\n• Matplotlib : 3.2.1\\n• Seaborn : 0.10.1\\n• MLFlow : 1.10.0',\n",
       " 'were used:\\n• Python : 3.6.5\\n• numpy : 1.18.5\\n• scikit-learn : 0.22.2.post1\\n• pandas : 1.1.0\\n• Matplotlib : 3.2.1\\n• Seaborn : 0.10.1\\n• MLFlow : 1.10.0\\nYou don’t need the exact versions of the packages we used, but in case \\nsome functionality is removed, renamed, or just changed in the newer \\nversions and the code runs into an error, you have the exact version of the \\nmodule you can try running the code with.\\nMLFlow in particular is updated quite frequently, so you are more \\nlikely to run into issues running code with something like MLFlow \\ncompared to a package like numpy.\\nWith that being said, let’s dive into the first example. In this case, let’s \\nrevisit the scikit-learn code from the previous chapter and add MLFlow \\nintegration to it.\\n Data Processing',\n",
       " 'With that being said, let’s dive into the first example. In this case, let’s \\nrevisit the scikit-learn code from the previous chapter and add MLFlow \\nintegration to it.\\n Data Processing\\nFirst, you begin with all of the imports:\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib #Chapter 4  Introdu CtIon to\\xa0MLF Low130import matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport sklearn #\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve, \\nconfusion_matrix\\nfrom sklearn.model_selection import KFold\\nimport mlflow\\nimport mlflow.sklearn\\nprint(\"Numpy: {}\".format(np.__version__))\\nprint(\"Pandas: {}\".format(pd.__version__))',\n",
       " 'confusion_matrix\\nfrom sklearn.model_selection import KFold\\nimport mlflow\\nimport mlflow.sklearn\\nprint(\"Numpy: {}\".format(np.__version__))\\nprint(\"Pandas: {}\".format(pd.__version__))\\nprint(\"matplotlib: {}\".format(matplotlib.__version__))\\nprint(\"seaborn: {}\".format(sns.__version__))\\nprint(\"Scikit-Learn: {}\".format(sklearn.__version__))\\nprint(\"MLFlow: {}\".format(mlflow.__version__))\\nThe output should look something like Figure\\xa0 4-1.\\nFigure 4-1.  The output of importing the necessary modules and \\nprinting out their versionsChapter 4  Introdu CtIon to\\xa0MLF Low131Now you can move on to loading the data:\\ndata_path = \"data/creditcard.csv\"\\ndf = pd.read_csv(data_path)\\ndf = df.drop(\"Time\", axis=1)\\nRefer to Figure\\xa0 4-2 to see the code in a cell.\\nNote that you are once again dropping the column Time.',\n",
       " 'data_path = \"data/creditcard.csv\"\\ndf = pd.read_csv(data_path)\\ndf = df.drop(\"Time\", axis=1)\\nRefer to Figure\\xa0 4-2 to see the code in a cell.\\nNote that you are once again dropping the column Time.\\nYou can now check to see if the data loaded in correctly:\\ndf.head()\\nRefer to Figure\\xa0 4-3 to see the head() function.\\nFigure 4-2.  Loading the data set and dropping the column named \\nTime because it adds very large data values that ultimately don’t have \\nmuch of a correlation with the column Class. Model performance is \\nboosted slightly simply by dropping the extraneous information\\nFigure 4-3.  Verifying that the data was loaded correctly by using \\nthe head() function. As you can see, the columns and the data have',\n",
       " 'boosted slightly simply by dropping the extraneous information\\nFigure 4-3.  Verifying that the data was loaded correctly by using \\nthe head() function. As you can see, the columns and the data have \\nloaded in correctlyChapter 4  Introdu CtIon to\\xa0MLF Low132Again, you are dropping the column Time  from the data frame this \\ntime. This is because this column was found to add data that isn’t very \\nhelpful in finding an anomaly and only adds extra complexity to the data.\\nIn the case of deep learning models, your model might eventually learn \\nthat the Time  data does not correlate very well with the Class  labels and \\nmay place less importance on nodes processing that data. Eventually, it \\nmight even ignore the Time  data. However, you can speed up the learning',\n",
       " 'may place less importance on nodes processing that data. Eventually, it \\nmight even ignore the Time  data. However, you can speed up the learning \\nprocess by cutting out these types of features from your training sets. This \\nis because you’re sparing the models the time and resources needed to \\nfigure that out.\\nMoving on, you will split the normal points and the anomalies:\\nnormal = df[df.Class == 0].sample(frac=0.5,  \\nrandom_state=2020).reset_index(drop= True)\\nanomaly = df[df.Class == 1]\\nLet’s print out their respective shapes:\\nprint(f\"Normal: {normal.shape}\")\\nprint(f\"Anomaly: {anomaly.shape}\")\\nRefer to Figure\\xa0 4-4 to see the above two cells in Jupyter along with their \\noutputs.\\nFigure 4-4.  Randomly sampling 50% of all the normal data points',\n",
       " 'print(f\"Anomaly: {anomaly.shape}\")\\nRefer to Figure\\xa0 4-4 to see the above two cells in Jupyter along with their \\noutputs.\\nFigure 4-4.  Randomly sampling 50% of all the normal data points \\nin the data frame and picking out all of the anomalies from the data \\nframe as separate data frames. Then, you print the shapes of both \\ndata sets. As you can see, the normal points massively outnumber the \\nanomaly pointsChapter 4  Introdu CtIon to\\xa0MLF Low133You are going to split the normal and anomaly sets into train-test-  \\nvalidate subsets. Run the following two code blocks:\\nnormal_train, normal_test = train_test_split(normal,  \\ntest_size = 0.2, random_state = 2020)\\nanomaly_train, anomaly_test = train_test_split \\n(anomaly, test_size = 0.2, random_state = 2020)',\n",
       " 'normal_train, normal_test = train_test_split(normal,  \\ntest_size = 0.2, random_state = 2020)\\nanomaly_train, anomaly_test = train_test_split \\n(anomaly, test_size = 0.2, random_state = 2020)\\nnormal_train, normal_validate = train_test_split(normal_train, \\ntest_size = 0.25, random_state = 2020)\\nanomaly_train, anomaly_validate = train_test_split \\n(anomaly_train, test_size = 0.25, random_state = 2020)\\nRefer to Figure\\xa0 4-5 to see both code blocks in their respective cells.\\nNow, you can process these sets and create the x-y splits:\\nx_train = pd.concat((normal_train, anomaly_train))\\nx_test = pd.concat((normal_test, anomaly_test))\\nx_validate = pd.concat((normal_validate, anomaly_validate))\\ny_train = np.array(x_train[\"Class\"])\\ny_test = np.array(x_test[\"Class\"])',\n",
       " 'x_test = pd.concat((normal_test, anomaly_test))\\nx_validate = pd.concat((normal_validate, anomaly_validate))\\ny_train = np.array(x_train[\"Class\"])\\ny_test = np.array(x_test[\"Class\"])\\ny_validate = np.array(x_validate[\"Class\"])\\nFigure 4-5.  Partitioning the normal and anomaly data frames \\nseparately into train, test, and validation splits. Initially, 20% of \\nthe normal and anomaly points are used as the test split. From \\nthe remaining 80% of data, 25% of that train split is used as the \\nvalidation split, meaning the validation split is 20% of the original \\ndata. This leaves the final training split at 60% of the original data. In \\nthe end, the train-test-validate split has a 60-20-20 ratio, respectivelyChapter 4  Introdu CtIon to\\xa0MLF Low134x_train = x_train.drop(\"Class\", axis=1)',\n",
       " 'the end, the train-test-validate split has a 60-20-20 ratio, respectivelyChapter 4  Introdu CtIon to\\xa0MLF Low134x_train = x_train.drop(\"Class\", axis=1)\\nx_test = x_test.drop(\"Class\", axis=1)\\nx_validate = x_validate.drop(\"Class\", axis=1)\\nRefer to Figure\\xa0 4-6 to see the above code block in a cell.\\nYou can print out the shapes of these sets:\\nprint(\"Training sets:\\\\nx_train: {} \\\\ny_train:  \\n{}\".format(x_train.shape, y_train.shape))\\nprint(\"\\\\nTesting sets:\\\\nx_test: {} \\\\ny_test:  \\n{}\".format(x_test.shape, y_test.shape))\\nprint(\"\\\\nValidation sets:\\\\nx_validate: {} \\\\ny_validate: {}\".\\nformat(x_validate.shape, y_validate.shape))\\nRefer to Figure\\xa0 4-7 to see the output shapes.\\nFigure 4-6.  Creating the respective x and y splits of the training,',\n",
       " 'format(x_validate.shape, y_validate.shape))\\nRefer to Figure\\xa0 4-7 to see the output shapes.\\nFigure 4-6.  Creating the respective x and y splits of the training, \\ntesting, and validation sets by concatenating the respective normal \\nand anomaly sets. You drop Class from the x-sets because it would be \\ncheating otherwise to give it the label directly. You are trying to get the \\nmodel to learn the labels by reading the x-data, not learn how to read \\nthe Class column in the x-dataChapter 4  Introdu CtIon to\\xa0MLF Low135Finally, you scale your data using scikit-learn’s standard scaler:\\nscaler = StandardScaler()\\nscaler.fit(pd.concat((normal, anomaly)).drop(\"Class\", axis=1))\\nx_train = scaler.transform(x_train)\\nx_test = scaler.transform(x_test)\\nx_validate = scaler.transform(x_validate)',\n",
       " 'scaler = StandardScaler()\\nscaler.fit(pd.concat((normal, anomaly)).drop(\"Class\", axis=1))\\nx_train = scaler.transform(x_train)\\nx_test = scaler.transform(x_test)\\nx_validate = scaler.transform(x_validate)\\nRefer to Figure\\xa0 4-8.\\nFigure 4-7.  Printing out the shapes of the training, testing, and \\nvalidation sets\\nFigure 4-8.  Fitting the scaler on the superset of normal and anomaly \\npoints after dropping Class to scale the x-setsChapter 4  Introdu CtIon to\\xa0MLF Low136 Training and\\xa0Evaluating with\\xa0MLFlow\\nAll that is left now is to train and evaluate your model. We will showcase \\nvalidation with MLFlow functionality in a bit, but first let’s define the \\ntrain and test functions to organize the code. This is also where you start \\nintegrating MLFlow into your code. Here is the train  function:',\n",
       " 'train and test functions to organize the code. This is also where you start \\nintegrating MLFlow into your code. Here is the train  function:\\ndef train(sk_model, x_train, y_train):\\n    sk_model = sk_model.fit(x_train, y_train)\\n    train_acc = sk_model.score(x_train, y_train)\\n    mlflow.log_metric(\"train_acc\", train_acc)\\n    print(f\"Train Accuracy: {train_acc:.3%}\")\\nRefer to Figure\\xa0 4-9 to see this code in a cell.\\nYou may have noticed the first of the new code with this line:\\nmlflow.log_metric(\"train_acc\", train_acc)\\nYou create a new metric here specifically for the training accuracy so \\nthat you can keep track of this metric. Furthermore, you are telling MLFlow \\nto log this metric, so that MLFlow will keep track of this value in each run.',\n",
       " 'that you can keep track of this metric. Furthermore, you are telling MLFlow \\nto log this metric, so that MLFlow will keep track of this value in each run. \\nWhen you log multiple runs, you can compare this metric across each \\nof those runs so that you can pick a model with the best AUC score for \\nexample.\\nFigure 4-9.  Defining the train function to better organize the code. \\nAdditionally, you are defining a training accuracy metric that will be \\nlogged by MLFlowChapter 4  Introdu CtIon to\\xa0MLF Low137Let’s now move on to the evaluate  function:\\ndef evaluate(sk_model, x_test, y_test):\\n    eval_acc = sk_model.score(x_test, y_test)\\n    preds = sk_model.predict(x_test)\\n    auc_score = roc_auc_score(y_test, preds)\\n    mlflow.log_metric(\"eval_acc\", eval_acc)',\n",
       " 'eval_acc = sk_model.score(x_test, y_test)\\n    preds = sk_model.predict(x_test)\\n    auc_score = roc_auc_score(y_test, preds)\\n    mlflow.log_metric(\"eval_acc\", eval_acc)\\n    mlflow.log_metric(\"auc_score\", auc_score)\\n    print(f\"Auc Score: {auc_score:.3%}\")\\n    print(f\"Eval Accuracy: {eval_acc:.3%}\")\\n     roc_plot = plot_roc_curve(sk_model, x_test, y_test, \\nname=\\'Scikit-learn ROC Curve\\')\\n    plt.savefig(\"sklearn_roc_plot.png\")\\n    plt.show()\\n    plt.clf()\\n    conf_matrix = confusion_matrix(y_test, preds)\\n    ax = sns.heatmap(conf_matrix, annot= True,fmt=\\'g\\')\\n    ax.invert_xaxis()\\n    ax.invert_yaxis()\\n    plt.ylabel(\\'Actual\\')\\n    plt.xlabel(\\'Predicted\\')\\n    plt.title(\"Confusion Matrix\")\\n    plt.savefig(\"sklearn_conf_matrix.png\")\\n    mlflow.log_artifact(\"sklearn_roc_plot.png\")',\n",
       " 'ax.invert_yaxis()\\n    plt.ylabel(\\'Actual\\')\\n    plt.xlabel(\\'Predicted\\')\\n    plt.title(\"Confusion Matrix\")\\n    plt.savefig(\"sklearn_conf_matrix.png\")\\n    mlflow.log_artifact(\"sklearn_roc_plot.png\")\\n    mlflow.log_artifact(\"sklearn_conf_matrix.png\")\\nRefer to Figure\\xa0 4-10  to see the above code in a cell.Chapter 4  Introdu CtIon to\\xa0MLF Low138Once again, you have told MLFlow to log two more metrics: the AUC \\nscore and the accuracy on the test set. You do so with these lines of code:\\nmlflow.log_metric(\"eval_acc\", eval_acc)\\nmlflow.log_metric(\"auc_score\", auc_score)\\nFurthermore, you can also tell MLFlow to save the plots generated by \\nmatplotlib and by seaborn. With this, you can look at each of the graphs for \\neach training run and do so in a highly organized manner. You must first',\n",
       " 'matplotlib and by seaborn. With this, you can look at each of the graphs for \\neach training run and do so in a highly organized manner. You must first \\nsave these plots, which you do in the same directory. Then, you must tell \\nMLFlow to grab the artifacts to log them like so:\\nmlflow.log_artifact(\"sklearn_roc_plot.png\")\\nmlflow.log_artifact(\"sklearn_conf_matrix.png\")\\nMake sure that they have the same names as the graphs you saved.\\nFigure 4-10.  A function to calculate the evaluation metrics for the \\nAUC score and accuracy. Plots for the confusion matrix and the ROC \\ncurve are generated, and both the metrics and the graphs are logged \\nto MLFlowChapter 4  Introdu CtIon to\\xa0MLF Low139 Logging and\\xa0Viewing MLFlow Runs\\nFinally, let’s run the code that actually sets the experiment name, starts the',\n",
       " 'to MLFlowChapter 4  Introdu CtIon to\\xa0MLF Low139 Logging and\\xa0Viewing MLFlow Runs\\nFinally, let’s run the code that actually sets the experiment name, starts the \\nMLFlow run, and executes all this code:\\nsk_model = LogisticRegression(random_state= None,  \\nmax_iter=400, solver=\\'newton-cg\\')\\nmlflow.set_experiment(\"scikit_learn_experiment\")\\nwith mlflow.start_run():\\n    train(sk_model, x_train, y_train)\\n    evaluate(sk_model, x_test, y_test)\\n    mlflow.sklearn.log_model(sk_model, \"log_reg_model\")\\n     print(\"Model run: \", mlflow.active_run().info.run_uuid)\\nmlflow.end_run()\\nNotice the new lines of MLFlow code. We will go through them one by \\none.\\nFirst, let’s begin with what appears to set the experiment name:\\nmlflow.set_experiment(\"scikit_learn_experiment\")',\n",
       " 'Notice the new lines of MLFlow code. We will go through them one by \\none.\\nFirst, let’s begin with what appears to set the experiment name:\\nmlflow.set_experiment(\"scikit_learn_experiment\")\\nWhat this does is that it puts the run under whatever experiment name \\nyou pass in as a parameter. If that name does not exist, MLFlow will create \\na new one under that name and put the run there.\\nwith mlflow.start_run():\\n     ...\\n     ...\\nThis line of code allows you to chunk all of your code under the context \\nof one MLFlow run. This ensures that there are no discrepancies between \\nwhere your metrics are being logged, and that it doesn’t create two \\ndifferent runs when you mean it to log everything for the same run.',\n",
       " 'where your metrics are being logged, and that it doesn’t create two \\ndifferent runs when you mean it to log everything for the same run.\\nmlflow.sklearn.log_model(sk_model, \"log_reg_model\")Chapter 4  Introdu CtIon to\\xa0MLF Low140This line of code is the general convention to use when you’re logging a \\nmodel. The parameters, in order, are the model you’re saving and then the \\nname you’re setting for the model when saving. In this case, you are saving \\nyour logistic regression model with the name log_reg_model  in this run.\\nAs you will see later, most other frameworks follow the same style \\nwhen saving the model. There are a couple exceptions, but we will cover \\nthis when the time comes. In this case, you are calling mlflow.sklearn ,',\n",
       " 'when saving the model. There are a couple exceptions, but we will cover \\nthis when the time comes. In this case, you are calling mlflow.sklearn , \\nbut if you wanted to log a PySpark model, you would do mlflow.spark .\\nBasically, the framework the model was built in must match the \\nframework module of MLFlow when logging the model. It is possible \\nto create a custom “model” in MLFlow and log this as well, something \\nthat is covered in the documentation. You can use this custom model to \\nthen specify how you want the prediction function to work. If you’d like \\nto process the data some more before making predictions, for example, \\nMLFlow allows you to specify this extra functionality through the use of the \\nMLFlow PyFunc module. Refer to the documentation, which you can find',\n",
       " 'MLFlow allows you to specify this extra functionality through the use of the \\nMLFlow PyFunc module. Refer to the documentation, which you can find \\nhere: www.mlflow.org/docs/latest/models.html#model-customization .\\nprint(\"Model run: \", mlflow.active_run().info.run_uuid)\\nThis line of code essentially gets the current run that the model and \\nmetrics are being logged to and prints it out. This makes it handy if you \\nwant to retrieve the run directly from the notebook itself instead of going to \\nthe UI to do so.\\nmlflow.end_run()\\nFinally, this tells MLFlow to end the current run. In cases where there \\nis an error in the MLFlow start run code block, and the run does not \\nterminate, do this to forcibly end the current run. Basically, it is there to',\n",
       " 'is an error in the MLFlow start run code block, and the run does not \\nterminate, do this to forcibly end the current run. Basically, it is there to \\nensure that MLFlow stops the run after you executed all the code relevant \\nto the current run.\\nMoving on, refer to Figure\\xa0 4-11  to see the full output of the code.Chapter 4  Introdu CtIon to\\xa0MLF Low141You can see that MLFlow automatically generates a new experiment \\nif it does not already exist, so you can create a new experiment directly \\nfrom the code. You can also see that the rest of the code basically outputs \\nas usual, except it also prints the run ID of the current MLFlow run just as \\nyou specified. You will use this later when you select the specific model \\nthat you want to serve. What you will do next is open up the UI MLFlow',\n",
       " 'you specified. You will use this later when you select the specific model \\nthat you want to serve. What you will do next is open up the UI MLFlow \\nprovides where you can actually look at all the experiments and model \\nruns. Finally, you also log the model itself as an artifact with MLFlow. \\nMLFlow will modularize this code so that it will work with the code \\nprovided by MLFlow to support implementations of a variety of MLOps \\nprinciples.\\nFigure 4-11.  The output of running the MLFlow experiment. Under \\nan MLFlow run context, you are training the model, outputting the \\ngraphs from the evaluation function, and logging all the metrics \\nincluding the model to this runChapter 4  Introdu CtIon to\\xa0MLF Low142The following was done on Windows 10 , but it should be the same',\n",
       " 'graphs from the evaluation function, and logging all the metrics \\nincluding the model to this runChapter 4  Introdu CtIon to\\xa0MLF Low142The following was done on Windows 10 , but it should be the same \\non MacOS or Linux. First, open command prompt/powershell/terminal. \\nThen, you must go into the directory that contains this notebook file. List \\nthe contents of the directory (or view this in file explorer/within Jupyter \\nitself) and you will notice a new directory named mlruns .\\nIf you installed all of your packages in Conda, make sure you’ve \\nactivated the Conda environment before running this.\\nWhat you want to do now is to make sure your command prompt, \\npowershell, or terminal is in the same directory that contains mlruns , and \\ntype the following:\\nmlflow ui -p 1234',\n",
       " 'What you want to do now is to make sure your command prompt, \\npowershell, or terminal is in the same directory that contains mlruns , and \\ntype the following:\\nmlflow ui -p 1234\\nThe command mlflow ui  hosts the MLFlow UI locally on the default \\nport of 5000. However, the options -p 1234  tell it that you want to host it \\nspecifically on the port 1234.\\nIf it all goes well, and it can take several seconds, you should see \\nsomething like Figure\\xa0 4-12 .\\nFigure 4-12.  Making sure that the current directory contains the \\nfolder mlruns and calling the command to start the UI.\\xa0If successful, \\nit should state “Serving on http:// … :1234. ” We have docker on our \\nsystem, hence why yours might say localhost instead of kubernetes.',\n",
       " 'it should state “Serving on http:// … :1234. ” We have docker on our \\nsystem, hence why yours might say localhost instead of kubernetes.\\ndocker.internalChapter 4  Introdu CtIon to\\xa0MLF Low143Now, open a browser and type in http://localhost:1234  or \\nhttp://127.0.0.1:1234 . Both should take you to the same MLFlow UI.\\xa0If \\nyou used a different port, it should generally look like this:\\nhttp://localhost:PORT_NUMBER  or http://127.0.0.1:PORT_NUMBER , \\nwhere you replace PORT_NUMBER  with the one you used. If you did not \\nspecify a port parameter, then the default port used by MLFlow is 5000.\\nRegardless, if it works correctly, you should see something like \\nFigure\\xa0 4-13  once you visit that URL.\\nNotice that there is now an experiment titled scikit_learn_experiment .',\n",
       " 'Regardless, if it works correctly, you should see something like \\nFigure\\xa0 4-13  once you visit that URL.\\nNotice that there is now an experiment titled scikit_learn_experiment . \\nClick it, and you should see something like Figure\\xa0 4-14 .\\nFigure 4-13.  Your MLFlow UI should look something like this. To \\nthe left are the experiments. Notice that there is an experiment titled \\nDefault and one titled scitkit_learn_experiment, which is the one you \\njust createdChapter 4  Introdu CtIon to\\xa0MLF Low144You should see something like Figure\\xa0 4-15 .\\nFigure 4-14.  This is what your experiment, scikit_learn_experiment, \\nshould look like once you click it. Notice that there is one run here, \\nwhich is what was just createdYou can see the run that just completed, along with the metrics you',\n",
       " 'should look like once you click it. Notice that there is one run here, \\nwhich is what was just createdYou can see the run that just completed, along with the metrics you \\nlogged. Click it so that you can explore it. The run that was just completed \\nshould have a green check mark beside the time stamp when it finished if \\neverything went well, which you can see is the case in Figure\\xa0 4-14 .Chapter 4  Introdu CtIon to\\xa0MLF Low145\\nFigure 4-16.  The logged artifacts of this run. Notice that the graphs \\nappear to be logged as well as the model itself, which was named \\nlog_reg_model when you were logging it in the code\\nFigure 4-15.  This is the run that was just completed. Notice that the \\nmetrics you logged show up here\\nYou should now see the details of this run much more clearly. Here,',\n",
       " 'Figure 4-15.  This is the run that was just completed. Notice that the \\nmetrics you logged show up here\\nYou should now see the details of this run much more clearly. Here, \\nyou can see all of the parameters and metrics that were logged. Keep \\nscrolling down and you should be able to see all of the logged artifacts. \\nRefer to Figure\\xa0 4-16 .Chapter 4  Introdu CtIon to\\xa0MLF Low146Here, you can see the model that has been logged, along with the two \\ngraphs that you logged as artifacts. Click the graphs and you should see \\nsomething like Figure\\xa0 4-17 .\\nAmazing, right? Everything is extremely organized, and you don’t \\nhave to worry about creating multiple folders for everything and staying \\norganized. Simply tell MLFlow what to do and it will log all the information',\n",
       " 'have to worry about creating multiple folders for everything and staying \\norganized. Simply tell MLFlow what to do and it will log all the information \\nrelevant to this run that you need. You can log your deep learning model’s \\nhyperparameters for learning rate, number of epochs, specific optimizer \\nparameters like beta1 and beta2 for the Adam optimizer, and so on.\\nYou can even log graphs, as you can see in Figure\\xa0 4-17 , along with the \\nmodels themselves. With MLFlow, you can stay highly organized with \\nyour experiments even if you don’t necessarily need the deployment \\ncapabilities to the cloud services.\\nLet’s now try logging a few more runs. Rerun the cell in Figure\\xa0 4-11  \\na couple times to completion and go back to the MLFlow UI.\\xa0Make sure',\n",
       " 'capabilities to the cloud services.\\nLet’s now try logging a few more runs. Rerun the cell in Figure\\xa0 4-11  \\na couple times to completion and go back to the MLFlow UI.\\xa0Make sure \\nyou have selected the experiment named scikit_learn_experiment . You \\nshould see something like Figure\\xa0 4-18 .\\nFigure 4-17.  Inspecting the graph of the confusion matrix that you \\nsaved. Feel free to click the other graph as well, which is of the ROC \\nplotChapter 4  Introdu CtIon to\\xa0MLF Low147Let’s compare the metrics you’ve logged for these runs. Select at least \\ntwo runs, and ensure your UI looks somewhat like Figure\\xa0 4-19 . We selected \\nthree runs.\\nFigure 4-19.  This is what your UI should look like after selecting \\nseveral runs. Make sure to select at least two so that there is something',\n",
       " 'three runs.\\nFigure 4-19.  This is what your UI should look like after selecting \\nseveral runs. Make sure to select at least two so that there is something \\nto compare. Also notice that the button named Compare turns solid\\nFigure 4-18.  Revisiting your experiment after logging some runs in. \\nThe runs are logged in ascending order by timestamp, so the latest \\nruns are on topChapter 4  Introdu CtIon to\\xa0MLF Low148After clicking Compare, you should see something like Figure\\xa0 4-20 .\\nHere, you can directly compare the relevant parameters and metrics \\nbetween the runs you have chosen. You have the option of viewing a scatter \\nplot, a contour plot, or a parallel coordinates plot. Feel free to play around \\nwith the metrics and with the plots. You can even save these plots if you wish.',\n",
       " 'plot, a contour plot, or a parallel coordinates plot. Feel free to play around \\nwith the metrics and with the plots. You can even save these plots if you wish.\\nNote that since these runs have the exact same metrics, there will only \\nappear to be one point plotted.\\n Loading a\\xa0Logged Model\\nNext, let’s briefly look at how you can load the models logged by MLFlow. \\nGo back to the experiment and click a run. Note the run ID at the top and \\ncopy it. Then, go back to the notebook, and run the following. Note that \\nthere is a placeholder for the run ID:\\nloaded_model =  mlflow.sklearn.load_model  \\n(\"runs:/YOUR_RUNID_HERE/log_reg_model\")\\nFigure 4-20.  The UI after selecting three runs to compare. As you can \\nsee, you can look at all of the metrics at once. There is also a graphing',\n",
       " '(\"runs:/YOUR_RUNID_HERE/log_reg_model\")\\nFigure 4-20.  The UI after selecting three runs to compare. As you can \\nsee, you can look at all of the metrics at once. There is also a graphing \\ntool that lets you compare these values graphically, though you won’t \\nsee proper graphs as every value is the same across the runsChapter 4  Introdu CtIon to\\xa0MLF Low149To better understand what this path is, let’s split it up into three \\nsections: the format ( runs:/) , the run ID ( YOUR_RUNID_HERE ), and the \\nmodel name that you used when you logged it ( log_reg_model) .\\nIn our case, our run ID was 3862eb3bd89b43e8ace610c521d974e6, \\nso our cell looks like Figure\\xa0 4-21 . Ensure your code looks somewhat like \\nFigure\\xa0 4-21 , with the only difference being the run ID that you chose since',\n",
       " 'so our cell looks like Figure\\xa0 4-21 . Ensure your code looks somewhat like \\nFigure\\xa0 4-21 , with the only difference being the run ID that you chose since \\nit will be different from ours.\\nThis is now the same model that you had when MLFlow logged it in the \\nfirst place. With this, you can call something like .score()  and see that it’s \\nthe same as during training:\\nloaded_model.score(x_test, y_test)\\nThis outputs the accuracy as the model is evaluated on the test set. If \\nthis truly is the same model, then the accuracy should match what was \\noutput earlier during the evaluation portion of the model run.\\nRefer to Figure\\xa0 4-22  to see the output.\\nAs you can see, this value matches the evaluation accuracy from \\nFigure\\xa0 4-11 .\\nFigure 4-22.  This is the evaluation accuracy of the loaded model',\n",
       " 'Refer to Figure\\xa0 4-22  to see the output.\\nAs you can see, this value matches the evaluation accuracy from \\nFigure\\xa0 4-11 .\\nFigure 4-22.  This is the evaluation accuracy of the loaded model \\nafter evaluation on the test sets. If you compare this with Figure\\xa0 4- 11,  \\nyou can see that the numbers more or less match, disregarding \\nrounding\\nFigure 4-21.  The code to load a model that we logged using the \\nspecific run ID we logged it in along with the model’s name we used \\nwhen we logged itChapter 4  Introdu CtIon to\\xa0MLF Low150Now you know how to load a model from a specific MLFlow run.\\nWith that, you’ve seen some of the functionality that MLFlow provides \\nand how it can help in keeping your prototyping experiments much',\n",
       " 'With that, you’ve seen some of the functionality that MLFlow provides \\nand how it can help in keeping your prototyping experiments much \\nmore organized. As you will see shortly, this entire pipeline that you just \\nexplored is pretty much all you need to recreate the train, test, validate \\npipeline that you saw earlier. Before you move on to looking at how you \\ncan use MLFlow with other frameworks, let’s go over how you can use \\nMLFlow functionality to vastly improve the model validation process.\\n Model Validation (Parameter Tuning) \\nwith\\xa0MLFlow\\n Parameter Tuning\\xa0– Broad Search\\nJust like in Chapter 2, you will use a script to help with model validation \\nwith respect to hyperparameter tuning. The tuning script will largely',\n",
       " 'with\\xa0MLFlow\\n Parameter Tuning\\xa0– Broad Search\\nJust like in Chapter 2, you will use a script to help with model validation \\nwith respect to hyperparameter tuning. The tuning script will largely \\nremain the same, except for a few modifications where MLFlow code has \\nbeen added in.\\nRun the following code to set the range of anomaly weights and to set \\nthe number of folds:\\nanomaly_weights = [1, 5, 10, 15]\\nnum_folds = 5\\nkfold = KFold(n_splits=num_folds, shuffle= True,  \\nrandom_state=2020)\\nThe code should look like Figure\\xa0 4-23 .\\nFigure 4-23.  The code to determine the list of anomaly weights to \\nperform validation over, to determine the number of folds, and to',\n",
       " 'random_state=2020)\\nThe code should look like Figure\\xa0 4-23 .\\nFigure 4-23.  The code to determine the list of anomaly weights to \\nperform validation over, to determine the number of folds, and to \\ninitialize the KFolds generator based on the number of foldsChapter 4  Introdu CtIon to\\xa0MLF Low151Now, paste the following. This is the first half of the entire function:\\nmlflow.set_experiment(\"sklearn_creditcard_broad_search\")\\nlogs = []\\nfor f in range(len(anomaly_weights)):\\n    fold = 1\\n    accuracies = []\\n    auc_scores= []\\n    for train, test in kfold.split(x_validate, y_validate):\\n        with mlflow.start_run():\\n            weight = anomaly_weights[f]\\n            mlflow.log_param(\"anomaly_weight\", weight)\\n            class_weights= {\\n                0: 1,\\n                1: weight',\n",
       " 'weight = anomaly_weights[f]\\n            mlflow.log_param(\"anomaly_weight\", weight)\\n            class_weights= {\\n                0: 1,\\n                1: weight\\n            }\\n            sk_model = LogisticRegression(random_state= None,\\n                                    max_iter=400,\\n                                    solver=\\'newton-cg\\',\\n                                     class_weight=class_\\nweights).fit \\n(x_validate[train],  \\ny_validate[train])\\n            for h in range(40): print(\\'-\\', end=\"\")\\n            print(f\"\\\\nfold {fold}\\\\nAnomaly Weight: {weight}\")\\n             train_acc = sk_model.score(x_validate[train],  \\ny_validate[train])\\n            mlflow.log_metric(\"train_acc\", train_acc)\\n             eval_acc = sk_model.score(x_validate[test],  \\ny_validate[test])',\n",
       " 'y_validate[train])\\n            mlflow.log_metric(\"train_acc\", train_acc)\\n             eval_acc = sk_model.score(x_validate[test],  \\ny_validate[test])\\n            preds = sk_model.predict(x_validate[test])\\n            mlflow.log_metric(\"eval_acc\", eval_acc)Chapter 4  Introdu CtIon to\\xa0MLF Low152Here is some more of the code. Make sure this all aligns with the code \\nfrom above.\\n            try:\\n                auc_score = roc_auc_score(y_validate[test], preds)\\n            except:\\n                auc_score = -1\\n            mlflow.log_metric(\"auc_score\", auc_score)\\n             print(\"AUC: {}\\\\neval_acc: {}\".format(auc_score, \\neval_acc))\\n            accuracies.append(eval_acc)\\n            auc_scores.append(auc_score)\\n             log = [sk_model, x_validate[test],  \\ny_validate[test], preds]',\n",
       " 'eval_acc))\\n            accuracies.append(eval_acc)\\n            auc_scores.append(auc_score)\\n             log = [sk_model, x_validate[test],  \\ny_validate[test], preds]\\n            logs.append(log)\\n             mlflow.sklearn.log_model(sk_model,   \\nf\"anom_weight_{weight}_fold_{fold}\")\\n            fold = fold + 1\\n            mlflow.end_run()\\n    print(\"\\\\nAverages: \")\\n    print(\"Accuracy: \", np.mean(accuracies))\\n    print(\"AUC: \", np.mean(auc_scores))\\n    print(\"Best: \")\\n    print(\"Accuracy: \", np.max(accuracies))\\n    print(\"AUC: \", np.max(auc_scores))\\nFirst, let’s look at what that giant chunk of code looks like in a cell. \\nEnsure your code and alignment matches Figure\\xa0 4-24 .Chapter 4  Introdu CtIon to\\xa0MLF Low153Now, let’s run this script. It should log the parameter for the anomaly',\n",
       " 'Ensure your code and alignment matches Figure\\xa0 4-24 .Chapter 4  Introdu CtIon to\\xa0MLF Low153Now, let’s run this script. It should log the parameter for the anomaly \\nweight and all of the metrics that you specified for every fold generated. \\nWhen the script finishes, go to your MLFlow UI and switch the experiment \\nto sklearn_creditcard_broad_search  to see all the runs you just logged. \\nYou should see something like in Figure\\xa0 4-25 .\\nFigure 4-24.  The entire validation script from Chapter 2 with some \\nMLFlow code additions to log everything during the validation \\nprocessChapter 4  Introdu CtIon to\\xa0MLF Low154Let’s try sorting this by the AUC score to find the best parameters for \\nthe AUC.\\xa0In the metrics  column, click auc_score .',\n",
       " 'processChapter 4  Introdu CtIon to\\xa0MLF Low154Let’s try sorting this by the AUC score to find the best parameters for \\nthe AUC.\\xa0In the metrics  column, click auc_score .\\nThe action should result in something that looks like Figure\\xa0 4-26 .\\nFigure 4-25.  The output you should see after the validation \\nexperiment has finished. Make sure you select the experiment titled \\nsklearn_creditcard_broad_searchChapter 4  Introdu CtIon to\\xa0MLF Low155You want to sort the columns in descending order, so click it again to \\nsee something that looks like Figure\\xa0 4-27 .\\nFigure 4-26.  The values are all sorted by auc_score in descending \\norder. We’ve highlighted this column so that you can more easily spot \\nthe difference between this figure and Figure\\xa0 4-25 . As you can see,',\n",
       " 'order. We’ve highlighted this column so that you can more easily spot \\nthe difference between this figure and Figure\\xa0 4-25 . As you can see, \\nthe AUC scores are in ascending order. You want to see the best AUC \\nscores, so you must sort in descending orderChapter 4  Introdu CtIon to\\xa0MLF Low156Perhaps you don’t really care about anything but the absolute best \\nscores. Say that you are targeting AUC scores that are at least 0.90. How \\nwould you go about filtering everything? Well, the UI provides a search bar \\nthat performs a search based on the SQL WHERE clause. So, to filter your \\noutput, type the following and click Search:\\nmetrics.\"auc_score\" >= 0.90\\nYou should see something like Figure\\xa0 4-28 . If you have copied and',\n",
       " 'output, type the following and click Search:\\nmetrics.\"auc_score\" >= 0.90\\nYou should see something like Figure\\xa0 4-28 . If you have copied and \\npasted the line of code, be sure to delete it and put in the quotation marks \\nagain if you encounter any errors about the quotation marks.\\nFigure 4-27.  The values are now sorted by AUC score in descending \\norder. Now you can see the runs that produced the best AUC scores \\nalong with the specific anomaly weight it had in that runChapter 4  Introdu CtIon to\\xa0MLF Low157Notice that we put \"auc_score\"  in quotation marks. This is for cases \\nwhere the metric name that you’ve logged contains characters like a dash \\nwhere it might not recognize the name if you typed it out like so:\\nmetrics.auc-score',\n",
       " 'where the metric name that you’ve logged contains characters like a dash \\nwhere it might not recognize the name if you typed it out like so:\\nmetrics.auc-score\\nThe proper convention for a metric logged as \"auc-score\"  would be to \\nfilter it like so:\\nmetrics.\"auc-score\" >= 0.90\\nNow let’s say that of these scores, you want to see the scores for \\nanomaly weights of 5 only. It doesn’t appear that there are any results with \\nthe anomaly weight of 1, so we will start with 5. For that, let’s type and \\nsearch the following:\\nmetrics.\"auc_score\" >= 0.90 AND parameters.anomaly_weight = \"5\"\\nYou should see something like Figure\\xa0 4-29 .\\nFigure 4-28.  The results of filtering all of the AUC scores to be above \\n0.90. As you can see, only a handful of runs produced AUC scores that',\n",
       " 'You should see something like Figure\\xa0 4-29 .\\nFigure 4-28.  The results of filtering all of the AUC scores to be above \\n0.90. As you can see, only a handful of runs produced AUC scores that \\nare at least 0.90Chapter 4  Introdu CtIon to\\xa0MLF Low158You put the 5\\xa0in quotation marks because the parameters seem to be \\nlogged as string values, whereas the metrics are logged as floats.\\nFrom this output, it seems that only two of the five folds with the \\nanomaly weight set to 5 had an AUC score above 0.90. Let’s quickly search \\nover the other parameters and check how many folds had an AUC score \\nabove 0.90 as well.\\nFor filtering the anomaly weight by 10, refer to Figure\\xa0 4-30 .\\nSo, three of the five folds with the anomaly weight set to 10 had an AUC \\nscore above 0.90.',\n",
       " 'above 0.90 as well.\\nFor filtering the anomaly weight by 10, refer to Figure\\xa0 4-30 .\\nSo, three of the five folds with the anomaly weight set to 10 had an AUC \\nscore above 0.90.\\nLet’s check 15 now. Refer to Figure\\xa0 4-31 .\\nFigure 4-29.  Filtering the runs to have only runs with the anomaly \\nweight set to 5 and to have an AUC score above 0.90\\nFigure 4-30.  Three runs for an anomaly weight of 10 also met your \\ncriteria for minimum AUC scoreChapter 4  Introdu CtIon to\\xa0MLF Low159You see similar results with 15.\\nWhat if you tighten the AUC score requirement to be a minimum of \\n0.95? Let’s check the runs for a minimum AUC of 0.95 and with an anomaly \\nweight of 5. Refer to Figure\\xa0 4-32 .\\nSo, it seems that only one fold reached an AUC score above 0.95 when \\nthe anomaly weight was 5.',\n",
       " 'weight of 5. Refer to Figure\\xa0 4-32 .\\nSo, it seems that only one fold reached an AUC score above 0.95 when \\nthe anomaly weight was 5.\\nWhat do the results look like for an anomaly weight of 10? Refer to \\nFigure\\xa0 4-33 .\\nFigure 4-31.  You can see that with an anomaly weight of 15, there \\nseems to be two folds that had an AUC score above 0.95\\nFigure 4-32.  This time, you see that only one of the folds for the runs \\nwith anomaly weight set to 5 has an AUC score above 0.95Chapter 4  Introdu CtIon to\\xa0MLF Low160Let’s check the runs with an anomaly weight of 15. Refer to Figure\\xa0 4-34 .\\nIt seems that for an anomaly weight of 15, only one run has achieved \\nan AUC score above 0.95. It seems that you can’t look at how you can \\nnarrow the scope without looking at the rest of the AUC scores.',\n",
       " 'an AUC score above 0.95. It seems that you can’t look at how you can \\nnarrow the scope without looking at the rest of the AUC scores.\\nIt appears to be the case that the best AUC scores seem to be between 5 \\nand 15.\\nAlright, so what if the higher anomaly weights were more consistent \\nin their AUC scores, and the smaller anomaly weight runs achieving the \\nhighest AUC scores were just flukes? To see how each anomaly weight \\nFigure 4-33.  With an anomaly weight of 10, only one run has an \\nAUC score above 0.95\\nFigure 4-34.  With an anomaly weight of 15, only one run has \\nachieved an AUC score above 0.95. From these results, you cannot \\nreally infer which weight setting is the best, so you have to narrow the \\nscope of your hyperparameter search. As far as you know, you could',\n",
       " 'really infer which weight setting is the best, so you have to narrow the \\nscope of your hyperparameter search. As far as you know, you could \\nhave missed the best setting, and it could be somewhere in between 5 \\nand 10 or 10 and 15Chapter 4  Introdu CtIon to\\xa0MLF Low161setting did, first remove the query statement, and click Search again. Next, \\nmake sure that the AUC scores are in descending order. Once you’re done, \\nrefer to Figure\\xa0 4-35  and verify that your output looks similar.\\nUsing the following code, let’s filter over all of the values for anomaly \\nweights and check what the AUC scores look like, replacing 1 with 5, 10, \\nand 15.\\nparameters.anomaly_weight = \"1\"\\nRefer to Figure\\xa0 4-36  to see the results of filtering by an anomaly weight \\nof 1.',\n",
       " 'weights and check what the AUC scores look like, replacing 1 with 5, 10, \\nand 15.\\nparameters.anomaly_weight = \"1\"\\nRefer to Figure\\xa0 4-36  to see the results of filtering by an anomaly weight \\nof 1.\\nFigure 4-35.  Ordering the runs by descending AUC scoreChapter 4  Introdu CtIon to\\xa0MLF Low162None of the scores have gone above 0.9, so you can automatically rule \\nout this anomaly weight setting. If you go back to your script, you can see \\nthat the average AUC was around 0.8437.\\nLet’s look at the runs with an anomaly weight of 5. Refer to Figure\\xa0 4-37 .\\nThe scores have improved noticeably. If you go back to the original \\nscript’s output, you can see that the average AUC score is now 0.9116.\\nThe rest of the anomaly weights all achieved the highest AUC score of',\n",
       " 'script’s output, you can see that the average AUC score is now 0.9116.\\nThe rest of the anomaly weights all achieved the highest AUC score of \\naround 0.975, so the average AUC is a better metric to help you narrow the \\nrange.\\nFigure 4-37.  Looking at the AUC scores of the runs with anomaly \\nweight of 5\\xa0in descending order. You can see a noticeable increase in \\nthe average AUC score when compared to an anomaly weight of 1\\nFigure 4-36.  Looking at the AUC scores of the runs with an anomaly \\nweight of 1\\xa0in descending orderChapter 4  Introdu CtIon to\\xa0MLF Low163Let’s now look at the runs with an anomaly weight of 10. Refer to \\nFigure\\xa0 4-38 .\\nThese scores seem even better than the ones for an anomaly weight of \\n5. This time, the average AUC score is around 0.9215.',\n",
       " 'Figure\\xa0 4-38 .\\nThese scores seem even better than the ones for an anomaly weight of \\n5. This time, the average AUC score is around 0.9215.\\nFinally, let’s look at the scores for an anomaly weight of 15. Refer to \\nFigure\\xa0 4-39  to see the results of filtering by an anomaly weight of 15.\\nFigure 4-38.  Looking at the AUC scores of the runs with an anomaly \\nweight of 10\\xa0in descending order. These scores seem even better\\nFigure 4-39.  Looking at the AUC scores of the runs with an anomaly \\nweight of 15\\xa0in descending order. The scores are very similar, but \\nthe average is ever so slightly worse, so the true range seems to be \\nsomewhere in between 10 and 15Chapter 4  Introdu CtIon to\\xa0MLF Low164The scores are very similar to each other, and indeed, the average AUC \\nscore is now 0.9212.',\n",
       " 'somewhere in between 10 and 15Chapter 4  Introdu CtIon to\\xa0MLF Low164The scores are very similar to each other, and indeed, the average AUC \\nscore is now 0.9212.\\nBased on these results, you can see that there seems to be an increase \\nfrom 5 to 10, but a slight decrease from 10 to 15. From this data, the \\nideal range seems to be somewhere in between 10 and 15, but again, the \\ndecrease in average AUC from 10 to 15 is essentially negligible. And so, \\nwhat if it’s potentially beyond 15, and you started out with the wrong range \\nto search over?\\nFrom the results of this validation experiment, it seems that you \\nhaven’t found a definite range of values that you know for sure you can \\nfocus on. And so, you must expand your range even more just to see if you',\n",
       " 'haven’t found a definite range of values that you know for sure you can \\nfocus on. And so, you must expand your range even more just to see if you \\ncan get better results with higher anomaly weights.\\nLooking at the distribution of data and how heavily the normal points \\noutnumber the anomalies, you can use your intuition to help guide your \\nhyperparameter search and expand the range far more.\\nNow that you know this, let’s try expanding the range far more.\\n Parameter Tuning\\xa0– Guided Search\\nThe best overall performances were achieved by anomaly weights 10 and \\n15, but it seems to be on an upward trend the higher up you go with the \\nanomaly weight.\\nNow that you know this, let’s try another validation run with a broader \\nrange of anomaly weights to try.',\n",
       " '15, but it seems to be on an upward trend the higher up you go with the \\nanomaly weight.\\nNow that you know this, let’s try another validation run with a broader \\nrange of anomaly weights to try.\\nGo back to the cell (or copy-paste it into a new cell) in Figure\\xa0 4-23  and \\nchange the anomaly weights so that they look like the following:\\nanomaly_weights = [10, 50, 100, 150, 200]\\nYou should see something like Figure\\xa0 4-40 .Chapter 4  Introdu CtIon to\\xa0MLF Low165The validation script itself should be the same, so if you simply \\nreplaced the anomaly weights in the original cell, don’t run the validation \\nscript yet!  Let’s create a new experiment so that you don’t clutter the \\noriginal tuning experiment with these new runs.\\nModify the following line in the old validation script so that it goes',\n",
       " 'script yet!  Let’s create a new experiment so that you don’t clutter the \\noriginal tuning experiment with these new runs.\\nModify the following line in the old validation script so that it goes \\nfrom\\nmlflow.set_experiment(\" sklearn_creditcard_broad_search\")\\nto\\nmlflow.set_experiment(\"sklearn_creditcard_guided_search\")\\nYou should see something like Figure\\xa0 4-41 .\\nFigure 4-40.  Setting a narrow range of values to search over during \\nthe second validation run\\nFigure 4-41.  Setting a new experiment called sklearn_creditcard_\\nguided_search so that the results of this second validation experiment \\nare stored separately\\nNow you can run this code. Once it finishes, go back to the UI, refresh \\nit, and select the new experiment named sklearn_creditcard_guided_',\n",
       " 'are stored separately\\nNow you can run this code. Once it finishes, go back to the UI, refresh \\nit, and select the new experiment named sklearn_creditcard_guided_\\nsearch . You should see something like Figure\\xa0 4-42 .Chapter 4  Introdu CtIon to\\xa0MLF Low166The whole point of broadening the range of anomaly weights that \\nyou are performing the tuning experiment on is to help you understand \\nwhere the best hyperparameter range may lie. You did not know this \\ninitially, so you picked a range that was too small to help you discover the \\nbest value. Now that you do know, you have expanded your search range \\nconsiderably.\\nFrom the results of this experiment, you can hopefully narrow your \\nrange a lot more and repeat the experiment with a massively reduced',\n",
       " 'considerably.\\nFrom the results of this experiment, you can hopefully narrow your \\nrange a lot more and repeat the experiment with a massively reduced \\nrange and arrive at the correct hyperparameter setting.\\nYou will now filter out each of the values by each unique anomaly \\nweight (10, 50, 100, 150, and 200) to get an idea of how the runs with that \\nsetting performed.\\nMake sure you’re sorting AUC scores in descending order, type the \\nfollowing query, and search:\\nparameters.anomaly_weight = \"10\"\\nYou should see something like Figure 4-43 .\\nFigure 4-42.  The results of the second validation experimentChapter 4  Introdu CtIon to\\xa0MLF Low167The average AUC score as displayed by the validation script is around \\n0.9215. Of course, this is the same result as from earlier.',\n",
       " '0.9215. Of course, this is the same result as from earlier.\\nLet’s see how the scores look for an anomaly weight of 50. Refer to \\nFigure\\xa0 4-44 .\\nThere appears to be a minute difference in the range of AUC scores \\nalready. Looking at the script, you can see that the average AUC is around \\n0.9248, so there appears to be a small increase in the AUC score.\\nLet’s keep going and check the results for the anomaly weight of 100. \\nRefer to Figure\\xa0 4-45 .\\nFigure 4-43.  Filtering the runs by anomaly weight of 10 and setting \\nthe AUC score to display in descending order\\nFigure 4-44.  Filtering the runs by an anomaly weight of 50 and \\nsetting the AUC score to display in descending order. It seems there’s a',\n",
       " 'the AUC score to display in descending order\\nFigure 4-44.  Filtering the runs by an anomaly weight of 50 and \\nsetting the AUC score to display in descending order. It seems there’s a \\nslight difference in valuesChapter 4  Introdu CtIon to\\xa0MLF Low168The average this time appears to be 0.9327. Despite the massive \\nincrease in weight, the average AUC score did not go up that high. \\nHowever, notice that the first result with an AUC score of 0.995 has \\nappeared. The best AUC score up until the anomaly weight of 50 was 0.975, \\nbut this anomaly weight setting has broken past that.\\nLet’s keep going and see if it increases with an anomaly weight setting \\nof 150. Refer to Figure\\xa0 4-46A .\\nFigure 4-45.  Filtering the runs by an anomaly weight of 100 and',\n",
       " 'Let’s keep going and see if it increases with an anomaly weight setting \\nof 150. Refer to Figure\\xa0 4-46A .\\nFigure 4-45.  Filtering the runs by an anomaly weight of 100 and \\nsetting the AUC score to display in descending order\\nFigure 4-46A.  Filtering the runs by an anomaly weight of 150 and \\nsetting the AUC score to display in descending orderChapter 4  Introdu CtIon to\\xa0MLF Low169The AUC scores overall seem to be a bit higher. Indeed, the average \\nAUC score is now 0.9365, so there was an increase. Finally, let’s check the \\nAUC scores for an anomaly weight setting of 200. Refer to Figure\\xa0 4-46B .\\nThe new average AUC now is 0.9396, so this anomaly weight setting \\nseems even better.\\nIn fact, you still weren’t able to come to a conclusion about an optimal',\n",
       " 'The new average AUC now is 0.9396, so this anomaly weight setting \\nseems even better.\\nIn fact, you still weren’t able to come to a conclusion about an optimal \\nrange, since the AUC scores keep increasing as you set higher anomaly \\nweights.\\nSo, from this, you know that the best hyperparameter setting is \\nsomewhere above 200. You simply shift the range of the scope to start at \\n200 and search over a slightly different area, and once you have found a \\ngood range of values to search over (eventually the AUC scores will start \\ntrending down as you increase the anomaly weight), you can narrow the \\nfocus and start searching again.\\nAfter a certain amount of precision with the parameter value, you start \\nto see diminishing returns where the added effort only produces negligible',\n",
       " 'focus and start searching again.\\nAfter a certain amount of precision with the parameter value, you start \\nto see diminishing returns where the added effort only produces negligible \\nimprovements in performance, but you will likely encounter this as you \\nstart getting deeper into the decimal values.\\nFigure 4-46B.  Filtering the runs by an anomaly weight of 200 and \\nsetting the AUC score to display in descending orderChapter 4  Introdu CtIon to\\xa0MLF Low170Hopefully now you understand more about how you can integrate \\nMLFlow into the model training, testing, and validation pipeline using \\nscikit-learn. You also looked at how to use the UI for basic comparisons, \\nalong with how you might perform hyperparameter tuning more easily \\nusing MLFlow.',\n",
       " 'scikit-learn. You also looked at how to use the UI for basic comparisons, \\nalong with how you might perform hyperparameter tuning more easily \\nusing MLFlow.\\nA quick note to make is that if you’d like to perform more complicated \\nsearches over multiple metrics or parameters, MLFlow provides \\nfunctionality through the API to let you do so via SQL searches within the \\ncode, letting you order by multiple columns, for example.\\nMLFlow also provides support for logging metrics, parameters, \\nartifacts, and even models for other frameworks in their documentation. \\nWe will now take a look at how to integrate MLFlow with TensorFlow 2.0+/\\nKeras, PyTorch, and PySpark.\\n MLFlow and\\xa0Other Frameworks\\n MLFlow with\\xa0TensorFlow 2.0 (Keras)\\nMLFlow provides easy integration with TensorFlow 2.0+ (any version of',\n",
       " 'Keras, PyTorch, and PySpark.\\n MLFlow and\\xa0Other Frameworks\\n MLFlow with\\xa0TensorFlow 2.0 (Keras)\\nMLFlow provides easy integration with TensorFlow 2.0+ (any version of \\nTensorFlow 2.0 and above). To see how, let’s go over a very basic example \\nof a handwritten digit classifier model on the MNIST dataset. We will be \\nusing the built-in Keras module to keep things simple for demonstration \\npurposes. MLFlow supports TensorFlow 1.12 at a minimum, so this code \\nshould run as long as you have at least TensorFlow 1.12.\\nWe will assume a basic level of familiarity with TensorFlow 2, so \\nwe won’t go into much depth about what the functions, model layers, \\noptimizers, and loss functions mean.\\nBefore we begin, here are the versions of TensorFlow, CUDA, and',\n",
       " 'we won’t go into much depth about what the functions, model layers, \\noptimizers, and loss functions mean.\\nBefore we begin, here are the versions of TensorFlow, CUDA, and \\nCuDNN that we used. Keep in mind that we ran this using the GPU version Chapter 4  Introdu CtIon to\\xa0MLF Low171of TensorFlow (the package is called tensorflow-gpu), although you should \\nbe able to run this without a GPU at the cost of it taking longer:\\n• TensorFlow  (GPU version)\\xa0– 2.3.0\\n• CUDA \\xa0– 10.1\\n• CuDNN \\xa0– v7.6.5.32 for CUDA 10.1\\n• Sklearn \\xa0– 0.22.2.post1\\n• MLFlow –  1.10.0\\n Data Processing\\nHere is the code to import the necessary modules and print out their \\nversions:\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten',\n",
       " 'versions:\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten\\nfrom tensorflow.keras.datasets import mnist\\nimport numpy as np\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\nimport sklearn\\nfrom sklearn.metrics import roc_auc_score\\nimport mlflow\\nimport mlflow.tensorflow\\nprint(\"TensorFlow: {}\".format(tf.__version__))\\nprint(\"Scikit-Learn: {}\".format(sklearn.__version__))\\nprint(\"Numpy: {}\".format(np.__version__))\\nprint(\"MLFlow: {}\".format(mlflow.__version__))\\nprint(\"Matplotlib: {}\".format(matplotlib.__version__))Chapter 4  Introdu CtIon to\\xa0MLF Low172You should see something like Figure\\xa0 4-47 .\\nLet’s now load the data:\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()',\n",
       " 'Let’s now load the data:\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\nKeras, and by extension TensorFlow, provides the MNIST handwritten \\ndigit dataset for you, so all you need to do to load the data is call the \\nfunction, like in Figure\\xa0 4-48 .\\nRefer to Figure\\xa0 4-48  to see the code in a cell.\\nYou can even see what one of these images looks like. Run the \\nfollowing:\\nplt.imshow(x_train[0], cmap=\\'gray\\'), print(\"Class: \", y_train[0])\\nYou should see something like Figure\\xa0 4-49 .\\nFigure 4-47.  Importing the necessary modules and printing their \\nversions\\nFigure 4-48.  Defining x_train, y_train, x_test, and y_testChapter 4  Introdu CtIon to\\xa0MLF Low173Also notice that you printed out the class label associated with this',\n",
       " 'versions\\nFigure 4-48.  Defining x_train, y_train, x_test, and y_testChapter 4  Introdu CtIon to\\xa0MLF Low173Also notice that you printed out the class label associated with this \\nspecific image. The labels are all integers between 0 and 9, each associated \\nwith an image that shows a handwritten digit from 0 to 9.\\nSince 2D convolutional layers in TensorFlow/Keras expect four \\ndimensions in the format of (m, h, w, c) where m stands for the number of \\nsamples in the dataset, h and w stand for the height and width, respectively, \\nand c stands for the number of channels (three if it’s an RGB color image \\nfor example), you need to reshape your data so that it conforms to these \\nspecifications. Your images are all black and white, so they technically have',\n",
       " 'for example), you need to reshape your data so that it conforms to these \\nspecifications. Your images are all black and white, so they technically have \\na channel of one. And so, you must reshape them like so:\\nx_train = x_train.reshape(x_train.shape[0], x_train.shape[1], \\nx_train.shape[2], 1)\\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1],  \\nx_test.shape[2], 1)\\ny_train = tf.keras.utils.to_categorical(y_train)\\ny_test = tf.keras.utils.to_categorical(y_test)\\nRefer to Figure\\xa0 4-50  to see that code in a cell.\\nFigure 4-49.  Looking at what one of the data samples looks like \\nusing matplotlib. You also printed out the class label associated with \\nthis sample, which was 5Chapter 4  Introdu CtIon to\\xa0MLF Low174You converted the y sets by calling a function called',\n",
       " 'using matplotlib. You also printed out the class label associated with \\nthis sample, which was 5Chapter 4  Introdu CtIon to\\xa0MLF Low174You converted the y sets by calling a function called  \\nto_categorical().  This converts each sample from an integer value of \\nsay 2 or 4 corresponding to the digit represented by the x samples into a \\none-hot encoded vector.\\nSamples in this format are now 0 vectors with a num_classes  number \\nof digits. In other words, these vectors all have a length matching the total \\nnumber of classes. Whatever value the label was is now the index of the \\nvalue 1. And so, if the label is 1, the value at the index of 1\\xa0in this vector will \\nbe one, and everything else is a 0.\\nThis may be a little confusing, so refer to Figure\\xa0 4-51  to see what the',\n",
       " 'value 1. And so, if the label is 1, the value at the index of 1\\xa0in this vector will \\nbe one, and everything else is a 0.\\nThis may be a little confusing, so refer to Figure\\xa0 4-51  to see what the \\none-hot encoded label looks like for a digit representing 5.\\nFigure 4-50.  Reshaping the data to include one channel, conforming \\nwith the specifications of the convolutional layers. Additionally, the y \\nsets are being converted to one-hot encoded formats\\nFigure 4-51.  The new output of the one-hot encoded label \\nrepresenting a value of 5. Notice that the value at index 5 is now 1\\nAs you can see, the index of the 1 is 5, corresponding to the first  \\nx_train  example you looked at earlier, which was the digit 5.\\nNow, let’s print out the shapes:\\nprint(\"Shapes\")',\n",
       " 'As you can see, the index of the 1 is 5, corresponding to the first  \\nx_train  example you looked at earlier, which was the digit 5.\\nNow, let’s print out the shapes:\\nprint(\"Shapes\")\\nprint(\"x_train: {}\\\\ny_train: {}\".format(x_train.shape,  \\ny_train.shape))\\nprint(\"x_test: {}\\\\ny_test: {}\".format(x_test.shape,  \\ny_test.shape))\\nYou should now see something like Figure\\xa0 4-52 .Chapter 4  Introdu CtIon to\\xa0MLF Low175 MLFlow Run\\xa0– Training and\\xa0Evaluating\\nLet’s move on to the creation of your model. You will be using the \\nSequential  method of model creation. The model will be quite simple, \\nconsisting of a couple 2D convolutional layers that feed into three dense \\nlayers. Run the following:\\nmodel = Sequential()\\nmodel.add(Conv2D(filters=16, kernel_size=3, strides=2,',\n",
       " 'consisting of a couple 2D convolutional layers that feed into three dense \\nlayers. Run the following:\\nmodel = Sequential()\\nmodel.add(Conv2D(filters=16, kernel_size=3, strides=2, \\npadding=\\'same\\', input_shape=(28, 28, 1), activation=\"relu\"))\\nmodel.add(Conv2D(filters=8, kernel_size=3, strides=2, \\npadding=\\'same\\', input_shape=(28, 28, 1), activation=\"relu\"))\\nmodel.add(Flatten())\\nmodel.add(Dense(30, activation=\"relu\"))\\nmodel.add(Dense(20, activation=\"relu\"))\\nmodel.add(Dense(10, activation=\"softmax\"))\\nmodel.summary()\\nYou should see something like Figure\\xa0 4-53 .\\nFigure 4-52.  Printing the output shapes of the processed dataChapter 4  Introdu CtIon to\\xa0MLF Low176Let’s now compile your model using the Adam optimizer and \\ncategorical cross-entropy for your loss. For your metric, you will only be',\n",
       " 'categorical cross-entropy for your loss. For your metric, you will only be \\nusing accuracy. Run the following:\\nmodel.compile(optimizer=\"Adam\",  \\nloss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\\nYou should see something like Figure\\xa0 4-54 .\\nFigure 4-53.  Creating the model and outputting a summary of the \\nmodel’s architecture\\nFigure 4-54.  Compiling your model, setting the optimizer to Adam \\noptimizer, setting the loss to categorical cross-entropy, and setting the \\nmetric to be accuracy\\nNow you get to the part where you tell MLFlow to log this run. You \\nwant all of the metrics to be logged to the same run, so you must tell \\nMLFlow specifically to run a block of code in the context of the same run. \\nTo do so, you once again block your code using the following line:',\n",
       " 'MLFlow specifically to run a block of code in the context of the same run. \\nTo do so, you once again block your code using the following line:\\nwith mlflow.start_run():Chapter 4  Introdu CtIon to\\xa0MLF Low177With that, run the following to set the experiment name, train the \\nmodel, get the evaluation metrics you need, and log them all to MLFlow:\\nmlflow.set_experiment(\"TF_Keras_MNIST\")\\nwith mlflow.start_run():\\n    mlflow.tensorflow.autolog()\\n    model.fit(x=x_train, y=y_train, batch_size=256, epochs=10)\\n    preds = model.predict(x_test)\\n    preds = np.round(preds)\\n    eval_acc = model.evaluate(x_test, y_test)[1]\\n    auc_score = roc_auc_score(y_test, preds)\\n    print(\"eval_acc: \", eval_acc)\\n    print(\"auc_score: \", auc_score)\\n    mlflow.tensorflow.mlflow.log_metric(\"eval_acc\", eval_acc)',\n",
       " 'auc_score = roc_auc_score(y_test, preds)\\n    print(\"eval_acc: \", eval_acc)\\n    print(\"auc_score: \", auc_score)\\n    mlflow.tensorflow.mlflow.log_metric(\"eval_acc\", eval_acc)\\n    mlflow.tensorflow.mlflow.log_metric(\"auc_score\", auc_score)\\nmlflow.end_run()\\nRefer to Figure\\xa0 4-55  to see the output. Ignore the warning messages. \\nThey don’t hinder the training process or the performance of the model.Chapter 4  Introdu CtIon to\\xa0MLF Low178Another new line of code is the following:\\nmlflow.keras.autolog()\\nThis basically tells MLFlow to log all the parameters and metrics \\nassociated with the particular TensorFlow/Keras model. As you will see \\nshortly, MLFlow will log the hyperparameters, model metrics listed in \\nthe compile()  function, and even the model itself once the training has \\nfinished.',\n",
       " 'shortly, MLFlow will log the hyperparameters, model metrics listed in \\nthe compile()  function, and even the model itself once the training has \\nfinished.\\nFigure 4-55.  Output of the MLFlow run and the training process. You \\ncan also see that the metrics you calculated have been updatedChapter 4  Introdu CtIon to\\xa0MLF Low179 MLFlow UI\\xa0– Checking Your Run\\nWith that, let’s now open the MLFlow UI and check your run in MLFlow. \\nMake sure your terminal or command prompt is in the same directory \\nwhere the mlruns are stored. Usually, MLFlow saves all these runs in the \\nsame directory of the Jupyter notebook.\\nNow that you’ve opened the UI, you should see something like \\nFigure\\xa0 4-56 .\\nClick the tab called TF_Keras_MNIST to see the results of the',\n",
       " 'same directory of the Jupyter notebook.\\nNow that you’ve opened the UI, you should see something like \\nFigure\\xa0 4-56 .\\nClick the tab called TF_Keras_MNIST to see the results of the \\nexperiment you just logged. You should see something like Figure\\xa0 4-57 .\\nFigure 4-56.  The MLFlow UI after running the TensorFlow \\nexperiment. Notice that there is a new experiment titled TF_Keras_\\nMNISTChapter 4  Introdu CtIon to\\xa0MLF Low180As you can see, your run was just successfully logged. Next, click it to \\nexplore all of the parameters, metrics, and artifacts that MLFlow logged.\\nYou should see something like Figure\\xa0 4-58 .\\nFigure 4-57.  Opening the experiment titled TF_Keras_MNIST.\\xa0You \\ncan see that it successfully logged a run\\nFigure 4-58.  Looking at the specific run logged in the experiment.',\n",
       " 'Figure 4-57.  Opening the experiment titled TF_Keras_MNIST.\\xa0You \\ncan see that it successfully logged a run\\nFigure 4-58.  Looking at the specific run logged in the experiment. \\nAs you can see, all the parameters and metrics were logged, even the \\none you specified. It also shows you the duration and the status of the \\nrun, so now you know how long it took to train the model as well as \\nwhether or not it completedChapter 4  Introdu CtIon to\\xa0MLF Low181MLFlow saved all of the hyperparameters used when creating the \\nmodel. This could be very useful for hyperparameter tuning on a validation \\nset, for example, where you are trying to tune many hyperparameters \\nat once. For example, you can definitely tune batch_size , epochs , or \\nsomething related to the Adam optimizer like opt_learning_rate ,',\n",
       " 'at once. For example, you can definitely tune batch_size , epochs , or \\nsomething related to the Adam optimizer like opt_learning_rate ,  \\nopt_beta_1 , or opt_beta_2 .\\nAs you can see in Figure\\xa0 4-58 , MLFlow saved the model metrics for \\naccuracy and loss as calculated during the training process. In addition, \\nMLFlow also saved the metrics that you defined.\\nScroll down to artifacts and click model and then data. You should see \\nsomething like Figure\\xa0 4-59 .\\nHere, you can see that MLFlow also saved the model after the training \\nprocess finished. In fact, let’s briefly look at how you can load this model. \\nMake sure you go to the top and copy the run ID before doing this.\\n Loading an\\xa0MLFlow Model\\nWith the run ID copied, head on over to the notebook and create a new',\n",
       " 'Make sure you go to the top and copy the run ID before doing this.\\n Loading an\\xa0MLFlow Model\\nWith the run ID copied, head on over to the notebook and create a new \\ncell. Run the following code, but replace the run ID with yours:\\nloaded_model =  \\nmlflow.keras.load_model(\"runs:/YOUR_RUN_ID/model\")\\nFigure 4-59.  Upon closer inspection of the artifacts, it seems MLFlow \\nhas also logged the model itselfChapter 4  Introdu CtIon to\\xa0MLF Low182Your code should look similar to Figure\\xa0 4-60 . Our run was \\nba423a8f28d24b67b8f703ca6be43fc2 , so that’s what we replaced  \\nYOUR_RUN_ID  with.\\nYou’ll notice that we did mlflow.keras  instead of mlflow.tensorflow . \\nThis is because this model is technically a Keras model, and so it conforms \\nto the specific load_model()  code in the mlflow.keras  module.',\n",
       " 'This is because this model is technically a Keras model, and so it conforms \\nto the specific load_model()  code in the mlflow.keras  module.\\nRun the following code to quickly calculate the same evaluation \\nmetrics that you logged earlier:\\neval_loss, eval_acc = loaded_model.evaluate(x_test, y_test)\\npreds = loaded_model.predict(x_test)\\npreds = np.round(preds)\\neval_auc = roc_auc_score(y_test, preds)\\nprint(\"Eval Loss:\", eval_loss)\\nprint(\"Eval Acc:\", eval_acc)\\nprint(\"Eval AUC:\", eval_auc)\\nThis just ensures that the model is the same and demonstrates that \\nyou can use the model to make predictions. Refer to Figure\\xa0 4-61  to see the \\noutput.\\nFigure 4-60.  Loading a logged model using a specific run. Notice that \\nwe are doing mlflow.keras. This is because the model is technically a',\n",
       " 'output.\\nFigure 4-60.  Loading a logged model using a specific run. Notice that \\nwe are doing mlflow.keras. This is because the model is technically a \\nKeras modelChapter 4  Introdu CtIon to\\xa0MLF Low183As you can see, this output matches the values from the output of \\nthe run earlier. Additionally, this model is also functional and can make \\npredictions.\\nAnd with that, you now know how to integrate MLFlow into your \\nTensorFlow 2.0+ experiments. Again, MLFlow supports TensorFlow 1.12+, \\nwhich also contains the Keras submodule. This means that you should be \\nable to follow the same convention to log tf.keras module code as long as \\nyou have TensorFlow 1.12+.\\nIn practice, you are likely to have functions to build and compile the',\n",
       " 'able to follow the same convention to log tf.keras module code as long as \\nyou have TensorFlow 1.12+.\\nIn practice, you are likely to have functions to build and compile the \\nmodel, functions to train the model, and functions to evaluate and perhaps \\neven validate the model. Just be sure to call all of them in the block with \\nmlflow.start_run():  so that MLFlow knows all of this is happening \\nwithin the same run.\\nNext, let’s look at how to integrate MLFlow with PyTorch.\\n MLFlow with\\xa0PyTorch\\nMLFlow also provides integration with PyTorch. While the process isn’t as \\neasy as with Keras or TensorFlow, integrating MLFlow into your existing \\nPyTorch code is quite simple. To see how to do so, we will be exploring a \\nsimple convolutional neural network applied to the MNIST dataset once \\nagain.',\n",
       " 'PyTorch code is quite simple. To see how to do so, we will be exploring a \\nsimple convolutional neural network applied to the MNIST dataset once \\nagain.\\nFigure 4-61.  The output of the code block printing out the loss, \\naccuracy, and AUC score when the model was evaluated on the test \\nset. These three values match the corresponding values from the \\noutput of the run earlierChapter 4  Introdu CtIon to\\xa0MLF Low184Before we begin, here are the versions of the modules we are using, \\nincluding CUDA and CuDNN:\\n• Torch \\xa0- 1.6.0\\n• Torchvision \\xa0– 0.7.0\\n• CUDA \\xa0– 10.1\\n• CuDNN \\xa0– v7.6.5.32 for CUDA 10.1\\n• Sklearn \\xa0– 0.22.2.post1\\n• MLFlow –  1.10.0\\n• numpy –  1.18.5\\n Data Processing\\nLet’s get started. Here’s the code to import the necessary modules, print',\n",
       " '• CuDNN \\xa0– v7.6.5.32 for CUDA 10.1\\n• Sklearn \\xa0– 0.22.2.post1\\n• MLFlow –  1.10.0\\n• numpy –  1.18.5\\n Data Processing\\nLet’s get started. Here’s the code to import the necessary modules, print \\nout their versions, and set the device that PyTorch will use:\\nimport torch\\nimport torch.nn as nn\\nfrom torch.utils import data\\nimport torchvision\\nimport torchvision.datasets\\nimport sklearn\\nfrom sklearn.metrics import roc_auc_score, accuracy_score\\nimport numpy as np\\nimport mlflow\\nimport mlflow.pytorch\\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() \\nelse \"cpu\")Chapter 4  Introdu CtIon to\\xa0MLF Low185print(\"PyTorch: {}\".format(torch.__version__))\\nprint(\"torchvision: {}\".format(torchvision.__version__))\\nprint(\"sklearn: {}\".format(sklearn.__version__))\\nprint(\"MLFlow: {}\".format(mlflow.__version__))',\n",
       " 'print(\"torchvision: {}\".format(torchvision.__version__))\\nprint(\"sklearn: {}\".format(sklearn.__version__))\\nprint(\"MLFlow: {}\".format(mlflow.__version__))\\nprint(\"Numpy: {}\".format(np.__version__))\\nprint(\"Device: \", device)\\nRefer to Figure\\xa0 4-62  to see the output.\\nThe line of code\\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() \\nelse \"cpu\")\\ntells PyTorch which device to run the code on. If there is a GPU that CUDA \\ncan connect to, it will use that instead. Otherwise, it will run everything \\non the CPU.\\xa0In our case, we have CUDA set up with our GPU, so Torch \\ndisplays “cuda:0”  as seen in Figure\\xa0 4-62 .\\nFigure 4-62.  Importing the necessary modules and printing the \\nversions of the modulesChapter 4  Introdu CtIon to\\xa0MLF Low186Next, you will define some basic hyperparameters:',\n",
       " \"Figure 4-62.  Importing the necessary modules and printing the \\nversions of the modulesChapter 4  Introdu CtIon to\\xa0MLF Low186Next, you will define some basic hyperparameters:\\nbatch_size = 256\\nnum_classes = 10\\nlearning_rate = 0.001\\nRefer to Figure\\xa0 4-63  to see them in a cell.\\nNext, you will load in the MNIST dataset. Like Keras and TensorFlow, \\nPyTorch also provides example datasets. In this case, you are loading \\nMNIST:\\ntrain_set = torchvision.datasets.MNIST(root='./data', \\ntrain= True, download= True, transform= None)\\ntest_set = torchvision.datasets.MNIST(root='./data', \\ntrain= False, download= True, transform= None)\\nRefer to Figure\\xa0 4-64  to see this code in a cell.\\nYou will now define your x_train , y_train , x_test , and y_test  \\ndatasets:\",\n",
       " 'train= False, download= True, transform= None)\\nRefer to Figure\\xa0 4-64  to see this code in a cell.\\nYou will now define your x_train , y_train , x_test , and y_test  \\ndatasets:\\nx_train, y_train = train_set.data, train_set.targets\\nx_test, y_test = test_set.data, test_set.targets\\nRefer to Figure\\xa0 4-65 .\\nFigure 4-63.  Setting the hyperparameters relevant to the training of \\nthe model\\nFigure 4-64.  Defining the training and testing sets by loading the \\ndata from PyTorchChapter 4  Introdu CtIon to\\xa0MLF Low187In PyTorch, you want the data to be channels first. In other words, \\nthe format of the data should be (m, c, h, w) , where m stands for the \\nnumber of samples, c stands for the number of channels, h stands for the \\nheight of the samples, and w stands for the width of the samples.',\n",
       " 'number of samples, c stands for the number of channels, h stands for the \\nheight of the samples, and w stands for the width of the samples.\\nNotice that this is the “opposite” format of how Keras and TensorFlow \\ndo it by default, which is channels last. In Keras and TensorFlow, you can \\nalso do channels first, but you must specify that you are doing it this way.\\nLet’s reshape your x-sets:\\nx_train, y_train = train_set.data, train_set.targets\\nx_test, y_test = test_set.data, test_set.targets\\nRefer to Figure\\xa0 4-66  to see this code in a cell.\\nFigure 4-65.  Creating your x_train, y_train, x_test, and y_test data \\nsets from the training and testing sets\\nFigure 4-66.  Reshaping the x-sets so the data is encoded in a \\nchannels-first format',\n",
       " 'Figure 4-65.  Creating your x_train, y_train, x_test, and y_test data \\nsets from the training and testing sets\\nFigure 4-66.  Reshaping the x-sets so the data is encoded in a \\nchannels-first format\\nFigure 4-67.  The output of the first sample in the y_train set. Note \\nthat the numbers are not in a one-hot encoded formatBefore you print out all the shapes, note that your y-sets are not in a \\none-hot encoded format. Run the following:\\ny_train[0]\\nRefer to Figure\\xa0 4-67 .Chapter 4  Introdu CtIon to\\xa0MLF Low188Notice that this outputs a number, not a vector. You must convert \\nyour y-sets into a one-hot encoded format. However, there isn’t a handy \\nfunction like keras.utils.to_categorical()  you can just call, so you will \\ndefine one:\\ndef to_one_hot(num_classes, labels):',\n",
       " 'your y-sets into a one-hot encoded format. However, there isn’t a handy \\nfunction like keras.utils.to_categorical()  you can just call, so you will \\ndefine one:\\ndef to_one_hot(num_classes, labels):\\n    one_hot = torch.zeros(([labels.shape[0], num_classes]))\\n    for f in range(len(labels)):\\n        one_hot[f][labels[f]] = 1\\n    return one_hot\\nThat being said, you can always call keras.utils.to_categorical() : \\nand type-cast the resulting output to a PyTorch tensor.\\nRefer to Figure\\xa0 4-68  to see this in a cell.\\nNow let’s convert your y-sets to be in a one-hot encoded format:\\ny_train = to_one_hot(num_classes, y_train)\\ny_test = to_one_hot(num_classes, y_test)\\nRefer to Figure\\xa0 4-69  to see this code in a cell.\\nFigure 4-68.  A custom function that converts the input called',\n",
       " 'y_train = to_one_hot(num_classes, y_train)\\ny_test = to_one_hot(num_classes, y_test)\\nRefer to Figure\\xa0 4-69  to see this code in a cell.\\nFigure 4-68.  A custom function that converts the input called \\n“labels, ” given the number of classes, into a one-hot encoded format \\nand returns it\\nFigure 4-69.  Converting your y-sets into a one-hot encoded format \\nusing your custom functionChapter 4  Introdu CtIon to\\xa0MLF Low189Let’s check what y_train  looks like now:\\ny_train[0]\\nRefer to Figure\\xa0 4-70 .\\nAs you can see, it is now in a one-hot encoded format. Now you can \\nproceed to checking the shapes of your data sets:\\nprint(\"Shapes\")\\nprint(\"x_train: {}\\\\ny_train: {}\".format(x_train.shape,  \\ny_train.shape))\\nprint(\"x_test: {}\\\\ny_test: {}\".format(x_test.shape,  \\ny_test.shape))',\n",
       " 'print(\"Shapes\")\\nprint(\"x_train: {}\\\\ny_train: {}\".format(x_train.shape,  \\ny_train.shape))\\nprint(\"x_test: {}\\\\ny_test: {}\".format(x_test.shape,  \\ny_test.shape))\\nYou should see something like Figure\\xa0 4-71 .\\nFigure 4-70.  Checking the output of the first sample in y_train, you \\nnow see that the tensor has been converted into a one-hot encoded \\nformat\\nFigure 4-71.  Printing the shapes of your training and testing sets. As \\nyou can see, the x-sets are in a channels-first format, and the y-sets are \\nin a one-hot encoded formatChapter 4  Introdu CtIon to\\xa0MLF Low190 MLFlow Run\\xa0– Training and\\xa0Evaluating\\nNow, let’s define your model. A popular convention in PyTorch is to define \\nthe model as a class since it allows you to much more easily use the GPU',\n",
       " 'Now, let’s define your model. A popular convention in PyTorch is to define \\nthe model as a class since it allows you to much more easily use the GPU \\nwhile training. Instead of passing in every layer to the GPU, you can just \\nsend in the model object directly.\\nRun the following code to define your model:\\nclass model(nn.Module):\\n    def __init__(self):\\n        super(model, self).__init__()\\n        # IN 1x28x28 OUT 16x14x14\\n         self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, \\nkernel_size=3, stride=2, padding=1, dilation=1)\\n        # IN 16x14x14 OUT 32x6x6\\n         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, \\nkernel_size=3, stride=2, padding=0, dilation=1)\\n        # IN 32x6x6 OUT 64x2x2\\n         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64,',\n",
       " 'kernel_size=3, stride=2, padding=0, dilation=1)\\n        # IN 32x6x6 OUT 64x2x2\\n         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, \\nkernel_size=3, stride=2, padding=0, dilation=1)\\n        # IN 64x2x2 OUT 256\\n        self.flat1 = nn.Flatten()\\n         self.dense1 = nn.Linear(in_features=256,  \\nout_features=128)\\n         self.dense2 = nn.Linear(in_features=128,  \\nout_features=64)\\n         self.dense3 = nn.Linear(in_features=64,  \\nout_features=10)\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = nn.ReLU()(x)Chapter 4  Introdu CtIon to\\xa0MLF Low191        x = self.conv2(x)\\n        x = nn.ReLU()(x)\\n        x = self.conv3(x)\\n        x = nn.ReLU()(x)\\n        x = self.flat1(x)\\n        x = self.dense1(x)\\n        x = nn.ReLU()(x)\\n        x = self.dense2(x)',\n",
       " 'x = nn.ReLU()(x)\\n        x = self.conv3(x)\\n        x = nn.ReLU()(x)\\n        x = self.flat1(x)\\n        x = self.dense1(x)\\n        x = nn.ReLU()(x)\\n        x = self.dense2(x)\\n        x = nn.ReLU()(x)\\n        x = self.dense3(x)\\n        x = nn.Softmax()(x)\\n        return x\\nRefer to Figure\\xa0 4-72 .\\nFigure 4-72.  Defining the model’s architecture as a classChapter 4  Introdu CtIon to\\xa0MLF Low192Next, let’s send your model to the device, define and initialize an \\ninstance of Adam optimizer with the learning rate you set earlier, and set \\nyour loss function:\\nmodel = model().to(device)\\noptimizer = torch.optim.Adam(model.parameters(),  \\nlr=learning_rate)\\ncriterion = nn.BCELoss()\\nRefer to Figure\\xa0 4-73 .\\nNext, you will define a data loader using functionality provided by',\n",
       " 'optimizer = torch.optim.Adam(model.parameters(),  \\nlr=learning_rate)\\ncriterion = nn.BCELoss()\\nRefer to Figure\\xa0 4-73 .\\nNext, you will define a data loader using functionality provided by \\nPyTorch to take care of batching your data set:\\ndataset = data.TensorDataset(x_train,y_train)\\ntrain_loader = data.DataLoader(dataset, batch_size=batch_size)\\nRefer to Figure\\xa0 4-74 .\\nFigure 4-73.  Sending the model object to the device, defining your \\noptimizer, and initializing the loss function\\nFigure 4-74.  Creating a data loader object out of your data set. With \\nthis functionality, PyTorch will batch your data set for you, allowing \\nyou to pass in a minibatch at a time in your training loop. This \\nessentially is what the TensorFlow 2/Keras .fit()  function does, but',\n",
       " 'you to pass in a minibatch at a time in your training loop. This \\nessentially is what the TensorFlow 2/Keras .fit()  function does, but \\nit’s all abstracted for youChapter 4  Introdu CtIon to\\xa0MLF Low193As you can see, this is much simpler than having to make an intricate \\nloop to batch and pass in data yourself.\\nFinally, let’s define the training loop:\\nnum_epochs = 5\\nfor f in range(num_epochs):\\n    for batch_num, minibatch in enumerate(train_loader):\\n        minibatch_x, minibatch_y = minibatch[0], minibatch[1]\\n         output = model.forward(torch.Tensor  \\n(minibatch_x.float()).cuda())\\n         loss = criterion(output, torch.Tensor  \\n(minibatch_y.float()).cuda())\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()',\n",
       " '(minibatch_x.float()).cuda())\\n         loss = criterion(output, torch.Tensor  \\n(minibatch_y.float()).cuda())\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        print(f\"Epoch {f} Batch_Num {batch_num} Loss {loss}\")\\nThis can take at least a couple minutes depending on your GPU, and \\neven longer if you’re using a CPU.\\xa0Feel free to lower the number of epochs \\nif you’d like to decrease total training time.\\nYou should see an output like Figure\\xa0 4-75 .Chapter 4  Introdu CtIon to\\xa0MLF Low194Now, let’s start an MLFlow run, calculate the metrics you want, and log \\neverything:\\nmlflow.set_experiment(\"PyTorch_MNIST\")\\nwith mlflow.start_run():\\n    preds = model.forward(torch.Tensor(x_test.float()).cuda())\\n    preds = np.round(preds.detach().cpu().numpy())',\n",
       " 'everything:\\nmlflow.set_experiment(\"PyTorch_MNIST\")\\nwith mlflow.start_run():\\n    preds = model.forward(torch.Tensor(x_test.float()).cuda())\\n    preds = np.round(preds.detach().cpu().numpy())\\n    eval_acc = accuracy_score(y_test, preds)\\n    auc_score = roc_auc_score(y_test, preds)\\n    mlflow.log_param(\"batch_size\", batch_size)\\n    mlflow.log_param(\"num_epochs\", num_epochs)\\n    mlflow.log_param(\"learning_rate\", learning_rate)\\nFigure 4-75.  Output of your training loop. Feel free to reduce the \\nnumber of epochs to save on training time, but this could potentially \\nhinder the model’s performanceChapter 4  Introdu CtIon to\\xa0MLF Low195    mlflow.log_metric(\"eval_acc\", eval_acc)\\n    mlflow.log_metric(\"auc_score\", auc_score)\\n    print(\"eval_acc: \", eval_acc)\\n    print(\"auc_score: \", auc_score)',\n",
       " 'mlflow.log_metric(\"auc_score\", auc_score)\\n    print(\"eval_acc: \", eval_acc)\\n    print(\"auc_score: \", auc_score)\\n     mlflow.pytorch.log_model(model, \"PyTorch_MNIST\")\\nmlflow.end_run()\\nAs y ou can see, MLFlow integration is still quite easy with PyTorch. \\nRefer to Figure\\xa0 4-76  to see the output.\\nFigure 4-76.  Setting the experiment, and logging the parameters, \\nmetrics, and the model to the MLFlow runChapter 4  Introdu CtIon to\\xa0MLF Low196 MLFlow UI\\xa0– Checking Your Run\\nLet’s open up the UI.\\xa0Refer to Figure 4-77 .\\nAs you can see, there is a new experiment titled PyTorch_MNIST.\\xa0Click \\nit. You should now see the run you just completed. Refer to Figure\\xa0 4-78 .\\nNow that your run has shown up, click it. You should see all the \\nparameters and metrics logged in that run. Refer to Figure\\xa0 4-79 .',\n",
       " 'Now that your run has shown up, click it. You should see all the \\nparameters and metrics logged in that run. Refer to Figure\\xa0 4-79 .\\nFigure 4-77.  Looking at the MLFlow UI now. Notice that your \\nexperiment, PyTorch_MNIST, is created\\nFigure 4-78.  The MLFlow UI showing your completed runChapter 4  Introdu CtIon to\\xa0MLF Low197Also notice the model that’s been saved by MLFlow under artifacts. \\nRefer to Figure\\xa0 4-80 .\\nFigure 4-79.  All the p arameters, metrics, and artifacts (the model) \\nyou specified have been logged\\nFigure 4-80.  MLFlow has successfully logged the model as wellChapter 4  Introdu CtIon to\\xa0MLF Low198 Loading an\\xa0MLFlow Model\\nLet’s now go over how to load this model using MLFlow. Copy the run \\nID, and head back to the notebook. Run the following, but replace the',\n",
       " 'Let’s now go over how to load this model using MLFlow. Copy the run \\nID, and head back to the notebook. Run the following, but replace the \\nplaceholders with your run ID:\\nloaded_model = mlflow.pytorch.load_model(\"runs:/YOUR_RUN_ID/\\nPyTorch_MNIST\")\\nIn our case, our run ID was 094a9f92cd714711926114b4c96f6d73 , so \\nour code looks like Figure\\xa0 4-81 .\\nNow that’s done, so let’s make predictions and calculate the metrics \\nagain:\\npreds = loaded_model.forward(torch.Tensor(x_test.float()).\\ncuda())\\npreds = np.round(preds.detach().cpu().numpy())\\neval_acc = accuracy_score(y_test, preds)\\nauc_score = roc_auc_score(y_test, preds)\\nprint(\"eval_acc: \", eval_acc)\\nprint(\"auc_score: \", auc_score)\\nRefer to Figure\\xa0 4-82  to see the output.',\n",
       " 'eval_acc = accuracy_score(y_test, preds)\\nauc_score = roc_auc_score(y_test, preds)\\nprint(\"eval_acc: \", eval_acc)\\nprint(\"auc_score: \", auc_score)\\nRefer to Figure\\xa0 4-82  to see the output.\\nFigure 4-81.  Loading the logged MLFlow modelChapter 4  Introdu CtIon to\\xa0MLF Low199As you can see, these metrics are the same as from the training run. \\nYou now know how to load a PyTorch model using MLFlow and how you \\ncan use it to make predictions.\\nWith that, you now know how to integrate MLFlow into your PyTorch \\nexperiments. Next, we will look at how you can integrate MLFlow into \\nPySpark.\\n MLFlow with\\xa0PySpark\\nIn our final example, we will look at how MLFlow integrates with PySpark. \\nLike in the scikit-learn example, we will be looking at the application of a',\n",
       " 'PySpark.\\n MLFlow with\\xa0PySpark\\nIn our final example, we will look at how MLFlow integrates with PySpark. \\nLike in the scikit-learn example, we will be looking at the application of a \\nlogistic regression model to the credit card dataset. In fact, this code is very \\nsimilar to the PySpark example from Chapter 2.\\nBefore we begin, here are the versions of the modules we are using, \\nincluding CUDA and CuDNN:\\n• PySpark \\xa0– 2.4.5\\n• Matplotlib \\xa0– 3.2.1\\n• Sklearn \\xa0– 0.22.2.post1\\n• MLFlow \\xa0– 1.10.0\\n• mump y\\xa0– 1.18.5\\nFigure 4-82.  The output of calculating the evaluation metrics from \\nearlier but with the logged model. As you can see, the scores match \\nexactlyChapter 4  Introdu CtIon to\\xa0MLF Low200 Data Processing\\nWith that, let’s get started. First, you must import all the necessary modules',\n",
       " 'exactlyChapter 4  Introdu CtIon to\\xa0MLF Low200 Data Processing\\nWith that, let’s get started. First, you must import all the necessary modules \\nand set up some variables for Spark:\\nimport pyspark #\\nfrom pyspark.sql import SparkSession\\nfrom pyspark import SparkConf, SparkContext\\nfrom pyspark.sql.types import *\\nfrom pyspark.ml.feature import VectorAssembler\\nfrom pyspark.ml import Pipeline\\nfrom pyspark.ml.classification import LogisticRegression\\nimport pyspark.sql.functions as F\\nimport os\\nimport seaborn as sns\\nimport sklearn #\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import roc_auc_score, accuracy_score\\nimport matplotlib #\\nimport matplotlib.pyplot as plt\\nimport mlflow\\nimport mlflow.spark\\nos.environ[\"SPARK_LOCAL_IP\"]=\\'127.0.0.1\\'',\n",
       " 'from sklearn.metrics import roc_auc_score, accuracy_score\\nimport matplotlib #\\nimport matplotlib.pyplot as plt\\nimport mlflow\\nimport mlflow.spark\\nos.environ[\"SPARK_LOCAL_IP\"]=\\'127.0.0.1\\'\\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\\nspark.sparkContext._conf.getAll()\\nprint(\"pyspark: {}\".format(pyspark.__version__))\\nprint(\"matplotlib: {}\".format(matplotlib.__version__))\\nprint(\"seaborn: {}\".format(sns.__version__))\\nprint(\"sklearn: {}\".format(sklearn.__version__))\\nprint(\"mlflow: {}\".format(mlflow.__version__))Chapter 4  Introdu CtIon to\\xa0MLF Low201Refer to Figure\\xa0 4-83 .\\nNext, let’s load your data set and specify what columns you want to \\ntake:\\ndata_path = \\'data/creditcard.csv\\'\\ndf = spark.read.csv(data_path, header = True,  \\ninferSchema = True)\\nlabelColumn = \"Class\"',\n",
       " 'Next, let’s load your data set and specify what columns you want to \\ntake:\\ndata_path = \\'data/creditcard.csv\\'\\ndf = spark.read.csv(data_path, header = True,  \\ninferSchema = True)\\nlabelColumn = \"Class\"\\ncolumns = df.columns\\nnumericCols = columns\\nnumericCols.remove(\"Time\")\\nnumericCols.remove(labelColumn)\\nprint(numericCols)\\nFigure 4-83.  Importing the necessary modules and printing their \\nversionsChapter 4  Introdu CtIon to\\xa0MLF Low202Refer to Figure\\xa0 4-84  to see the output.\\nNotice that you dropped the column Time  here, like with the scikit-  \\nlearn example. This column just adds a lot of extraneous information that \\ndoesn’t actually correlate very much with the label column and could even \\npossibly make the learning task harder than it needs to be.\\nLet’s see what the data frame looks like:',\n",
       " 'doesn’t actually correlate very much with the label column and could even \\npossibly make the learning task harder than it needs to be.\\nLet’s see what the data frame looks like:\\ndf.toPandas().head()\\nRefer to Figure\\xa0 4-85  to see the output.\\nFigure 4-84.  Loading the data and specifying the columns that you \\nwant as a list\\nFigure 4-85.  Converting the Spark data frame to Pandas and \\nchecking the output. As you can see, the columns have loaded in \\ncorrectly, along with the data. The column Time has not been \\ndropped because you did not filter the data frame yetChapter 4  Introdu CtIon to\\xa0MLF Low203You’ll notice that the columns you “dropped” are still showing up, like \\nTime . You haven’t filtered the columns you want yet, which you are going',\n",
       " 'Time . You haven’t filtered the columns you want yet, which you are going \\nto do now. Run the following to select the features you want from the data \\nframe and create your normal and anomaly splits:\\nstages = []\\nassemblerInputs =   numericCols\\nassembler = VectorAssembler(inputCols=assemblerInputs, \\noutputCol=\"features\")\\nstages += [assembler]\\ndfFeatures = df.select(F.col(labelColumn).alias(\\'label\\'), \\n*numericCols )\\nnormal = dfFeatures.filter(\"Class == 0\").\\nsample(withReplacement= False, fraction=0.5, seed=2020)\\nanomaly = dfFeatures.filter(\"Class == 1\")\\nnormal_train, normal_test = normal.randomSplit([0.8, 0.2],  \\nseed = 2020)\\nanomaly_train, anomaly_test = anomaly.randomSplit([0.8, 0.2], \\nseed = 2020)\\nRefer to Figure\\xa0 4-86  to see the code in a cell.',\n",
       " 'normal_train, normal_test = normal.randomSplit([0.8, 0.2],  \\nseed = 2020)\\nanomaly_train, anomaly_test = anomaly.randomSplit([0.8, 0.2], \\nseed = 2020)\\nRefer to Figure\\xa0 4-86  to see the code in a cell.\\nFigure 4-86.  Selecting the columns that you want and defining your \\nnormal and anomaly train and test setsChapter 4  Introdu CtIon to\\xa0MLF Low204Let’s look at the new data frame now:\\ndfFeatures.toPandas().head()\\nRefer to Figure\\xa0 4-87 .\\nNotice that the columns you dropped are gone. Now you know that \\nnormal and anomaly don’t have the features you dropped either and that \\neverything is proceeding as planned. Let’s create the train and test sets:\\ntrain_set = normal_train.union(anomaly_train)\\ntest_set = normal_test.union(anomaly_test)\\nRefer to Figure\\xa0 4-88 .',\n",
       " 'everything is proceeding as planned. Let’s create the train and test sets:\\ntrain_set = normal_train.union(anomaly_train)\\ntest_set = normal_test.union(anomaly_test)\\nRefer to Figure\\xa0 4-88 .\\nLet’s now move on to creating the feature vector that the logistic \\nregression model is going to use. Run the following to define the pipeline \\nand create your final train and test sets:\\npipeline = Pipeline(stages = stages)\\npipelineModel = pipeline.fit(dfFeatures)\\ntrain_set = pipelineModel.transform(train_set)\\nFigure 4-87.  As y ou can see, Time has been dropped. This is the data \\nframe that your training and testing sets are derived from\\nFigure 4-88.  Concatenating the normal and anomaly sets to create',\n",
       " 'Figure 4-87.  As y ou can see, Time has been dropped. This is the data \\nframe that your training and testing sets are derived from\\nFigure 4-88.  Concatenating the normal and anomaly sets to create \\nthe train and test setsChapter 4  Introdu CtIon to\\xa0MLF Low205test_set = pipelineModel.transform(test_set)\\nselectedCols = [\\'label\\', \\'features\\'] + numericCols\\ntrain_set = train_set.select(selectedCols)\\ntest_set = test_set.select(selectedCols)\\nprint(\"Training Dataset Count: \", train_set.count())\\nprint(\"Test Dataset Count: \", test_set.count())\\nRefer to Figure\\xa0 4-89 .\\nNow that you’ve finished processing the data, let’s define a function to \\ntrain the model and calculate some relevant metrics:\\ndef train(spark_model, train_set):\\n    trained_model = spark_model.fit(train_set)',\n",
       " 'train the model and calculate some relevant metrics:\\ndef train(spark_model, train_set):\\n    trained_model = spark_model.fit(train_set)\\n    trainingSummary = trained_model.summary\\n    pyspark_auc_score = trainingSummary.areaUnderROC\\n    mlflow.log_metric(\"train_acc\", trainingSummary.accuracy)\\n    mlflow.log_metric(\"train_AUC\", pyspark_auc_score)\\n    print(\"Training Accuracy: \", trainingSummary.accuracy)\\n    print(\"Training AUC:\", pyspark_auc_score)\\n    return trained_model\\nFigure 4-89.  Defining the pipeline used to create the feature vector \\nthat will be used to train the model. From the feature vector and the \\nlabel vector, you define your final train and test setsChapter 4  Introdu CtIon to\\xa0MLF Low206Refer to Figure\\xa0 4-90  to see the function in a cell.',\n",
       " 'label vector, you define your final train and test setsChapter 4  Introdu CtIon to\\xa0MLF Low206Refer to Figure\\xa0 4-90  to see the function in a cell.\\nLet’s now define a function to evaluate the model and calculate those \\nmetrics, too:\\ndef evaluate(spark_model, test_set):\\n    evaluation_summary = spark_model.evaluate(test_set)\\n    eval_acc = evaluation_summary.accuracy\\n    eval_AUC = evaluation_summary.areaUnderROC\\n    mlflow.log_metric(\"eval_acc\", eval_acc)\\n    mlflow.log_metric(\"eval_AUC\", eval_AUC)\\n    print(\"Evaluation Accuracy: \", eval_acc)\\n    print(\"Evaluation AUC: \", eval_AUC)\\nRefer to Figure\\xa0 4-91 .\\nFigure 4-90.  The code to train the PySpark logistic regression model',\n",
       " 'print(\"Evaluation Accuracy: \", eval_acc)\\n    print(\"Evaluation AUC: \", eval_AUC)\\nRefer to Figure\\xa0 4-91 .\\nFigure 4-90.  The code to train the PySpark logistic regression model \\nand log the training accuracy and AUC score metricsChapter 4  Introdu CtIon to\\xa0MLF Low207 MLFlow Run\\xa0– Training, UI, and\\xa0Loading an\\xa0MLFlow \\nModel\\nNow that you have finished defining the training and evaluation functions \\nalong with the metrics you want to log, it’s time to start an MLFlow run and \\nbuild a model:\\nlr = LogisticRegression(featuresCol = \\'features\\', labelCol = \\n\\'label\\', maxIter=10)\\nmlflow.set_experiment(\"PySpark_CreditCard\")\\nwith mlflow.start_run():\\n    trainedLR = train(lr, train_set)\\n    evaluate(trainedLR, test_set)\\n     mlflow.spark.log_model(trainedLR,  \\n\"creditcard_model_pyspark\")',\n",
       " 'with mlflow.start_run():\\n    trainedLR = train(lr, train_set)\\n    evaluate(trainedLR, test_set)\\n     mlflow.spark.log_model(trainedLR,  \\n\"creditcard_model_pyspark\")\\nmlflow.end_run()\\nFigure 4-91.  The code to evaluate the trained PySpark logistic \\nregression model and log the evaluation accuracy and AUC score \\nmetricsChapter 4  Introdu CtIon to\\xa0MLF Low208Refer to Figure\\xa0 4-92 .\\nAlright, now that MLFlow has finished logging everything and the \\nrun has ended, open up the MLFlow UI.\\xa0You should see something like \\nFigure\\xa0 4-93 .\\nNotice that a new experiment called PySpark_CreditCard  has been \\ncreated. Click it, and you should see something like Figure\\xa0 4-94 . If MLFlow \\ndidn’t log the run here, try rerunning the cell. It should log it correctly.',\n",
       " 'created. Click it, and you should see something like Figure\\xa0 4-94 . If MLFlow \\ndidn’t log the run here, try rerunning the cell. It should log it correctly.\\nFigure 4-92.  The output of the MLFlow run. The experiment has \\nbeen created and the metrics and model successfully logged\\nFigure 4-93.  The MLFlow UI showing that your experiment, \\nPySpark_CreditCard, has been createdChapter 4  Introdu CtIon to\\xa0MLF Low209If everything went well, you should see a run logged in this experiment. \\nClick it, and you should see something like Figure\\xa0 4-95 .\\nFinally, in the artifacts section, click the folder that says  \\ncreditcard_model_pyspark  to expand it. You should see a folder called \\nsparkml  that contains the PySpark logistic regression model. Refer to \\nFigure\\xa0 4-96 .',\n",
       " 'creditcard_model_pyspark  to expand it. You should see a folder called \\nsparkml  that contains the PySpark logistic regression model. Refer to \\nFigure\\xa0 4-96 .\\nFigure 4-94.  MLFlow UI showing that your run has successfully \\nfinished\\nFigure 4-95.  Looking at the run, it appears that all of your metrics \\nhave successfully been loggedChapter 4  Introdu CtIon to\\xa0MLF Low210Now that you’ve verified MLFlow has logged everything you specified, \\ncopy the run number at the top. Now go back to the notebook and run the \\nfollowing, replacing the placeholder with your run:\\nmodel = mlflow.spark.load_model(\"runs:/YOUR_RUN_ID/ \\ncreditcard_model_pyspark\")\\nIn our case, our run was 58e6aac5d43948c6948bee29c0c04cca , so our \\ncell looks like Figure\\xa0 4-97 .',\n",
       " 'model = mlflow.spark.load_model(\"runs:/YOUR_RUN_ID/ \\ncreditcard_model_pyspark\")\\nIn our case, our run was 58e6aac5d43948c6948bee29c0c04cca , so our \\ncell looks like Figure\\xa0 4-97 .\\nNow that the model has been loaded, let’s make some predictions with \\nit. Run the following:\\npredictions = model.transform(test_set)\\ny_true = predictions.select([\\'label\\']).collect()\\ny_pred = predictions.select([\\'prediction\\']).collect()\\nRefer to Figure\\xa0 4-98  to see the code in a cell.\\nFigure 4-96.  MLFlow has also logged the PySpark model. There is \\nno concrete model file like with the TensorFlow or PyTorch examples \\nbecause of the way PySpark stores its models\\nFigure 4-97.  Loading the logged MLFlow modelChapter 4  Introdu CtIon to\\xa0MLF Low211Let’s print out the evaluation accuracy and the AUC score:',\n",
       " 'because of the way PySpark stores its models\\nFigure 4-97.  Loading the logged MLFlow modelChapter 4  Introdu CtIon to\\xa0MLF Low211Let’s print out the evaluation accuracy and the AUC score:\\nprint(f\"AUC Score: {roc_auc_score(y_true, y_pred):.3%}\")\\nprint(f\"Accuracy Score: {accuracy_score(y_true, y_pred):.3%}\")\\nRefer to Figure\\xa0 4-99 .\\nYou will notice that the AUC score differs compared to what was \\ncalculated in the evaluation function. This is likely because PySpark \\ncalculates the ROC curve slightly differently because it has direct access \\nto the model itself. On the other hand, with scikit-learn, you only have the \\ntrue labels and the predictions to work with, so the ROC curve is calculated \\nslightly differently.\\nFinally, let’s construct the confusion matrix:',\n",
       " \"true labels and the predictions to work with, so the ROC curve is calculated \\nslightly differently.\\nFinally, let’s construct the confusion matrix:\\nconf_matrix = confusion_matrix(y_true, y_pred)\\nax = sns.heatmap(conf_matrix, annot= True,fmt='g')\\nax.invert_xaxis()\\nax.invert_yaxis()\\nplt.ylabel('Actual')\\nplt.xlabel('Predicted')\\nFigure 4-98.  Making predictions with your loaded model\\nFigure 4-99.  Printing out the evaluation metrics. The AUC score \\nnoticeably differs, but the accuracy score matches what was displayed \\nduring the MLFlow runChapter 4  Introdu CtIon to\\xa0MLF Low212Refer to Figure\\xa0 4-100 .\\nFrom the confusion matrix, you can see that the AUC score as \\ncalculated by PySpark must be reflecting its performance on how well\",\n",
       " 'From the confusion matrix, you can see that the AUC score as \\ncalculated by PySpark must be reflecting its performance on how well \\nit classifies normal data. Looking at the anomalies, a fair chunk of the \\nfraudulent data has been misclassified. Roughly speaking, the model only \\ngot two-thirds of the anomalies when evaluated on the test data. Perhaps \\nthis explains the disparity between what scikit-Learn says is the AUC score \\nand what PySpark says is the AUC score. Both must have calculated the \\nROC curves slightly differently with PySpark’s graph somehow favoring the \\nexcellent true positive rate of the normal data’s classification.\\nWith that, you now know how to integrate MLFlow into your PySpark \\nexperiments.\\nNext, we will take a look at how you can deploy your models locally',\n",
       " 'With that, you now know how to integrate MLFlow into your PySpark \\nexperiments.\\nNext, we will take a look at how you can deploy your models locally \\nand how you can query the models with samples of data and receive \\npredictions.\\nFigure 4-100.  Displaying the confusion matrix using the true values \\nand the predictions made by the model you loadedChapter 4  Introdu CtIon to\\xa0MLF Low213 Local Model Serving\\n Deploying the\\xa0Model\\nServing and querying models locally is very easy and can be done in the \\ncommand line. You only need the experiment ID and the run ID to serve \\nthe model. This is where the print statement from earlier can apply, as it \\nprints the run ID of that specific run. If you just want to serve the latest \\nmodel, you may do so using that ID.',\n",
       " 'the model. This is where the print statement from earlier can apply, as it \\nprints the run ID of that specific run. If you just want to serve the latest \\nmodel, you may do so using that ID.\\nOtherwise, you can look in the MLFlow UI, select a model run that \\nsuits your needs, and paste the run this way.\\nBefore you begin, go to the MLFlow UI once again, and click the \\nexperiment scikit_learn_experiment . Pick a run and copy the run \\nID.\\xa0Don’t forget the model name that you logged the model with either, \\nwhich should be log_reg_model .\\nYou may create a new notebook at this point to keep the code more \\norganized, but be sure to import the following:\\nimport pandas as pd\\nimport mlflow\\nimport mlflow.sklearn\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt',\n",
       " 'organized, but be sure to import the following:\\nimport pandas as pd\\nimport mlflow\\nimport mlflow.sklearn\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import roc_auc_score, accuracy_score, \\nconfusion_matrixChapter 4  Introdu CtIon to\\xa0MLF Low214import numpy as np\\nimport subprocess\\nimport json\\nYou’ll notice that you are now importing subprocess. If you’re using the \\nsame notebook, make sure to import this module as well.\\nRefer to Figure\\xa0 4-101  to see this code in a cell.\\nNow, open up your command prompt/terminal so that you can begin \\nto serve your local model. First, you need to change your directory to one',\n",
       " 'Refer to Figure\\xa0 4-101  to see this code in a cell.\\nNow, open up your command prompt/terminal so that you can begin \\nto serve your local model. First, you need to change your directory to one \\nthat contains the mlruns  folder with all your experiments. Next, you need \\ntwo things: your model run  and your model name .\\nAgain, your model run can be anything you pick from the MLFlow UI \\nor it can simply be the latest run. The model name is whatever you set it to \\nwhen logging the model. In this case, it will be log_reg_model .\\nOnce you have that, run the following command in your command \\nprompt/terminal. We have generalized the command, so be sure to replace \\nthe fields with your model run and model name, respectively:\\nmlflow models serve --model-uri runs:/YOUR_MODEL_RUN/',\n",
       " 'prompt/terminal. We have generalized the command, so be sure to replace \\nthe fields with your model run and model name, respectively:\\nmlflow models serve --model-uri runs:/YOUR_MODEL_RUN/ \\nYOUR_MODEL_NAME -p 1235\\nFigure 4-101.  Importing the necessary modulesChapter 4  Introdu CtIon to\\xa0MLF Low215In our case, our model run was 3862eb3bd89b43e8ace610c521d974e6 , \\nand our model name was once again log_reg_model . And so, the \\ncommand we ran looks like Figure\\xa0 4-102 .\\nIn text, the command looks like this:\\nmlflow models serve --model-uri runs:/3862eb3bd89b43e8ace610c52\\n1d974e6/log_reg_model -p 1235\\nMLFlow should start constructing a new conda environment right \\naway that it will use to serve locally. In this environment, it installs basic',\n",
       " '1d974e6/log_reg_model -p 1235\\nMLFlow should start constructing a new conda environment right \\naway that it will use to serve locally. In this environment, it installs basic \\npackages and specific packages that the model needs to be able to run.\\nAfter some time, you should see something like in Figure\\xa0 4-103 .\\nFigure 4-102.  The command that we ran to serve our model locally\\nFigure 4-103.  The result of running the command to deploy \\nthe model locally. You might see something different, such as \\nlocalhost:1235, but this is because we have docker installedChapter 4  Introdu CtIon to\\xa0MLF Low216MLFlow should create a new conda environment before hosting the \\nmodel on your local server. The port option -p lets you set a specific port to',\n",
       " 'model on your local server. The port option -p lets you set a specific port to \\nhost the model on. We selected a specific port so that we can have MLFlow \\nUI running at the same time, as both of them default to port 5000. In our \\ncase, our MLFlow UI is running on port 1234, so we are serving the model \\non port 1235.\\n Querying the\\xa0Model\\nYou are now ready to query the model with data and receive predictions. \\nThis is where the subprocess module comes in, and you’ll see why shortly. \\nFirst, let’s load up your data frame again. Run the following code:\\ndf = pd.read_csv(\"data/creditcard.csv\")\\nYou should see something like Figure\\xa0 4-104 .\\nNext, select 80 values from your data frame to query your model with. \\nRun the following code:\\ninput_json = df.iloc[:80].drop([\"Time\", \"Class\"],',\n",
       " 'You should see something like Figure\\xa0 4-104 .\\nNext, select 80 values from your data frame to query your model with. \\nRun the following code:\\ninput_json = df.iloc[:80].drop([\"Time\", \"Class\"],  \\naxis=1).to_json(orient=\"split\")\\nYou should see something like Figure\\xa0 4-105 .\\nFigure 4-105.  Converting a selection of 80 rows, dropping the Time \\nand Class columns since they were dropped in the original x_train \\nused to train the model, to a JSON with a split orient\\nFigure 4-104.  Loading the credit card datasetChapter 4  Introdu CtIon to\\xa0MLF Low217The next step is important because of how you preprocessed the data \\nbefore training your model originally. To show why it’s so important, we \\nwill quickly demonstrate the difference in evaluation metrics from passing',\n",
       " 'before training your model originally. To show why it’s so important, we \\nwill quickly demonstrate the difference in evaluation metrics from passing \\nin non-scaled data and scaled data. First of all, here is the code to send \\ndata to the model and receive predictions back:\\nproc = subprocess.run([\"curl\",   \"-X\", \"POST\", \"-H\",  \\n\"Content-  Type:application/json; format=pandas-split\",  \\n\"--data\", input_json, \"http://127.0.0.1:1235/invocations\"], \\nstdout=subprocess.PIPE, encoding=\\'utf-8\\')\\noutput = proc.stdout\\ndf2 = pd.DataFrame([json.loads(output)])\\ndf2\\nEssentially, what this does is run the following command within \\nPython itself:\\ncurl -X POST -H \"Content-Type:application/json;  \\nformat=pandas-  split\" –data \"CONTENT_OF_INPUT_JSON\"    \\n\"http://127.0.0.1:1235/invocations\"',\n",
       " 'Python itself:\\ncurl -X POST -H \"Content-Type:application/json;  \\nformat=pandas-  split\" –data \"CONTENT_OF_INPUT_JSON\"    \\n\"http://127.0.0.1:1235/invocations\"\\nThe core of the problem is that if you are running this in command \\nline, pasting the JSON format data of the data frame can get very messy \\nbecause there’s so many columns. That is why we chose to use subprocess \\nas it is easier to directly pass in the JSON itself using a variable name, \\ninput_json  in this case, to hold the contents of the JSON.\\nYou should see something like Figure\\xa0 4-106 .\\nNow, you will query the model with input data that is not scaled.\\nFigure 4-106.  Sending data to the locally hosted model and receiving \\npredictions from the modelChapter 4  Introdu CtIon to\\xa0MLF Low218 Querying Without Scaling',\n",
       " 'Figure 4-106.  Sending data to the locally hosted model and receiving \\npredictions from the modelChapter 4  Introdu CtIon to\\xa0MLF Low218 Querying Without Scaling\\nYou will keep the selection of 80 values from earlier and query the model. \\nThe model accepts data in the JSON format, so you will have to convert \\nthe format of your data before sending it to the model. Run the cell in \\nFigure\\xa0 4- 106.\\nYou should see something like Figure\\xa0 4-107 .\\nThe resulting data frame is what you get by converting the predictions \\nthat you got back from the model into a data frame. Since you have the true \\npredictions, let’s calculate an AUC score and an accuracy score to see how \\nthe model did. Run the following code:\\ny_true = df.iloc[:80].Class\\ndf2 = df2.T\\neval_acc = accuracy_score(y_true, df2)',\n",
       " 'predictions, let’s calculate an AUC score and an accuracy score to see how \\nthe model did. Run the following code:\\ny_true = df.iloc[:80].Class\\ndf2 = df2.T\\neval_acc = accuracy_score(y_true, df2)\\ny_true.iloc[-1] = 1\\neval_auc = roc_auc_score(y_true, df2)\\nprint(\"Eval Acc\", eval_acc)\\nprint(\"Eval AUC\", eval_auc)\\nFigure 4-107.  The list of predictions that you get after querying the \\nmodel with input_json. Notice that it’s predicting a lot of anomalies. \\nThis is the first red flag that indicates something’s wrongChapter 4  Introdu CtIon to\\xa0MLF Low219First of all, you had to transpose df2 using .T so that you can get the \\npredictions to be in a Pandas Series format. Next, the AUC score cannot \\nbe calculated if one of the arrays y_true  or y_preds  only have one class.',\n",
       " 'predictions to be in a Pandas Series format. Next, the AUC score cannot \\nbe calculated if one of the arrays y_true  or y_preds  only have one class. \\nIn this case, y_true  is only comprised of normal values, so you had to \\nmanipulate the last value and make it 1 when it really isn’t just to get an \\nAUC score. Of course, the resulting AUC score will be nonsense.\\nYou should see something like Figure\\xa0 4-108 .\\nAs you can see, the accuracy score is horrible. This basically means \\nthat the model doesn’t know the difference between the anomalies and the \\nnormal points but seems to have some idea about normal points.\\nThe reason the model did so poorly despite doing so well during the \\ntraining process is that the input data has not been scaled. You will see',\n",
       " 'The reason the model did so poorly despite doing so well during the \\ntraining process is that the input data has not been scaled. You will see \\nthe difference in model performance when you now scale the data before \\npassing it in.\\nFigure 4-108.  Evaluating the accuracy and the AUC score from the \\npredictions. The AUC score is nonsense, but the accuracy score reveals \\nthat the model has performed very poorlyChapter 4  Introdu CtIon to\\xa0MLF Low220 Querying with\\xa0Scaling\\nYou will take the same split of data except you will now scale it before \\npassing it in. Run the following code to recreate the data that you used to fit \\nthe scaler when training the model originally:\\nnormal = df[df.Class == 0].sample(frac=0.5, random_state=2020).\\nreset_index(drop= True)\\nanomaly = df[df.Class == 1]',\n",
       " 'the scaler when training the model originally:\\nnormal = df[df.Class == 0].sample(frac=0.5, random_state=2020).\\nreset_index(drop= True)\\nanomaly = df[df.Class == 1]\\nnormal_train, normal_test = train_test_split(normal,  \\ntest_size = 0.2, random_state = 2020)\\nanomaly_train, anomaly_test = train_test_split \\n(anomaly, test_size = 0.2, random_state = 2020)\\nscaler = StandardScaler()\\nscaler.fit(pd.concat((normal, anomaly)).drop([\"Time\", \"Class\"], \\naxis=1))\\nYou should see something like Figure\\xa0 4-109 .\\nNow that you have fit the scaler, let’s transform your data selection:\\nscaled_selection = scaler.transform(df.iloc[:80].drop \\n([\"Time\", \"Class\"], axis=1))\\ninput_json = pd.DataFrame \\n(scaled_selection).to_json(orient=\"split\")\\nFigure 4-109.  Recreating the original dataset that you used to fit the',\n",
       " '([\"Time\", \"Class\"], axis=1))\\ninput_json = pd.DataFrame \\n(scaled_selection).to_json(orient=\"split\")\\nFigure 4-109.  Recreating the original dataset that you used to fit the \\nstandard scaler when processing the data originally. Using this, you \\nwill transform your new sample of data and pass it into the modelChapter 4  Introdu CtIon to\\xa0MLF Low221Refer to Figure\\xa0 4-110 .\\nNow run the following:\\nproc =  subprocess.run([\"curl\",   \"-X\", \"POST\", \"-H\",  \\n\"Content-  Type:application/json; format=pandas-split\",\\n        \"--data\", input_json, \"http://127.0.0.1:1235/invocations\"],\\n       stdout=subprocess.PIPE, encoding=\\'utf-8\\')\\noutput = proc.stdout\\npreds = pd.DataFrame([json.loads(output)])\\npreds\\nYou should see something like Figure\\xa0 4-111 .',\n",
       " \"stdout=subprocess.PIPE, encoding='utf-8')\\noutput = proc.stdout\\npreds = pd.DataFrame([json.loads(output)])\\npreds\\nYou should see something like Figure\\xa0 4-111 .\\nOne thing to note is that you are scaling it on the combination of all \\nnormal data and all anomaly data, as you did when you were creating \\nthe train, test, and validation splits. Since the model was trained on data \\nthat was scaled on the partition of data you used in the training process \\nFigure 4-110.  Scaling the selection of 80 values from the original \\ndata frame and converting it into a JSON format to be sent to the \\nmodel\\nFigure 4-111.  Querying the model with the scaled values. From a\",\n",
       " 'data frame and converting it into a JSON format to be sent to the \\nmodel\\nFigure 4-111.  Querying the model with the scaled values. From a \\nfirst glance, the predictions appear to be correct this time aroundChapter 4  Introdu CtIon to\\xa0MLF Low222(the training, testing, and validation data together), passing in data scaled \\ndifferently won’t result in the correct predictions. When you scale the new \\ndata, it must be scaled after fitting it on the training set.\\nOne problem that may eventually arise is that new data might have \\na different distribution than the original training data. This could lead to \\nperformance issues with the model, but really that’s a sign that you need to \\ntrain your model to update it on the new data.\\nLet’s check how your model did now:\\ny_true = df.iloc[:80].Class',\n",
       " 'performance issues with the model, but really that’s a sign that you need to \\ntrain your model to update it on the new data.\\nLet’s check how your model did now:\\ny_true = df.iloc[:80].Class\\npreds = preds.T\\neval_acc = accuracy_score(y_true, preds)\\ny_true.iloc[-1] = 1\\neval_auc = roc_auc_score(y_true, preds)\\nprint(\"Eval Acc\", eval_acc)\\nprint(\"Eval AUC\", eval_auc)\\nRefer to Figure\\xa0 4-112 .\\nAs you can see, the accuracy score is noticeably higher, and the \\nmodel’s performance is reminiscent of when it was trained and evaluated. \\nUnfortunately, the AUC score isn’t a very accurate reflection of the model’s \\nFigure 4-112.  Checking the accuracy and the AUC scores of the \\npredictions. The accuracy score is far better, but you will need more',\n",
       " 'Figure 4-112.  Checking the accuracy and the AUC scores of the \\npredictions. The accuracy score is far better, but you will need more \\nprediction data with both normal and anomaly values to be able to \\nget AUC scoresChapter 4  Introdu CtIon to\\xa0MLF Low223performance since the samples you are querying the model with only have \\nnormal data.\\nLet’s see how the model performs when you query it with a larger \\nsample of data.\\n Batch Querying\\nUnfortunately, there is a limit to how many data samples you can ask \\nthe model to make predictions on. The number 80 is really close to the \\nmaximum number of samples you can send at one time. So how do you get \\naround this issue and make predictions on more than just 80 samples? For \\none, you can try batching the samples and making predictions one batch at',\n",
       " 'around this issue and make predictions on more than just 80 samples? For \\none, you can try batching the samples and making predictions one batch at \\na time.\\nRun the following code:\\ntest = df.iloc[:8000]\\ntrue = test.Class\\ntest = scaler.transform(test.drop([\"Time\", \"Class\"], axis=1))\\npreds = []\\nbatch_size = 80\\nfor f in range(100):\\n     sample = pd.DataFrame(test[f*batch_size:(f+1)*batch_size]).\\nto_json(orient=\"split\")\\n    proc = subprocess.run([\"curl\",   \"-X\", \"POST\", \"-H\",\\n                           \"Content-Type:application/json; \\nformat=pandas-split\", \"--data\",\\n                           sample, \"http://127.0.0.1:1235/\\ninvocations\"],\\n                           stdout=subprocess.PIPE, \\nencoding=\\'utf-8\\')Chapter 4  Introdu CtIon to\\xa0MLF Low224    output = proc.stdout',\n",
       " 'invocations\"],\\n                           stdout=subprocess.PIPE, \\nencoding=\\'utf-8\\')Chapter 4  Introdu CtIon to\\xa0MLF Low224    output = proc.stdout\\n    resp = pd.DataFrame([json.loads(output)])\\n    preds = np.concatenate((preds, resp.values[0]))\\neval_acc = accuracy_score(true, preds)\\neval_auc = roc_auc_score(true, preds)\\nprint(\"Eval Acc\", eval_acc)\\nprint(\"Eval AUC\", eval_auc)\\nHere, you are selecting the first 8,000 samples from the data frame. \\nSince the batch size is 80, you have 100 batches that you are passing to the \\nmodel. Of course, you must scale this data as well before passing it in. You \\nwill scale it in a manner similar to how you did it earlier: you will fit the \\nscaler on the same normal and anomaly data that you used in the model',\n",
       " 'will scale it in a manner similar to how you did it earlier: you will fit the \\nscaler on the same normal and anomaly data that you used in the model \\ntraining pipeline samples to transform the values you want to send to the \\nmodel. Once finished, you should see something like Figure\\xa0 4-113 . This \\nmight take several seconds to finish, so sit tight!\\nFigure 4-113.  The results of querying the model with the first 8,000 \\nsamples in the data frame. Notice that the AUC score is far better \\nsamplesChapter 4  Introdu CtIon to\\xa0MLF Low225This time, you don’t have to worry about only having one class in \\nthe entire data. This is because there are examples of anomalies in this \\nselection of 8,000 data points, so the true labels and predictions should \\ncontain samples of both classes.',\n",
       " \"the entire data. This is because there are examples of anomalies in this \\nselection of 8,000 data points, so the true labels and predictions should \\ncontain samples of both classes.\\nYou can see that the model performs quite well on this data, which \\nincludes data that the model has never seen before. Although you did \\nend up using all of the anomalies when training the data, the model still \\nperforms well on the normal data, as evidenced by the relatively high AUC \\nscore.\\nIn fact, let’s plot a confusion matrix to see how the model did and \\nwhat’s bringing down the AUC score. Run the following code:\\nconf_matrix = confusion_matrix(true, preds)\\nax = sns.heatmap(conf_matrix, annot= True,fmt='g')\\nax.invert_xaxis()\\nax.invert_yaxis()\\nplt.ylabel('Actual')\\nplt.xlabel('Predicted')\",\n",
       " 'conf_matrix = confusion_matrix(true, preds)\\nax = sns.heatmap(conf_matrix, annot= True,fmt=\\'g\\')\\nax.invert_xaxis()\\nax.invert_yaxis()\\nplt.ylabel(\\'Actual\\')\\nplt.xlabel(\\'Predicted\\')\\nplt.title(\"Confusion Matrix\")\\nRefer to Figure\\xa0 4-114  to see the output.Chapter 4  Introdu CtIon to\\xa0MLF Low226\\nFigure 4-114.  The confusion matrix for the predictions and true \\nvalues. The model performed excellently and was able to classify every \\nnormal point correctly and a majority of the anomaly points correctly \\nsamples\\nAs you can see, the confusion matrix shows that the model has \\nperformed very well on this data. Not only did it classify the normal points \\nperfectly, but it even classified most of the anomaly points correctly as \\nwell.\\nWith that, you hopefully know more about the process of deploying',\n",
       " 'perfectly, but it even classified most of the anomaly points correctly as \\nwell.\\nWith that, you hopefully know more about the process of deploying \\nand querying a model. When you deploy to a cloud platform, the querying \\nprocess follows a similar path where you must deploy a model on the cloud \\nplatform and query it by sending in the data in a JSON format.\\n Summary\\nMLFlow is an API that can help you integrate MLOps principles into your \\nexisting code base, supporting a wide variety of popular frameworks. \\nIn this chapter, we covered how you can use MLFlow to log metrics, \\nparameters, graphs, and the models themselves. Additionally, you learned \\nhow to load the logged model and make use of its functionality. As for',\n",
       " 'parameters, graphs, and the models themselves. Additionally, you learned \\nhow to load the logged model and make use of its functionality. As for \\nframeworks, we covered how you can apply MLFlow to your experiments Chapter 4  Introdu CtIon to\\xa0MLF Low227in scikit-learn, TensorFlow 2.0/Keras, PyTorch, and PySpark, and we also \\nlooked at how you can take one of these models, deploy it locally, and \\nmake predictions with your model.\\nIn the next chapter, we will look at how you can take your MLFlow \\nmodels and use MLFlow functionality to help deploy them to Amazon \\nSageMaker. Furthermore, we will also look at how you can make \\npredictions using your deployed model.Chapter 4  Introdu CtIon to\\xa0MLF Low229© Sridhar Alla, Suman Kalyan Adari 2021',\n",
       " 'SageMaker. Furthermore, we will also look at how you can make \\npredictions using your deployed model.Chapter 4  Introdu CtIon to\\xa0MLF Low229© Sridhar Alla, Suman Kalyan Adari 2021 \\nS. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \\nhttps://doi.org/10.1007/978-1-4842-6549-9_5CHAPTER 5\\nDeploying in\\xa0AWS\\nIn this chapter, we will cover how you can operationalize your MLFlow \\nmodels using AWS SageMaker. We will cover how you can upload your \\nruns to S3 storage, how you can build and push an MLFlow Docker \\ncontainer image to AWS, and how you can deploy your model, query it, \\nupdate the model once it is deployed, and remove a deployed model.\\n Introduction\\nIn the previous chapter, you learned what MLFlow is and how you can',\n",
       " 'update the model once it is deployed, and remove a deployed model.\\n Introduction\\nIn the previous chapter, you learned what MLFlow is and how you can \\nutilize the functionality it provides to integrate MLOps principles into \\nyour code. You also looked at how to deploy a model to a local server and \\nperform model inference. However, now it’s time to move to the next stage \\nand explore how you can deploy your machine learning models to a cloud \\nplatform so that multiple entities can use its prediction services.\\nBefore you begin, here are some important prerequisites :\\n• You must have the AWS command line interface (CLI) \\ninstalled and have your credentials configured.\\n – Once your credentials are verified, the AWS CLI lets \\nyou connect to your AWS workspace. From here,',\n",
       " 'installed and have your credentials configured.\\n – Once your credentials are verified, the AWS CLI lets \\nyou connect to your AWS workspace. From here, \\nyou can create new buckets, check your SageMaker \\nendpoints, and so on all through the command \\nline.230• You must have an Identity and Access Management \\n(IAM) execution role defined that grants SageMaker \\naccess to your S3 buckets. Refer to Figure\\xa0 5-8 to see \\nmore on this.\\n• You must have Docker installed and working properly. \\nVerify that you can build Docker images.\\n – It is essential to have Docker working on your \\nsystem because without it, MLFlow cannot build \\nthe Docker container image to push to the AWS \\nECR.\\nWe also recommend that you learn about AWS in general and how it',\n",
       " 'system because without it, MLFlow cannot build \\nthe Docker container image to push to the AWS \\nECR.\\nWe also recommend that you learn about AWS in general and how it \\nworks. Having background knowledge of AWS and how it works can help \\nyou understand this chapter and allow you to fix any issues much more \\neasily.\\nIn detail, we will go over the following in this chapter:\\n• Config uring AWS:  Here, you set up a bucket and push \\nyour mlruns  folders here to be stored on the cloud. \\nThese folders contain information about all of the runs \\nassociated with the experiments along with the logged \\nmodels themselves. Next, you build a special Docker \\ncontainer as defined by MLFlow and push that to AWS \\nECR.\\xa0SageMaker uses this container image to serve the \\nMLFlow model.',\n",
       " 'models themselves. Next, you build a special Docker \\ncontainer as defined by MLFlow and push that to AWS \\nECR.\\xa0SageMaker uses this container image to serve the \\nMLFlow model.\\n• Deploying a model to AWS SageMaker:  Here, you \\nuse the built-in MLFlow SageMaker module code to \\npush a model to SageMaker. After SageMaker creates \\nan endpoint, the model is hosted on here utilizing the \\ndocker image that you pushed earlier to the ECR.Chapter 5  Deploying in\\xa0 aWS231• Making predictions:  Once the model has finished \\ndeployment and is ready to serve, you use Boto3 to \\nquery the model and receive predictions.\\n• Switching models:  MLFlow provides functionality that \\nenables you to switch out a deployed model with a new \\none. SageMaker essentially updates the endpoint with',\n",
       " '• Switching models:  MLFlow provides functionality that \\nenables you to switch out a deployed model with a new \\none. SageMaker essentially updates the endpoint with \\nthe new model you are trying to deploy.\\n• Removing the deployed model:  Finally, MLFlow lets \\nyou remove your deployed model altogether and delete \\nthe endpoint. This is important to do so that you don’t \\nincur the charges of leaving an endpoint running.\\nAlso, it is important to note that AWS is actively being worked on, and \\nfunctionality and operating procedures can change! What that means is \\nthat something that works now may not work later on.\\nHowever, MLFlow specifically provides support for SageMaker, so if \\nsomething fundamental to how SageMaker runs changes in the future,',\n",
       " 'that something that works now may not work later on.\\nHowever, MLFlow specifically provides support for SageMaker, so if \\nsomething fundamental to how SageMaker runs changes in the future, \\nMLFlow is likely to account for it in the next build.\\nIn the absolute worst-case scenario where that doesn’t happen, \\nyou can still run an MLFlow server and host it on AWS.\\xa0You will still be \\nable to deploy models and make inferences with them, and the overall \\nfunctionality is still preserved. Instead of SageMaker directly hosting the \\nmodel using an MLFlow container image, you would do something similar \\nto the local model deployment experiment we did in Chapter 4, except \\nyou would connect to the server IP and port that the MLFlow server is \\nhosted on.',\n",
       " 'to the local model deployment experiment we did in Chapter 4, except \\nyou would connect to the server IP and port that the MLFlow server is \\nhosted on.\\nWe will explore how to do this with Google Cloud, as MLFlow does not \\nsupport Google Cloud like it does SageMaker and Azure.\\nWith that, let’s get started!Chapter 5  Deploying in\\xa0 aWS232 Configuring AWS\\nBefore you can actually push any model to SageMaker, you need to set up \\nyour Amazon workspace. You can push models from your local mlruns  \\ndirectory, similar to how you did local model deployment, but it is much \\nmore convenient and centralized to have all your runs be pushed to AWS \\nand stored in a bucket. This way, all teams can access models that are \\nstored here. In a sense, this can act as your “model registry, ” although it',\n",
       " 'and stored in a bucket. This way, all teams can access models that are \\nstored here. In a sense, this can act as your “model registry, ” although it \\ndoesn’t offer the same functionality as the model registry provided by \\nMLFlow.\\nWhat MLFlow allows you to do is take specific runs and determine \\nwhether to stage that model to the development branch or to production. \\nIn this case, you can have buckets for each team, separated into \\ndevelopment or production branches. It’s a couple extra steps on top of \\nMLFlow’s model registry, but it would still allow you to enjoy the benefits \\nof having a model registry.\\nIn this case, you will simply be creating one bucket to host all of your \\nMLFlow runs. From here, you will be picking a specific run and deploying',\n",
       " 'of having a model registry.\\nIn this case, you will simply be creating one bucket to host all of your \\nMLFlow runs. From here, you will be picking a specific run and deploying \\nto SageMaker. To keep it simple, you will once again use the scikit-learn \\nlogistic regression model that you trained as the model you are deploying.\\nSo with that, create a simple bucket and name it something like \\nmlflow-sagemaker . You can either create it through the AWS CLI or do so \\nthrough the AWS console in your browser.\\nWe will do the latter so that you can visually see what Amazon is really \\ndoing when a bucket is created.\\nKeep in mind that AWS is always working on its UI, so your screen may \\nnot look exactly like what is portrayed. That being said, you are still likely',\n",
       " 'doing when a bucket is created.\\nKeep in mind that AWS is always working on its UI, so your screen may \\nnot look exactly like what is portrayed. That being said, you are still likely \\nable to access S3 bucket storage services, so the core functionality should \\nstill be the same, despite the UI changes.\\nWhen you log into your portal, you should see something like Figure\\xa0 5- 1.Chapter 5  Deploying in\\xa0 aWS233As you can see, you can look up services with the search bar. Here, \\ntype S3 and click the result that states “S3” with the description “Scalable \\nStorage in the Cloud. ”\\nYou should go to a page that looks like Figure\\xa0 5-2.\\nFigure 5-1.  The home screen of the AWS console. Keep in mind that \\nyours is likely to look different to the one shown here',\n",
       " 'Storage in the Cloud. ”\\nYou should go to a page that looks like Figure\\xa0 5-2.\\nFigure 5-1.  The home screen of the AWS console. Keep in mind that \\nyours is likely to look different to the one shown here\\nFigure 5-2.  What your screen might look like when you open the S3 \\nbucket services module. We have greyed out the names of the buckets, \\nbut you can see string names hereChapter 5  Deploying in\\xa0 aWS234You should see a button that says Create Bucket. Click it and you will \\nsee something like Figure\\xa0 5-3.\\nWe named our bucket mlops-sagemaker-runs . You don’t have to worry \\nabout the rest of the options, so scroll down to the bottom and click Create \\nBucket. Once done, you should be able to see your bucket in the list of \\nbuckets.',\n",
       " 'about the rest of the options, so scroll down to the bottom and click Create \\nBucket. Once done, you should be able to see your bucket in the list of \\nbuckets.\\nFrom here, let’s use a subprocess to sync the local mlruns  directory to \\nthis bucket. What this does is upload the entire mlruns  directory to your \\nbucket, so that all of your runs are stored on the cloud.\\nFigure 5-3.  This is how your bucket creation screen may look. In \\nthis case, you are just naming the bucket and aren’t concerned with \\nanything elseChapter 5  Deploying in\\xa0 aWS235First, collect the following attributes:\\n• s3_bucket_name : What is the name of the S3 bucket you \\nare trying to push to?\\n• mlruns_directory : What is the location of the mlruns  \\ndirectory you’re pushing to the bucket?',\n",
       " '• s3_bucket_name : What is the name of the S3 bucket you \\nare trying to push to?\\n• mlruns_directory : What is the location of the mlruns  \\ndirectory you’re pushing to the bucket?\\nBased on that, run the following. We included the bucket name and \\nmlruns  directory in our case, so just replace them with your respective \\nvalues.\\nimport subprocess\\ns3_bucket_name = \"mlops-sagemaker-runs\"\\nmlruns_direc = \"./mlruns/\"\\noutput = subprocess.run([\"aws\",   \"s3\", \"sync\", \"{}\".\\nformat(mlruns_direc), \"s3://{}\".format(s3_bucket_name)], \\nstdout=subprocess.PIPE, encoding=\\'utf-8\\')\\nprint(output.stdout)\\nprint(\"\\\\nSaved to bucket: \", s3_bucket_name)\\nAfter running that code, you should see something similar to Figure\\xa0 5- 4, \\nletting you know that it has synchronized your local mlruns  directory with',\n",
       " 'print(\"\\\\nSaved to bucket: \", s3_bucket_name)\\nAfter running that code, you should see something similar to Figure\\xa0 5- 4, \\nletting you know that it has synchronized your local mlruns  directory with \\nthe bucket. If you see no output, that means there’s nothing new to push \\n(if you are rerunning it). Ensure that the mlruns  directory is in the same \\ndirectory as this notebook; otherwise it won’t be able to find it.Chapter 5  Deploying in\\xa0 aWS236Once this is done, you can proceed to building the container that \\nSageMaker will use to host the model once you get to deployment. To do \\nthat, run the following command in your terminal:\\nmlflow sagemaker build-and-push-container\\nAgain, this requires you to have your Amazon credentials configured.',\n",
       " 'that, run the following command in your terminal:\\nmlflow sagemaker build-and-push-container\\nAgain, this requires you to have your Amazon credentials configured.\\nYou do not need to create a new docker image each time you use a new \\nframework. This one image will be able to handle all your MLFlow models \\nthanks to modularization. This is similar to the deployment pipeline we \\ndiscussed in Chapter 3 from which you simply need to swap models in and \\nout.\\nThis step can take some time, so sit back, relax, and let it do its thing. \\nYou should see something like Figure\\xa0 5-5.\\nFigure 5-4.  This is what your output may look like when you are first \\nsyncing your mlruns directory with the bucket. Make sure that your',\n",
       " 'You should see something like Figure\\xa0 5-5.\\nFigure 5-4.  This is what your output may look like when you are first \\nsyncing your mlruns directory with the bucket. Make sure that your \\nmlruns directory is in the same directory as this notebook fileChapter 5  Deploying in\\xa0 aWS237Once this is finished, the console should output something like \\nFigure\\xa0 5-6.\\nNow, you should be able to see a new container in the portal when you \\nnavigate to Amazon ECR.\\nFigure 5-5.  Something similar to what you should see when you run \\nthe command to build the container\\nFigure 5-6.  What you should see when the docker container image \\nhas successfully been built and pushed to Amazon ECRChapter 5  Deploying in\\xa0 aWS238From your home console, navigate to Amazon ECR, and verify you see',\n",
       " 'has successfully been built and pushed to Amazon ECRChapter 5  Deploying in\\xa0 aWS238From your home console, navigate to Amazon ECR, and verify you see \\nsomething called mlflow-pyfunc . You should see something like Figure\\xa0 5- 7, \\nconfirming that the docker image has successfully been pushed to AWS ECR.\\nWith that, you have set up everything related to MLFlow functionality \\nthat you need in your AWS console in order to deploy your models to \\nSageMaker.\\nLet’s now look at deploying one of the models.\\n Deploying a\\xa0Model to\\xa0AWS SageMaker\\nTo deploy a model to SageMaker, you need to gather the following \\ninformation:\\n• app_name\\n• model_uri\\n• execution_role\\n• region\\n• image_ecr_url\\nThe execution role refers to the Identity and Access Management',\n",
       " 'information:\\n• app_name\\n• model_uri\\n• execution_role\\n• region\\n• image_ecr_url\\nThe execution role refers to the Identity and Access Management \\n(IAM) role, which you can find by searching for “IAM” in the console. Once \\nyou have created or selected an execution role (make sure it can access S3 \\nand can perform get, put, delete, and list operations on it), copy the entire \\nvalue that exists there.\\nFigure 5-7.  After running the command, you should be able to see \\nyour container in the ECR repository listChapter 5  Deploying in\\xa0 aWS239As for the specific policy that this role should follow, refer to Figure\\xa0 5-8 \\nto see how our IAM execution role is set up.\\nAs for the execution role ARN number, you should see something like \\nFigure\\xa0 5-8.',\n",
       " 'to see how our IAM execution role is set up.\\nAs for the execution role ARN number, you should see something like \\nFigure\\xa0 5-8.\\nFigure 5-8.  In the IAM tab, under policies, select (or create) the role \\nyou are going to use to execute the deployment process. There, you \\nshould be able to see the specific Policy ARN value, which you must \\ncopy and keep track of\\nMake sure you have the Policy ARN value copied down. AWS lets you \\ncopy it to the clipboard if you click the little clipboard symbol next to the \\npolicy.\\nTo find the image_ecr_url  value, go back to the ECR and look for \\nsomething like Figure\\xa0 5-7. Now click it to see something like Figure\\xa0 5-9.Chapter 5  Deploying in\\xa0 aWS240Copy the value where it says Image URI, except for the version you',\n",
       " 'something like Figure\\xa0 5-7. Now click it to see something like Figure\\xa0 5-9.Chapter 5  Deploying in\\xa0 aWS240Copy the value where it says Image URI, except for the version you \\nwant. We are running MLFlow version 1.10.0, so copy the value for that one.\\nNext, find the specific run that you want to deploy. Go to your list of S3 \\nbuckets and click the one you created, which should be titled  \\nmlops- sagemaker-  runs .\\nIn here, navigate until you see the folder with several runs displayed. \\nWe picked the top run. Refer to Figure\\xa0 5-10 .\\nFigure 5-10.  Look at your bucket to find the run you want to deploy. \\n(These runs all have the same performance metrics, so it does not \\nmatter which one we pick. If it did, we could look at it through the',\n",
       " '(These runs all have the same performance metrics, so it does not \\nmatter which one we pick. If it did, we could look at it through the \\nMLFlow UI (ensuring the terminal is in the same directory as the \\nsame mlruns directory we pushed) and select the best run.) Also, \\nremember to take note of the experiment ID and the name of the \\nmodel you logged. You should be able to find it if you click the run ID \\nand then artifacts. For our case, it is log_reg_model\\nFigure 5-9.  The Image URI is the value you want to copyChapter 5  Deploying in\\xa0 aWS241With all that information gathered, let’s proceed to the deployment. \\nRun the following:\\nimport boto3\\nimport mlflow.sagemaker as mfs\\nimport json\\napp_name = \"mlops-sagemaker\"\\nexecution_role_arn = \"arn:aws:iam::180072566886:role/',\n",
       " 'Run the following:\\nimport boto3\\nimport mlflow.sagemaker as mfs\\nimport json\\napp_name = \"mlops-sagemaker\"\\nexecution_role_arn = \"arn:aws:iam::180072566886:role/ \\nservice-  role/AmazonSageMaker-ExecutionRole-20181112T142060\"\\nimage_ecr_url = \"180072566886.dkr.ecr.us-east-2.amazonaws.com/\\nmlflow-pyfunc:1.10.0\"\\nregion = \"us-east-2\"\\ns3_bucket_name = \"mlops-sagemaker-runs\"\\nexperiment_id = \"8\"\\nrun_id = \"1eb809b446d949d5a70a1e22e4b4f428\"\\nmodel_name = \"log_reg_model\"\\nmodel_uri = \"s3://{}/{}/{}/artifacts/{}/\".format \\n(s3_bucket_name, experiment_id, run_id, model_name)\\nThis will set up all of the parameters that you will use to run the \\ndeployment code.\\nFinally, let’s get on to the actual deployment code:\\nmfs.deploy(app_name=app_name,\\n           model_uri=model_uri,',\n",
       " 'This will set up all of the parameters that you will use to run the \\ndeployment code.\\nFinally, let’s get on to the actual deployment code:\\nmfs.deploy(app_name=app_name,\\n           model_uri=model_uri,\\n           execution_role_arn=execution_role_arn,\\n           region_name=region,\\n           image_url=image_ecr_url,\\n           mode=mfs.DEPLOYMENT_MODE_CREATE)\\nYou should see something like Figure\\xa0 5-11 .Chapter 5  Deploying in\\xa0 aWS242This step can take a while. If you want to check on the status of your \\nSageMaker endpoint, open up the portal and search for and navigate to \\nSageMaker. There should be a section for Endpoints where you can see \\nall of the SageMaker endpoints that exist. You should see your current \\nendpoint with the status of “creating, ” as in Figure\\xa0 5-12 .',\n",
       " 'all of the SageMaker endpoints that exist. You should see your current \\nendpoint with the status of “creating, ” as in Figure\\xa0 5-12 .\\nFigure 5-11.  You should see something like this when you are \\nattempting to deploy the model. Don’t worry if it takes its timeChapter 5  Deploying in\\xa0 aWS243Once this endpoint is successfully created, which you will know when \\nyou see the status update to “InService, ” you can now move on to making \\npredictions.\\n Making Predictions\\nMaking predictions is simple. All you need is the name of the endpoint and \\nthe functionality that boto3 provides in order for the model to be queried. \\nLet’s define a function to query the model:\\ndef query(input_json):\\n         client = boto3.session.Session().client \\n(\"sagemaker-  runtime\", region)',\n",
       " 'Let’s define a function to query the model:\\ndef query(input_json):\\n         client = boto3.session.Session().client \\n(\"sagemaker-  runtime\", region)\\n        response = client.invoke_endpoint(\\n            EndpointName=app_name,\\n            Body=input_json,\\n            ContentType=\\'application/json; format=pandas-  split\\',\\n        )\\nFigure 5-12.  What you should see in the Endpoints section of \\nAmazon SageMaker. Once it has finished creating the endpoint, you \\nshould see it update the status to “InService. ”Chapter 5  Deploying in\\xa0 aWS244        preds = response[\\'Body\\'].read().decode(\"ascii\")\\n        preds = json.loads(preds)\\n        return preds\\nNow, let’s load your data, process it, and scale it just like you did for the',\n",
       " 'preds = json.loads(preds)\\n        return preds\\nNow, let’s load your data, process it, and scale it just like you did for the \\nlocal model deployment example. Make sure that the folder data  exists, \\nensuring that creditcard.csv  exists within it. Run the following:\\nimport pandas as pd\\nimport mlflow\\nimport mlflow.sklearn\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import roc_auc_score, accuracy_score, \\nconfusion_matrix\\nimport numpy as np\\ndf = pd.read_csv(\"data/creditcard.csv\")\\nOnce the import  statements and the data frame has been loaded, run \\nthe following:\\nnormal = df[df.Class == 0].sample(frac=0.5,  random_state=2020).\\nreset_index(drop= True)',\n",
       " 'Once the import  statements and the data frame has been loaded, run \\nthe following:\\nnormal = df[df.Class == 0].sample(frac=0.5,  random_state=2020).\\nreset_index(drop= True)\\nanomaly = df[df.Class == 1]\\nnormal_train, normal_test = train_test_split(normal,  \\ntest_size = 0.2, random_state = 2020)\\nanomaly_train, anomaly_test = train_test_split(anomaly,  \\ntest_size = 0.2, random_state = 2020)Chapter 5  Deploying in\\xa0 aWS245scaler = StandardScaler()\\nscaler.fit(pd.concat((normal, anomaly)).drop([\"Time\",  \\n\"Class\"], axis=1))\\nOnce this is all finished, run the following to ensure that the model is \\nactually making predictions:\\nscaled_selection = scaler.transform(df.iloc[:80].drop \\n([\"Time\", \"Class\"], axis=1))\\ninput_json = pd.DataFrame \\n(scaled_selection).to_json(orient=\"split\")',\n",
       " 'actually making predictions:\\nscaled_selection = scaler.transform(df.iloc[:80].drop \\n([\"Time\", \"Class\"], axis=1))\\ninput_json = pd.DataFrame \\n(scaled_selection).to_json(orient=\"split\")\\npd.DataFrame(query(input_json)).T\\nYou should see an output like Figure\\xa0 5-13 .\\nFigure 5-13  shows a successful query of the model while it is hosted on \\na SageMaker endpoint and the predictions received as a response.\\nLet’s run the batch query script with some modifications:\\ntest = pd.concat((normal.iloc[:1900], anomaly.iloc[:100]))\\ntrue = test.Class\\ntest = scaler.transform(test.drop([\"Time\", \"Class\"], axis=1))\\npreds = []\\nFigure 5-13.  Querying the deployed model with the scaled data \\nrepresenting the first 80 rows of the data frame and getting a response \\nbackChapter 5  Deploying in\\xa0 aWS246batch_size = 80',\n",
       " 'preds = []\\nFigure 5-13.  Querying the deployed model with the scaled data \\nrepresenting the first 80 rows of the data frame and getting a response \\nbackChapter 5  Deploying in\\xa0 aWS246batch_size = 80\\nfor f in range(25):\\n    print(f\"Batch {f}\", end=\"  - \")\\n     sample =  pd.DataFrame(test[f*batch_size:(f+1)*batch_size]).\\nto_json(orient=\"split\")\\n    output = query(sample)\\n    resp = pd.DataFrame([output])\\n    preds = np.concatenate((preds, resp.values[0]))\\n    print(\"Completed\")\\neval_acc = accuracy_score(true, preds)\\neval_auc = roc_auc_score(true, preds)\\nprint(\"Eval Acc\", eval_acc)\\nprint(\"Eval AUC\", eval_auc)\\nOnce finished, you should see something like Figure\\xa0 5-14 .Chapter 5  Deploying in\\xa0 aWS247All this is great, but what do you do when you want to switch the model',\n",
       " 'print(\"Eval AUC\", eval_auc)\\nOnce finished, you should see something like Figure\\xa0 5-14 .Chapter 5  Deploying in\\xa0 aWS247All this is great, but what do you do when you want to switch the model \\nthat is deployed? Well, SageMaker allows you to update the endpoint and \\nswitch to a new model. Let’s look at how to do this.\\n Switching Models\\nPerhaps you want to update your model, or you have no more use for the \\ncurrent model and its prediction services so you want to replace it without \\nhaving to delete and create a new endpoint. In this case, you can simply \\nupdate the endpoint and swap out the model that is currently hosted on \\nthere. To do so, you only need to collect the new model_uri .\\nFigure 5-14.  Output of the batch querying script. You included a mix',\n",
       " 'there. To do so, you only need to collect the new model_uri .\\nFigure 5-14.  Output of the batch querying script. You included a mix \\nof 100 anomalies with 1900 normal points so that you can get a better \\nidea of how the model performs against anomalies as well. Otherwise, \\nyou would have gotten a handful of anomaliesChapter 5  Deploying in\\xa0 aWS248This time, the model_uri  refers to the URI of the new model that you \\nwant to deploy. In your case, you are selecting the second run of the three \\nruns you uploaded to your bucket. Everything else remains the same, so \\nyou only have to get a new model_uri .\\nNow, run the following, replacing the run_id  value with your chosen \\nrun_id :\\nnew_run_id = \"3862eb3bd89b43e8ace610c521d974e6\"\\nnew_model_uri = \"s3://{}/{}/{}/artifacts/{}/\".format',\n",
       " 'Now, run the following, replacing the run_id  value with your chosen \\nrun_id :\\nnew_run_id = \"3862eb3bd89b43e8ace610c521d974e6\"\\nnew_model_uri = \"s3://{}/{}/{}/artifacts/{}/\".format \\n(s3_bucket_name, experiment_id, new_run_id, model_name)\\nNow that you have run this, run the following code to update the \\nmodel:\\nmfs.deploy(app_name=app_name,\\n           model_uri=new_model_uri,\\n           execution_role_arn=execution_role_arn,\\n           region_name=region,\\n           image_url=image_ecr_url,\\n           mode=mfs.DEPLOYMENT_MODE_REPLACE)\\nYou will find that this function looks quite similar to the one you \\nused to deploy the model. The only parameter that differs is the mode, \\nas you are now doing mfs.DEPLOYMENT_MODE_REPLACE  instead of mfs.\\nDEPLOYMENT_MODE_CREATE .',\n",
       " 'used to deploy the model. The only parameter that differs is the mode, \\nas you are now doing mfs.DEPLOYMENT_MODE_REPLACE  instead of mfs.\\nDEPLOYMENT_MODE_CREATE .\\nRefer to Figure\\xa0 5-15  to see what the output should look like.  \\nNote that this also can take some time to finish.Chapter 5  Deploying in\\xa0 aWS249While this is running, you can check on the endpoint in your portal to \\nsee that it is now updating. Refer to Figure\\xa0 5-16  to see this.\\nFigure 5-15.  This is what your output should look like after running \\nthe update code\\nFigure 5-16.  The endpoint is now updating. Once finished, it should \\nshow “InService” just like when the endpoint was being createdChapter 5  Deploying in\\xa0 aWS250Once it finishes running, you can query this model again using the',\n",
       " 'show “InService” just like when the endpoint was being createdChapter 5  Deploying in\\xa0 aWS250Once it finishes running, you can query this model again using the \\nsame function. You don’t have to modify the batch script either.\\nNow that you know how to update the endpoint with a new model, we \\nwill look at how you can remove the endpoint and the deployed model.\\n Removing Deployed Model\\nPerhaps you have multiple endpoints each with a different model hosted, \\nand you no longer want to keep an endpoint running because of the cost. \\nTo delete an endpoint, you only need the following information:\\n• app_name\\n• region\\nWith that information defined, which it already should be, you can \\nsimply run the following:\\nmfs.delete(app_name=app_name,region_name=region)',\n",
       " '• app_name\\n• region\\nWith that information defined, which it already should be, you can \\nsimply run the following:\\nmfs.delete(app_name=app_name,region_name=region)\\nYou should see it output something like Figure\\xa0 5-17 . This process \\nfinishes quite quickly.\\nYou can go check the endpoint in the portal as well, and it should show \\nsomething like Figure\\xa0 5-18 .\\nFigure 5-17.  The output of the deletion commandChapter 5  Deploying in\\xa0 aWS251As you can see, the endpoint is now completely gone.\\nOne thing to note is that you should make sure you don’t accidentally \\nleave any resources running because the costs can certainly stack up over \\ntime and put a dent in your wallet. For services like SageMaker endpoints, \\nyou are charged by the hour, so be sure to delete them once you’re done \\nwith them.',\n",
       " 'time and put a dent in your wallet. For services like SageMaker endpoints, \\nyou are charged by the hour, so be sure to delete them once you’re done \\nwith them.\\nAs for the S3 bucket and the ECR container, those are a one-time \\ncharge that only bill for data transfer.\\nWith that, you now know how to operationalize your MLFlow model \\nwith AWS SageMaker.\\n Summary\\nMLFlow provides explicit AWS SageMaker support in its operationalization \\ncode. And so we covered how to upload your runs to an S3 bucket and \\nhow to create and push an MLFlow Docker container image for AWS \\nSageMaker to use when operationalizing your models. We also covered \\nFigure 5-18.  SageMaker endpoint resources after the deletion. There',\n",
       " 'SageMaker to use when operationalizing your models. We also covered \\nFigure 5-18.  SageMaker endpoint resources after the deletion. There \\nshould be nothing here if the deletion process went successfullyChapter 5  Deploying in\\xa0 aWS252how to deploy your model on an endpoint, query it, update the endpoint \\nwith a new model, and delete the endpoint. Hopefully now you now know \\nhow to operationalize your machine learning models with MLFlow and \\nAWS SageMaker.\\nIn the next chapter, we will look at how you can operationalize your \\nMLFlow models with Microsoft Azure.Chapter 5  Deploying in\\xa0 aWS253© Sridhar Alla, Suman Kalyan Adari 2021 \\nS. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \\nhttps://doi.org/10.1007/978-1-4842-6549-9_6CHAPTER 6\\nDeploying in\\xa0Azure',\n",
       " 'S. Alla and S. K. Adari, Beginning MLOps with MLFlow ,  \\nhttps://doi.org/10.1007/978-1-4842-6549-9_6CHAPTER 6\\nDeploying in\\xa0Azure\\nIn this chapter, we will cover how you can use Microsoft Azure to \\noperationalize your MLFlow models. In particular, we will look at how \\nyou can also utilize Azure’s built-in functionality to deploy a model to a \\ndevelopment branch and to a production branch, along with how you can \\nquery the models once deployed.\\n Introduction\\nIn the previous chapter, we went over how to deploy your models to \\nAmazon SageMaker, manage them through update or delete events, \\nand query them. Now, we will shift our focus to show how you can \\noperationalize your MLFlow models using Microsoft Azure.\\nBefore you begin, here is an important prerequisites :',\n",
       " 'and query them. Now, we will shift our focus to show how you can \\noperationalize your MLFlow models using Microsoft Azure.\\nBefore you begin, here is an important prerequisites :\\n• Install azureml-sdk in your Python environment.\\nJust like with AWS, Microsoft Azure is constantly being worked on and \\nupdated. Since MLFlow supports Microsoft Azure, you should be able \\nto utilize MLFlow to operationalize your models. Any new functionality \\nis sure to be documented by MLFlow, and in the absolute worst-case \\nscenario, you should still be able to host a server on Azure and maintain \\nyour MLOps functionality that way.\\nAgain, we will explore how to do this in the next chapter when we look',\n",
       " 'scenario, you should still be able to host a server on Azure and maintain \\nyour MLOps functionality that way.\\nAgain, we will explore how to do this in the next chapter when we look \\nat how to operationalize your MLFlow models with the Google Cloud API.254In detail, we will go over the following in this chapter:\\n• Configuring Azure:  Here, you basically use MLFlow’s \\nfunctionality to build a container image for the model \\nto be hosted in. Then, you push it to Azure’s Azure \\nContainer Instances (ACI), similar to how you pushed \\nan image to the Amazon AWS Elastic Container \\nRegistry (ECR).\\n• Deploying a model to Azure (dev stage):  Here, you \\nuse built-in azureml-sdk module code to push a \\nmodel to Azure. However, this is a development stage \\ndeployment, so this model is not production-ready',\n",
       " 'use built-in azureml-sdk module code to push a \\nmodel to Azure. However, this is a development stage \\ndeployment, so this model is not production-ready \\nsince its computational resources are limited.\\n• Making predictions:  Once the model has finished \\ndeployment, it is ready to be queried. This is done \\nthrough an HTTP request. This is how you can verify \\nthat your model works once hosted on the cloud since \\nit’s in the development stage.\\n• Deploying to production:  Here, you utilize MLFlow \\nAzure module code to deploy the model to production \\nby creating a container instance (or any other \\ndeployment configuration provided, like Azure \\nKubernetes Service).\\n• Making predictions:  Similar to how you query the \\nmodel in the dev stage, you query the model once it',\n",
       " 'deployment configuration provided, like Azure \\nKubernetes Service).\\n• Making predictions:  Similar to how you query the \\nmodel in the dev stage, you query the model once it \\nhas been deployed to the production stage and run the \\nbatch query script from the previous chapter.\\n• Switching models:  MLFlow does not provide explicit \\nfunctionality to switch your models, so you must delete \\nthe service and recreate it with another model run.Chapter 6  Deploying in\\xa0 azure255• Removing the deployed model:  Finally, you undo \\nevery deployment that you did and remove all \\nresources. That is, you delete both the development \\nand production branch services as well as the container \\nregistries and any additional services created once you \\nare done.\\nWith that, let’s get started!\\n Configuring Azure',\n",
       " 'and production branch services as well as the container \\nregistries and any additional services created once you \\nare done.\\nWith that, let’s get started!\\n Configuring Azure\\nBefore you can start using Azure’s functionality to operationalize your \\nmodels, you must first create or connect to an existing Azure workspace. \\nYou can do this either through code or the UI in a browser.\\nIn your case, you will open up the portal in the browser and learn how \\nto create a workspace. Refer to Figure\\xa0 6-1.\\nFigure 6-1.  An example of the Microsoft Azure portal home screenChapter 6  Deploying in\\xa0 azure256Next, click the Create a resource option and search for “Machine \\nLearning. ” You should see something like Figure\\xa0 6-2.\\nClick the Create button. You should see something like Figure\\xa0 6-3. (We',\n",
       " 'Learning. ” You should see something like Figure\\xa0 6-2.\\nClick the Create button. You should see something like Figure\\xa0 6-3. (We \\nfilled the fields with our own parameters.)\\nYour subscription might differ from ours. For the resource group, we \\ncreated a new one titled azure-mlops .\\nThe fields you completed in Figure\\xa0 6-3 are enough to create your \\nworkspace. Next, click the Review + create option and click Create once \\nAzure states that the validation procedure has been passed and allows you \\nto click Create.\\nFigure 6-2.  An example of the service “Machine Learning” provided \\nby Azure. You want to create a workspace within this service, so click \\nthe Create buttonChapter 6  Deploying in\\xa0 azure257This will take some time to deploy. Once the workspace has been',\n",
       " 'by Azure. You want to create a workspace within this service, so click \\nthe Create buttonChapter 6  Deploying in\\xa0 azure257This will take some time to deploy. Once the workspace has been \\ncreated, go back to the home portal and click the All resources option. You \\nshould see something like Figure\\xa0 6-4.\\nClick your workspace, which should have an image of a chemical \\nbeaker next to it.\\nIn this overview, you will see several parameters associated with \\nthis workspace. Make sure to keep track of the following attributes of the \\nworkspace so that you can connect to it in the code:\\nFigure 6-3.  Workspace creation UI (we filled in the fields with our \\nown parameters)Chapter 6  Deploying in\\xa0 azure258• workspace_name  (azure-mlops-workspace )',\n",
       " 'Figure 6-3.  Workspace creation UI (we filled in the fields with our \\nown parameters)Chapter 6  Deploying in\\xa0 azure258• workspace_name  (azure-mlops-workspace )\\n• subscription  (The value where it says Subscription-  ID)\\n• resource_group  (azure-mlops)\\n• location  (East-US)\\nRefer to Figure\\xa0 6-5.\\nFigure 6-5.  You should see something like this for your own \\nworkspace. Here we’ve censored potentially sensitive fields, but you \\nshould be able to see your own unique subscription ID on your screen. \\nThis is the value you want to use\\nFigure 6-4.  You might see something like this when you look at the \\nAll resources optionChapter 6  Deploying in\\xa0 azure259Now that you have that, run the following to create/connect to your \\nown workspace:\\nimport azureml\\nfrom azureml.core import Workspace',\n",
       " 'All resources optionChapter 6  Deploying in\\xa0 azure259Now that you have that, run the following to create/connect to your \\nown workspace:\\nimport azureml\\nfrom azureml.core import Workspace\\nworkspace_name = \"MLOps-Azure\"\\nworkspace_location=\"East US\"\\nresource_group = \"mlflow_azure\"\\nsubscription_id = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\\nworkspace = Workspace.create(name = workspace_name,\\n                             location = workspace_location,\\n                             resource_group = resource_group,\\n                             subscription_id = subscription_id,\\n                             exist_ok= True)\\nIf you have successfully connected to your workspace, the cell should \\nrun without any issues.\\nNext, you must build the MLFlow container image to be used by Azure.',\n",
       " 'If you have successfully connected to your workspace, the cell should \\nrun without any issues.\\nNext, you must build the MLFlow container image to be used by Azure. \\nHere, you also specify the run of the model you are trying to deploy.\\nIn the case of Amazon SageMaker, you were able to reference runs \\nfrom your local machine or runs from an S3 bucket. You can do the same \\nthing for Azure, except using Azure’s storage entities called blobs.\\nEither way, you need the run ID  of the model you are deploying and \\nthe artifact scheme  that the model is logged in. For the models you stored \\nin Amazon S3 buckets, you used the scheme s3:/ , but this time you will \\njust use a run locally. If you’d like, you can still use your Amazon S3 bucket',\n",
       " 'in Amazon S3 buckets, you used the scheme s3:/ , but this time you will \\njust use a run locally. If you’d like, you can still use your Amazon S3 bucket \\nor Google Cloud buckets. Where you store your run does not matter.\\nRun the following, replacing the values with your specific run and \\nstorage scheme:\\nrun_id = \"1eb809b446d949d5a70a1e22e4b4f428\"\\nmodel_name = \"log_reg_model\"\\nmodel_uri = f\"runs:/{run_id}/{model_name}\"Chapter 6  Deploying in\\xa0 azure260The model name should be the same in your case unless you changed \\nit. Since we are using local runs, we have a URI starting with runs:/ . Again, \\nchange this to whatever is appropriate in your case.\\nFinally, with all that information set, let’s create the container image:\\nimport mlflow.azureml',\n",
       " 'change this to whatever is appropriate in your case.\\nFinally, with all that information set, let’s create the container image:\\nimport mlflow.azureml\\nmodel_image, azure_model =  mlflow.azureml.build_image  \\n(model_uri=model_uri, \\nworkspace=workspace,\\n                           model_name=\"sklearn_logreg_dev\", \\n                           image_name=\"model\",\\n                            description=\"SkLearn LogReg Model \\nfor Anomaly Detection\",\\n                           synchronous= False)\\nYou should see something like Figure\\xa0 6-6. You may or may not see the \\nwarning messages depending on your version of MLFlow.\\nFigure 6-6.  Building and pushing the container to Azure’s container \\nregistry. Ignore the warning messages for now. You might not see these',\n",
       " 'warning messages depending on your version of MLFlow.\\nFigure 6-6.  Building and pushing the container to Azure’s container \\nregistry. Ignore the warning messages for now. You might not see these \\nmessages in the future. Since this is code created and maintained by \\nMLFlow, it is likely that they will provide support for whatever new \\nfunctionality Azure pushesChapter 6  Deploying in\\xa0 azure261Next, run the following to check the status of the container:\\nmodel_image.wait_for_creation(show_output= True)\\nYou should see something like Figure\\xa0 6-7.\\nOnce the image has been created, you can now deploy your model.\\n Deploying to\\xa0Azure (Dev Stage)\\nOne interesting bit of functionality that Azure provides is the ACI \\nwebservice. This webservice is specifically used for the purposes of',\n",
       " 'Deploying to\\xa0Azure (Dev Stage)\\nOne interesting bit of functionality that Azure provides is the ACI \\nwebservice. This webservice is specifically used for the purposes of \\ndebugging or testing some model under development, hence why it is \\nsuitable for use in the development stage.\\nYou are going to deploy an ACI webservice instance based on the \\nmodel image you just created.\\nRun the following:\\nfrom azureml.core.webservice import AciWebservice, Webservice\\naci_service_name = \"sklearn-model-dev\"\\naci_service_config = AciWebservice.deploy_configuration()\\naci_service =  Webservice.deploy_from_image  \\n(name=aci_service_name,\\n              image=model_image,\\n               deployment_config=aci_service_config,\\n              workspace=workspace)',\n",
       " 'aci_service =  Webservice.deploy_from_image  \\n(name=aci_service_name,\\n              image=model_image,\\n               deployment_config=aci_service_config,\\n              workspace=workspace)\\nFigure 6-7.  Checking the output of the progress in the image creation \\noperationChapter 6  Deploying in\\xa0 azure262You should see something like Figure\\xa0 6-8.\\nThis exact way of starting the service may be deprecated in the near \\nfuture in favor of Environments. For the time being, you should still be able \\nto start an ACI service in this manner, but the important thing to know \\nis that there is a web service specifically tailored for development stage \\ntesting.\\nNow run the following to check the progress:\\naci_service.wait_for_deployment(show_output= True)\\nYou should see something like Figure\\xa0 6-9.',\n",
       " 'testing.\\nNow run the following to check the progress:\\naci_service.wait_for_deployment(show_output= True)\\nYou should see something like Figure\\xa0 6-9.\\nBefore making your predictions, let’s first verify that you can reach your \\nservice:\\naci_service.scoring_uri\\nFigure 6-9.  The output you should see from checking if the \\ndeployment has succeeded\\nFigure 6-8.  The output of creating the ACI service. It seems that this \\nfunction may be removed in the future, but for now this is one way to \\naccess the ACI service and deploy the modelChapter 6  Deploying in\\xa0 azure263You should see something like Figure\\xa0 6-10 . If not, try going into your \\nresources in the portal to verify that a new container exists with the name \\nsklearn-model-dev . If not, try rerunning the cells in the same order. It',\n",
       " 'resources in the portal to verify that a new container exists with the name \\nsklearn-model-dev . If not, try rerunning the cells in the same order. It \\nshould display some URI this time.\\nYou should see something like Figure\\xa0 6-10 .\\nYou can now make predictions with this model.\\n Making Predictions\\nNow you need to acquire some data to predict with.\\nJust like before, you will be loading the credit card dataset, \\npreprocessing it, and setting aside a small batch that you will query the \\nmodel with. Run the following blocks of code, and make sure you have the \\nfolder named data  in this directory with creditcard.csv  in it:\\nimport pandas as pd\\nimport mlflow\\nimport mlflow.sklearn\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import StandardScaler',\n",
       " 'import pandas as pd\\nimport mlflow\\nimport mlflow.sklearn\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import roc_auc_score, accuracy_score, \\nconfusion_matrix\\nFigure 6-10.  The scoring URI is displayed, indicating that you can \\nconnect to it and make predictionsChapter 6  Deploying in\\xa0 azure264import numpy as np\\nimport subprocess\\nimport json\\ndf = pd.read_csv(\"data/creditcard.csv\")\\nOnce you have loaded all the modules and have loaded the data, run \\nthe following:\\nnormal = df[df.Class == 0].sample(frac=0.5, random_state=2020).\\nreset_index(drop= True)\\nanomaly = df[df.Class == 1]\\nnormal_train, normal_test = train_test_split(normal, test_size',\n",
       " 'the following:\\nnormal = df[df.Class == 0].sample(frac=0.5, random_state=2020).\\nreset_index(drop= True)\\nanomaly = df[df.Class == 1]\\nnormal_train, normal_test = train_test_split(normal, test_size \\n= 0.2, random_state = 2020)\\nanomaly_train, anomaly_test = train_test_split(anomaly,  \\ntest_size = 0.2, random_state = 2020)\\nscaler = StandardScaler()\\nscaler.fit(pd.concat((normal, anomaly)).drop([\"Time\", \"Class\"], \\naxis=1))\\nIn cells, the above two blocks of code should look like Figure\\xa0 6-11 .Chapter 6  Deploying in\\xa0 azure265Once you are all done with preparing the data, let’s define a function to \\nhelp you query the deployed model:\\nimport requests\\nimport json\\ndef query(scoring_uri, inputs):\\n    headers = {\\n    \"Content-Type\": \"application/json\",\\n    }']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The Dataset Into The Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 500 Headlines.\n"
     ]
    }
   ],
   "source": [
    "astra_vector_store.add_texts(texts[:500])\n",
    "print('Inserted %i Headlines.' % len(texts[:500]))\n",
    "astra_vector_index= VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Q&A Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUESTION: \"What is Model Validation in MLOps?\"\n",
      "ANSWER: \"Model Validation in MLOps is a stage where the model goes through a process to seek the best hyperparameters. This can be done by using a script to iterate through various configurations of hyperparameter values and utilizing k-fold cross-validation. The goal of this stage is to help tune the model’s hyperparameters and it can even be automated to save time and resources in the long run.\"n\n",
      "FIRST DOCUMENTS BY RELEVANCE:\n",
      "   [0.9343] \"other mechanism that the team has implemented, and the process moves \n",
      "on to the validation stage.\n",
      " Model Validation\n",
      "In this stage, the model begins th ...\"\n",
      "   [0.9319] \"new model or just update the current model.Chapter 3  What Is MLOps?97 3. Automated model building and analysis:  In this \n",
      "step, data scientists and m ...\"\n",
      "   [0.9257] \"can allow for deployment on a simple click of a button. Usually, the \n",
      "deployment is to a staging environment first, where the functionality can \n",
      "be te ...\"\n",
      "   [0.9241] \"model development teams reach a target level \n",
      "of performance, they must work on building \n",
      "a workable model that can be called by other \n",
      "code. For exam ...\"\n"
     ]
    }
   ],
   "source": [
    "first_question= True\n",
    "while True:\n",
    "    if first_question:\n",
    "        query_text= input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
    "    else:\n",
    "        query_text= input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
    "\n",
    "    if query_text.lower()==\"quit\":\n",
    "        break\n",
    "\n",
    "    if query_text==\"\":\n",
    "        continue\n",
    "\n",
    "    first_question= False\n",
    "\n",
    "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
    "    answer= astra_vector_index.query(query_text, llm=llm).strip()\n",
    "    print(\"ANSWER: \\\"%s\\\"n\" % answer)\n",
    "\n",
    "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
    "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
    "        print(\"   [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
