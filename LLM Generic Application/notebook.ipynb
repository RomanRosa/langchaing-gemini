{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import OpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.llms import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Read The Document\n",
    "def read_doc(directory):\n",
    "    file_loader=PyPDFDirectoryLoader(directory)\n",
    "    documents=file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Kubernetes \\nfor MLOps: \\nScaling Enterprise Machine Learning, \\nDeep Learning, and AI\\nBy Sam Charrington\\nFounder, TWIML  \\nHost, TWIML AI PodcastSPONSORED BY\\nSPONSORED BY', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 0}),\n",
       " Document(page_content='page 2 |  twimlai.com\\nTable of Contents\\n Preface to the Second Edition .............................................................. 4\\n Introduction .......................................................................................... 5\\n The Machine Learning Process ............................................................ 6\\n Machine Learning at Scale ................................................................. 10\\n Enter Containers and Kubernetes ....................................................... 14\\n Getting to Know Kubernetes ............................................................... 15\\n Kubernetes for Machine and Deep Learning  ....................................... 18\\n MLOps on Kubernetes with Kubeflow ................................................. 20\\n The Kubernetes MLOps Ecosystem .................................................... 23\\n Case Study: Volvo Cars ....................................................................... 23\\n Getting Started ................................................................................... 25\\n About TWIML ...................................................................................... 29', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 1}),\n",
       " Document(page_content='page 3 |  twimlai.com\\nSponsors\\nThis copy of Kubernetes for MLOps: Scaling Enterprise Machine Learning, Deep Learning, and \\nAI is brought to you by HPE and Intel. We thank them for their support.\\nHewlett Packard Enterprise is the global edge-to-cloud platform-as-a-service company that \\nhelps organizations accelerate outcomes by unlocking value from all of their data, everywhere. \\nBuilt on decades of reimagining the future and innovating to advance the way we live and \\nwork, HPE delivers unique, open and intelligent technology solutions, with a consistent \\nexperience across all clouds and edges, to help customers develop new business models, \\nengage in new ways, and increase operational performance. For more information, visit:  \\nwww.hpe.com.\\nIntel is an industry leader, creating world-changing technology that enables global progress \\nand enriches lives. Inspired by Moore’s Law, we continuously work to advance the design and \\nmanufacturing of semiconductors to help address our customers’ greatest challenges. By \\nembedding intelligence in the cloud, network, edge and every kind of computing device, we \\nunleash the potential of data to transform business and society for the better. To learn more \\nabout Intel’s innovations, visit www.intel.com.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 2}),\n",
       " Document(page_content='page 4 |  twimlai.com\\nPreface to the Second Edition\\nIn December 2018, I raced to complete the first edition of this ebook in time for Kubecon \\nSeattle. After weeks of research, interviews, and writing, I was excited to unveil a timely and \\nneeded resource for a small but growing community.\\nI made the deadline, but as I sat through the conference’s keynotes and sessions, listening \\nto the many new announcements relating to machine learning (ML) and artificial intelligence \\n(AI) workloads on Kubernetes, my heart sank. While I knew the ebook would prove a valuable \\nresource to readers, I knew too that it would soon need an update.\\nSince that time, the use of Kubernetes to support an organization’s ML/AI projects has evolved \\nfrom an obscure idea to one being explored and pursued by many organizations facing the \\nchallenge of getting ML models into production and scaling them.\\nAs interest in running ML/AI workloads atop Kubernetes has grown, so has the surrounding \\necosystem. When the first edition was written, Kubeflow was yet in its infancy, having just \\nseen its 0.3 release.1 At the time, the Google-founded project was far from production ready, \\nand still fairly narrowly focused on running TensorFlow jobs on Kubernetes. \\nKubeflow has since published a general availability (GA) release of the project, and now aspires \\nto support a wide variety of end-to-end machine learning workflows on Kubernetes. The \\nannouncement of version 1.0 of the project in March of 2020 was accompanied by testimonials \\nfrom companies such as US Bank, Chase Commercial Bank, and Volvo Cars. (I’m excited to \\ninclude the first public profile of Volvo’s journey in this book.)\\nThe broader Kubernetes ML/AI ecosystem has exploded as well, with a wide variety of open \\nsource and commercial offerings building upon the foundation offered by Kubernetes. We’ll \\nmention a few of these in this book, but with so many services and offerings to keep track of, \\nand with offerings evolving so quickly, we’ve decided to move our directory of open source \\nand commercial tools to a new home: the online TWIML AI Solutions Guide.\\nFollowing the evolution of this space has been a fascinating journey, and I’m pleased to bring \\nyou the second edition of this ebook. My hope is that the information shared here will help \\nyou better understand this promising direction for building, deploying, and scaling machine \\nlearning models in your organization, and ultimately contribute to your organization’s own \\ninnovative use of machine learning and AI.\\nThanks for reading!\\nSam ', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 3}),\n",
       " Document(page_content='page 5 |  twimlai.com\\nIntroduction\\nEnterprise interest in machine learning and artificial intelligence continues to grow, with \\norganizations dedicating increasingly large teams and resources to ML/AI projects. As \\nbusinesses scale their investments, it becomes critical to build repeatable, efficient, and \\nsustainable processes for model development and deployment.\\nThe move to drive more consistent and efficient processes in machine learning parallels \\nefforts towards the same goals in software development. Whereas the latter has come to be \\ncalled DevOps, the former is increasingly referred to as MLOps.\\nWhile DevOps, and likewise MLOps, are principally about practices rather than technology, to \\nthe extent that those practices are focused on automation and repeatability, tools have been \\nan important contributor to their rise. In particular, the advent of container technologies like \\nDocker was a significant enabler of DevOps, allowing users to drive increased agility, efficiency, \\nmanageability, and scalability in their software development efforts. \\nContainers remain a foundational technology for both DevOps and MLOps. Containers provide \\na core piece of functionality that allow us to run a given piece of code—whether a notebook, \\nan experiment, or a deployed model—anywhere, without the “dependency hell” that plagues \\nother methods of sharing software. But, additional technology is required to scale containers \\nto support large teams, workloads, or applications. This technology is known as a container \\norchestration system, the most popular of which is Kubernetes.\\nIn this book we explore the role that Kubernetes plays in supporting MLOps:\\n• Chapter 1 presents a high-level overview of the machine learning process and the \\nchallenges organizations tend to encounter when it comes time to scale it.\\n• Chapter 2 elaborates on why supporting machine learning at scale is so difficult, and \\ncovers a few of the many challenges faced by data scientists and machine learning \\nengineers in getting their models into production, at scale.\\n• Chapter 3 provides an introduction to container technology and Kubernetes, and shows \\nhow they help address the challenges referenced earlier.\\n• In chapter 4, we zoom in to discuss Kubernetes in more detail.\\n• In chapter 5 , we return to the ML workflow, and show how Kubernetes can help \\norganizations overcome the various challenges it presents.\\n• Chapters 6 and 7 look at the role of Kubeflow and other higher-level tools in delivering \\nMLOps.\\n• In chapter 8 , we get an opportunity to sit in the passenger seat and ride along with Volvo \\nCars, an early Kubernetes and Kubeflow user, to learn from their experiences building \\na ML platform atop these technologies.\\n• Chapter 8 offers some brief advice on how you can get started with your own project.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 4}),\n",
       " Document(page_content='page 6 |  twimlai.com\\nIf you’re just getting started with MLOps, know that there’s no better time than the present to \\nbegin this journey. The user and practitioner communities are exploding, the practices and \\ntechnology are maturing, and the support and educational resources are more plentiful and \\nof higher quality with each passing day.\\nThe Machine Learning Process\\nBefore we can discuss enabling and scaling the delivery of machine learning, we must \\nunderstand the full scope of the machine learning process. While much of the industry dialogue \\naround machine learning continues to be focused on modeling, the reality is that modeling \\nis a small part of the process. This was poignantly illustrated in a 2015 paper2 by authors \\nfrom Google, and remains true today.\\nFigure 1. Illustration from Hidden Technical Debt in Machine Learning Systems paper.  \\nMachine learning models are represented by the small box labeled “ML Code” in the center of the diagram.  \\nThe paper refers to the supporting infrastructure as “vast and complex.”\\nIn order for organizations to reap the rewards of their machine learning efforts and achieve \\nany level of scale and efficiency, models must be developed within a repeatable process that \\naccounts for the critical activities that precede and follow model development. \\nWhen presented with a new problem to solve, data scientists will develop and run a series of \\nexperiments that take available data as input and aim to produce an acceptable model as \\noutput. Figure 2 presents a high-level view of the end-to-end ML workflow, representing the \\nthree major “steps” in the process—data acquisition and preparation, experiment management \\nand model development, and model deployment and performance monitoring—as well as \\nthe iterative loops that connect them. Conﬁguration Data CollectionData\\nVeriﬁcationMachine\\nResource\\nManagement\\nAnalysis Tools\\nProcess\\nManagement ToolsFeature\\nExtractionServing\\nInfrastructureMonitoring\\nML\\nCode', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 5}),\n",
       " Document(page_content='page 7 |  twimlai.com\\nFigure 2. The machine learning process\\nData acquisition & preparation\\nTo build high quality models, data scientists and machine learning engineers must have \\naccess to large quantities of high-quality labeled    training data. This data very rarely exists \\nin a single place or in a form directly usable by data scientists. Rather, in the vast majority of \\ncases, the training dataset must be built by data scientists, data engineers, machine learning \\nengineers, and business domain experts working together.\\nIn the real world, as opposed to in academia or Kaggle competitions, the creation of training \\ndatasets usually involves combining data from multiple sources. For example, a data scientist \\nbuilding a product recommendation model might build the training dataset by joining data \\nfrom web activity logs, search history, mobile interactions, product catalogs, and transactional \\nsystems.\\nWhile the specific use of labeled training datasets is characteristic of supervised learning, \\nas opposed to unsupervised, self-supervised or reinforcement learning, the former remains \\nthe most popular type of machine learning in use today. While this book occasionally refers \\nto labels and other supervised learning concepts, its key messages apply broadly.\\nOnce compiled, training data must then be prepared for model development. To do this, data \\nscientists will apply a series of transformations to the raw data to cleanse and normalize it. \\nExamples include removing corrupt records, filling in missing values, and correcting \\ninconsistencies like differing representations for states or countries.\\nTransformations may also be required to extract labels. For example, developing a model that \\npredicts the likelihood of churn among customers will require a label indicating which of the \\ncustomers in our transactional database are examples of churn. This can, in turn, require a \\ncomplex query against the data warehouse that considers factors such as the products or \\nservices that we are basing the prediction on, the number of days without a transaction, the \\nwindow in which we want to make predictions, and more.3 Data Acquisition &\\nPreparatio nExperiment Management &\\nModel Developmen t Model Deployment &\\nMonitorin g', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 6}),\n",
       " Document(page_content='page 8 |  twimlai.com\\nAs a result of the organizational and technical complexity involved, the process of acquiring \\nand preparing enterprise data can consume the vast majority of a data scientist’s effort on \\na given project—50 to 80 percent, according to some reports.4\\nIf not accounted for during planning or supported by appropriate processes and tools, data \\nacquisition can consume all of the time and resources allocated for a project, leaving little \\navailable for the remaining steps in the machine learning process. \\nExperiment management and model development\\nExperimentation is central to the machine learning process. During modeling, data scientists \\nand machine learning engineers (MLEs) run a series of experiments to identify a robust \\npredictive model. Typically, many models—possibly hundreds or even thousands—will be \\ntrained and evaluated in order to identify the techniques, architectures, learning algorithms, \\nand parameters that work best for a particular problem.\\nFeature engineering is the iterative process of creating the features and labels needed to train \\nthe model through a series of data transformations. Feature engineering is often performed \\nin lockstep with model training, because the ability to identify helpful features can have a \\nsignificant impact on the overall success of the modeling effort. That said, a fine line separates \\nthe preliminary data cleansing and normalization steps associated with data preparation from \\nfeature engineering.\\nSimple examples of feature engineering include generating derived features (such as calculating \\nan age from a birthdate) or converting categorical variables (such as transaction types) into \\none-hot encoded, or binary, vectors.\\nWith deep learning, features are usually straightforward because deep neural networks (DNNs) \\ngenerate their own internal transformations. With traditional machine learning, feature \\nengineering can be quite challenging and relies heavily on the creativity and experience of the \\ndata scientist and their understanding of the business domain or ability to effectively collaborate \\nwith domain experts.\\nIn addition, a variety of tools under the broad banner of “AutoML” are finding popular use to \\nautomate aspects of experimentation and model development. Offerings exist to automate \\nfeature engineering by creating and testing a wide variety of candidate features, identify the \\nbest machine learning or deep learning models (the latter referred to as neural architecture \\nsearch), automatically tune or optimize model hyperparameters, and automate the compression \\nof large DNNs so that they’re more amenable to mobile or edge deployment.\\nWhether manually performed or automated, the various aspects of experimentation and \\nmodel training can be quite computationally intensive, especially in the case of deep learning. \\nHaving ready access to the right infrastructure for each experiment, such as CPUs, GPUs, \\nmemory, etc.—whether locally or in the cloud—can significantly impact the speed and agility \\nof the data science team during the model development phase of the process.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 7}),\n",
       " Document(page_content='page 9 |  twimlai.com\\nModel deployment\\nOnce a model has been developed, it must be deployed in order to be used. While deployed \\nmodels can take many forms, typically the model is embedded directly into application code \\nor put behind an API of some sort. HTTP-based (i.e. REST or gRPC) APIs are increasingly \\nused so that developers can access model predictions as microservices.\\nWhile model training might require large bursts of computing hardware over the course of \\nseveral hours, days or weeks, model inference—making queries against models—can be even \\nmore computationally expensive than training over time. Each inference against a deployed \\nmodel requires a small but significant amount of computing power.    Unlike the demands of \\ntraining, the computational burden of inference scales with the number of inferences made \\nand continues for as long as the model is in production. \\nOr not so small in the case of some deep learning models. Google’s efforts to build its \\nTensor Processing Unit (TPU), a specialized chip for inference, began in 2013 when engineer \\nJeff Dean projected that if people were to use voice search for 3 minutes a day, meeting \\nthe inference demands of speech recognition would require Google data centers to double \\nin capacity.5 \\nMeeting the requirements of inference at scale is a classic systems engineering problem. \\nAddressing issues like scalability, availability, latency, and cost are typically primary concerns. \\nWhen mobile or edge deployment is required, additional considerations like processing cycles, \\nmemory, size, weight, and power consumption come into play.\\nAt a certain point, we often end up needing to turn to distributed systems to meet our scalability \\nor performance goals. This of course brings along its own set of challenges and operational \\nconsiderations. How do we get our model onto multiple machines and ensure consistency \\nover time. How do we update to new models without taking our applications out of service? \\nWhat happens when problems arise during an upgrade? How can we test new models on live \\ntraffic? \\nFortunately for those deploying models, much progress has been made addressing these \\nquestions for microservices and other software systems.\\nModel monitoring\\nOnce a model is put into production, it is important to monitor its ongoing performance. \\nMachine learning models are perishable, meaning that the statistical distribution of the data \\na model sees in production will inevitably start to drift away from that seen during training. \\nThis will cause model performance to degrade, which can result in negative business outcomes \\nsuch as lost sales or undetected fraudulent activity if not detected and addressed.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 8}),\n",
       " Document(page_content='page 10 |  twimlai.com\\nProduction models should be instrumented so that the inputs to and results of each inference \\nare logged, allowing usage to be reviewed and performance to be monitored on an ongoing \\nbasis. Owners of models experiencing degraded performance can use this information to \\ntake corrective action, such as retraining or re-tuning the model.\\nBusiness or regulatory requirements may impose additional requirements dictating the form \\nor function of model monitoring for audit or compliance purposes, including in some cases \\nthe ability to explain model decisions.\\nModel monitoring is generally a concern shared by all of an organization’s production models, \\nand thus ideally supported by a common framework or platform. This has the advantage of \\nmaking monitoring “free” for data scientists, that is, something they get the benefit of but \\ndon’t need to worry about building. While some models or applications may have unique \\nmonitoring or reporting requirements requiring the involvement of data scientists or ML \\nengineering staff, they should ideally be able to take advantage of low level “plumbing” that \\nis already in place.\\nMachine Learning at Scale\\nWhen an enterprise is just getting started with machine learning, it has few established ML \\npractices or processes. During this period, its data scientists are typically working in an ad \\nhoc manner to meet the immediate needs of their projects. Data acquisition and preparation, \\nas well as model training, deployment, and evaluation, are all done hand crafted with little \\nautomation or integration between steps.\\nOnce a team has operated this way for more than a handful of projects, it becomes clear that \\na great deal of effort is spent on repetitive tasks, or worse, on reinventing the wheel. For \\nexample, they may find themselves repeatedly copying the same data, performing the same \\ndata transformations, engineering the same features, or following the same deployment steps.\\nLeft to their own devices, individual data scientists or MLEs will build scripts or tools to help \\nautomate some of the more tedious aspects of the ML process. This can be an effective \\nstopgap, but left unplanned and uncoordinated, these efforts can be a source of distraction \\nand lead to technical debt.\\nFor organizations at a certain level of scale—typically when multiple machine learning teams \\nand their projects must be supported simultaneously—”data science platform” or “ML \\ninfrastructure” teams are established to drive efficiency and ensure that data scientists and \\nMLEs have access to the tools and resources they need to work efficiently.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 9}),\n",
       " Document(page_content='page 11 |  twimlai.com\\nAt Airbnb, for example, after gaining experience with applying machine learning to applications \\nlike search ranking, smart pricing, and fraud prevention, the company realized that it would \\nneed to dramatically increase the number of models it was putting into production in order \\nto meet its business goals. To enable this, an ML infrastructure team was established. The \\nteam’s mission is to eliminate what it calls the incidental complexity of machine learning—that \\nis, getting access to data, setting up servers, and scaling model training and inference—as \\nopposed to its intrinsic complexity—such as identifying the right model, selecting the right \\nfeatures, and tuning the model’s performance to meet business goals.\\nCommon challenges encountered by ML infrastructure teams include:\\nSimplifying and automating data access & management\\nBecause so much of model building involves acquiring and manipulating data, providing a \\nway to simplify and automate data acquisition and data transformation, feature engineering, \\nand ETL pipelines is necessary to increase modeling efficiency and ensure reproducibility.\\nData acquisition is greatly facilitated when a centralized data repository or directory is available, \\nsuch as a data lake, fabric, warehouse, or catalog. These enable efficient data storage, \\nmanagement, and discovery, allowing data that is generated from a variety of disparate \\nsystems to be more easily worked with by minimizing the time data scientists spend looking \\nfor data or trying to figure out how to access new systems.\\nData and feature transformations create new data needed for training and often inference. \\nThe data produced by these transformations is not typically saved back to the systems of \\norigin, such as transactional databases or log storage. Rather, it is persisted back to facilities \\nsuch as those mentioned above. Ideally, transformation and feature engineering pipelines are \\ncataloged for easy sharing across projects and teams. \\nIn addition to the efficiency benefits they offer, feature data repositories can also help eliminate \\ntime consuming and wasteful data replication when architected in a way that meets the \\nlatency and throughput requirements of a wide range of machine learning training and inference \\nworkloads. Because of the scalability, security, and other technical requirements of feature \\ndata repositories, ML infrastructure teams typically work with data engineers and corporate \\nIT to establish them. \\nDriving efficient resource use\\nToday, we have more raw computing power at our disposal than ever before. In addition, \\ninnovations such as high-density CPU cores, GPUs, TPUs, FPGAs, and other hardware \\naccelerators are increasingly targeting ML and DL workloads, promising a continued proliferation \\nof computing resources for these applications. ', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 10}),\n",
       " Document(page_content='page 12 |  twimlai.com\\nDespite declining computing costs, the machine learning process is so bursty and resource-\\nintensive that efficient use of available computing capacity is critical to supporting ML at scale. \\nThe following are key requirements for efficiently delivering compute to machine learning teams:\\n• Multitenancy. Establishing dedicated hardware environments for each machine learning \\nteam or workload is inefficient. Rather, the focus should be on creating shared environments \\nthat can support the training and deployment needs of multiple concurrent projects.\\n• Elasticity. Data preparation, model training, and model inference are all highly variable \\nworkloads, with the amount and type of resources they require often varying widely in \\ntime. To efficiently meet the needs of a portfolio of machine learning workloads it is \\nbest when the resources dedicated to individual tasks can be scaled up when needed, \\nand scaled back down when done.\\n• Immediacy. Data scientists and MLEs should have direct, self-service access to the \\nnumber and type of computing resources they need for training and testing models \\nwithout waiting for manual provisioning.\\n• Programmability. The creation, configuration, deployment and scaling of new \\nenvironments and workloads should be available via APIs to enable automated \\ninfrastructure provisioning and to maximize resource utilization.\\nThese are, of course, the characteristics of modern, cloud-based environments. However, \\nthis does not mean that we’re required to use the third-party “public” cloud services to do \\nmachine learning at scale.\\nWhile the public cloud’s operating characteristics make it a strong choice for running some \\nmachine learning workloads, there are often other considerations at play. Performance \\nrequirements often demand co-locating training and inference workloads with production \\napplications and data in order to minimize latency.\\nEconomics is an important consideration as well. The cost of renting computing infrastructure \\nin the cloud can be high, as can the cost of inbound and outbound data transfers, leading \\nmany organizations to choose local servers instead.\\nAs both cloud and on-premises resources have their place, hybrid cloud deployments that \\nharness resources from both are a worthy consideration for many organizations. Hybrid cloud \\ndeployment allows organizations to distribute workloads across cloud and on-premises \\nresources in ways that allow them to quickly and cost effectively meet fluctuating workload \\ndemands and provide increased computational power when needed.\\nUltimately, in a world of rapid hardware innovation, dynamic infrastructure economics, and \\nshifting workloads, it is prudent to build flexibility into new tools and platforms, so that they \\ncan be efficiently operated in any of these environments.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 11}),\n",
       " Document(page_content='page 13 |  twimlai.com\\nHiding complexity\\nWith the rise of Platform-as-a-Service (PaaS) offerings and DevOps automation tools, software \\ndevelopers gained the ability to operate at a higher level of abstraction, allowing them to focus \\non the applications they are building and not worry about the underlying infrastructure on \\nwhich their software runs.\\nSimilarly, in order for the machine learning process to operate at full scale and efficiency, data \\nscientists and MLEs must be able to focus on their models and data products rather than \\ninfrastructure. \\nThis is especially important because data products are built on a complex stack of rapidly \\nevolving technologies. These include more established tools like the TensorFlow and PyTorch \\ndeep learning frameworks; language-specific libraries like SciPy, NumPy and Pandas; and \\ndata processing engines like Spark and MapReduce. In addition, there has been an explosion \\nof specialty tools aiming to address specific pain-points in the machine learning process, \\nmany of which we discuss in our accompanying book, The Definitive Guide to Machine Learning \\nPlatforms, and cover in our online ML/AI Solutions Guide.\\nThese high-level tools are supported by a variety of low-level, vendor-provided drivers and \\nlibraries, allowing training and inference workloads to take advantage of hardware acceleration \\ncapabilities. Some of these driver stacks are notoriously difficult to correctly install and \\nconfigure, requiring that complex sets of interdependencies be satisfied.\\nManually managing rapidly churning software tools and the resulting web of dependencies \\ncan be a constant drain on data scientist productivity, and the source of hard-to-debug \\ndiscrepancies between results seen in training and production.\\nUnifying the ML workflow\\nUltimately, as an organization’s use of data science and machine learning matures, both \\nbusiness and technical stakeholders alike benefit from a unified ML workflow, with a common \\nframework for working with the organization’s data, experiments, models, and tools. \\nThe benefits of a common platform apply across the ML workflow. A unified view of data, as \\nwe’ve previously discussed, helps data scientists find the data they need to build models \\nmore quickly. A unified view of experiments helps individual users and teams identify what’s \\nworking faster, and helps managers understand how resources are being allocated. A unified \\nview of deployed models helps operations and DevOps teams monitor performance across \\na wide fleet of services. And a unified view of infrastructure helps data scientists more readily \\naccess the resources they need for training.\\nWith a unified approach to the machine learning workflow, it becomes much easier to facilitate \\nand manage cross-team collaboration, promote the reuse of existing resources, and take \\nadvantage of shared skills. It also enables individual team members to more quickly become \\nproductive when transitioning to new projects, driving increased efficiency and better overall \\noutcomes for data science and ML/AI projects.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 12}),\n",
       " Document(page_content='page 14 |  twimlai.com\\nEnter Containers and Kubernetes\\nOrganizations are increasingly turning to containers and Kubernetes to overcome the \\nchallenges of scaling their machine and deep learning efforts. \\nIt turns out we’ve seen this before. The introduction of Docker containers in 2013 initiated a \\ndramatic shift in the way software applications are developed and deployed by allowing \\nengineering teams to overcome a similar set of challenges to those faced by data scientists \\nand ML engineers. \\nContainer images provide a standardized, executable package that provides everything needed \\nto run an application, including its code, dependencies, tools, libraries and configuration files. \\nA running Docker container is an instantiation of a container image. \\nCompared with virtual machines, container images are lighter weight, easier to move between \\nenvironments, and faster to spin up. This is in large part because they can share the host \\noperating system’s (OS) kernel, as opposed to containing their own copy of a complete OS. \\nThe fact that lightweight containers could be reliably run across disparate computing \\nenvironments helped address many of the difficulties faced by software development and \\noperations organizations as they sought to modernize their software delivery processes, \\napplications, and infrastructure, leading to their ultimate popularity.\\nBut containers alone offer a partial solution. For organizations operating at scale, supporting \\nmultiple concurrent machine learning projects in various stages of development, it quickly \\nbecomes necessary to use multiple containers, make the services they offer easily accessible, \\nand connect them to a variety of external data sources. In this case, a container orchestration \\nplatform is required—in practice if not in theory—to efficiently manage the containers and \\ntheir interactions with one another and the outside world.\\nThat’s where Kubernetes comes in. Kubernetes is an open source container orchestration \\nplatform developed and open-sourced by Google in 2014. Kubernetes has since become the \\nde facto standard for container orchestration, due to its popularity among users, the flexibility \\nit confers to them, the support it has gained from cloud computing and software vendors, \\nand the degree of portability it offers between on-premises and public cloud environments.\\nKubernetes provides the necessary features required for complete lifecycle management of \\ncontainerized applications and services in a manner that has proved to be highly scalable \\nand reliable. \\nBefore exploring what this means for machine learning workloads, let’s learn a bit about \\nKubernetes itself.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 13}),\n",
       " Document(page_content='page 15 |  twimlai.com\\nGetting to Know Kubernetes \\nKubernetes takes a hierarchical approach to managing the various resources that it is \\nresponsible for, with each layer hiding the complexity beneath it. \\nThe highest-level concept in Kubernetes is the cluster. A Kubernetes cluster consists of at \\nleast one Kubernetes Master which controls multiple worker machines called nodes. Clusters \\nabstract their underlying computing resources, allowing users to deploy workloads to the \\ncluster, as opposed to on particular nodes.\\nA pod is a collection of one or more containers that share common configuration and are run \\non the same machine. It is the basic workload unit in Kubernetes. All containers within a pod \\nshare the same context, resources, and lifecycle. Resources include local and remote storage \\nvolumes and networking. All containers in a pod share an IP address and port space. To run \\ncontainers in pods, Kubernetes uses a container runtime, such as Docker, running on each node.\\nFigure 3. Basic Kubernetes architecture\\nThe master can be thought of as the “brain” of the cluster. It responds to cluster configuration \\nand management requests submitted via the Kubernetes client or API. It is responsible for \\ndetermining which pods are deployed to which nodes based on their respective requirements \\nand capabilities, a process called scheduling. The master maintains the overall health of the \\ncluster by re-scheduling pods in reaction to faults such as server failures. \\nNodes in a Kubernetes cluster run an agent called the kubelet that listens for instructions \\nfrom the master and creates, runs, and destroys containers accordingly. Collectively, the \\nmaster and the kubelet processes govern the operation of the cluster and are referred to as \\nthe Kubernetes Control Plane.Container s\\nHost OSPod\\nContainer sPodNode\\nKubernetes\\nMasterContainer runtime (Docker)Admi n\\nCLIAPI', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 14}),\n",
       " Document(page_content='page 16 |  twimlai.com\\nTo run a workload on a Kubernetes cluster, the user provides a plan that defines which pods \\nto create and how to manage them. This plan can be specified via configuration documents \\nwhich are sent to the cluster via Kubernetes’ APIs or client libraries. In this way, Kubernetes \\nis said to be a “declarative” system; users declare the state of the system they want, and \\nKubernetes tries to affect that state given the resources at its disposal.\\nTo run a workload on a Kubernetes cluster, the user provides a plan that defines which pods \\nto create and how to manage them. This plan can be specified via configuration documents \\nwhich are sent to the cluster via Kubernetes’ APIs or client libraries. In this way, Kubernetes \\nis said to be a “declarative” system; users declare the state of the system they want, and \\nKubernetes tries to affect that state given the resources at its disposal.\\nWhen the master receives a new plan, it examines its requirements and compares them to \\nthe current state of the system. The master then takes the actions required to converge the \\nobserved and desired states. When pods are scheduled to a node, the node pulls the appropriate \\ncontainer images from an image registry and coordinates with the local container runtime to \\nlaunch the container.\\nFiles created within a container are ephemeral, meaning they don’t persist beyond the lifetime \\nof the container. This presents critical issues when building real-world applications in general \\nand with machine learning systems in particular. \\nFirst, most applications are stateful in some way. This means they store data about the requests \\nthey receive and the work they perform as those requests are processed. An example might \\nbe a long training job that stores intermediate checkpoints so that the job doesn’t need to \\nstart over from the beginning should a failure occur. If this intermediate data were stored in a \\ncontainer, it would not be available if a container fails or needs to be moved to another machine. \\nNext, many applications must access and process existing data. This is certainly the case \\nwhen building machine learning models, which requires that training data be accessible during \\ntraining. If our only option for data access was files created within a container, we would need \\nto copy data into each container that needed it, which wouldn’t be very practical.\\nTo address these issues, container runtimes like Docker provide mechanisms to attach \\npersistent storage to the containers they manage. However, these mechanisms are tied to \\nthe scope of a single container, are limited in capability, and lack flexibility.\\nKubernetes supports several abstractions designed to address the shortcomings of container-\\nbased storage mechanisms. Volumes allow data to be shared by all containers within a pod \\nand remain available until the pod is terminated. \\nPersistent volumes are volumes whose lifecycle is managed at the cluster level as opposed \\nto the pod level. Persistent volumes provide a way for cluster administrators to centrally \\nconfigure and maintain connections to external data sources and provide a mechanism for \\ngranting pods access to them. ', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 15}),\n",
       " Document(page_content='page 17 |  twimlai.com\\nFigure 4. Kubernetes volumes provide a flexible abstraction for accessing  \\n a wide variety of storage systems from containerized workloads\\nKubernetes has long supported first- and third-party volume plugins allowing users to access \\ndifferent types of storage resources. More recently, the general availability of support for the \\nContainer Storage Interface (CSI) specification has significantly expanded the ability to access \\na wide variety of software and hardware-based storage systems via community- and vendor-\\nprovided drivers.\\nBeyond CSI, Kubernetes offers additional capabilities that allow users to customize their \\ndeployments and support complex application scenarios, including:\\n• Custom resources, which extend the Kubernetes API to manage new types of objects\\n• Operators, which combine custom resources and controllers to manage stateful \\napplications and systems\\n• Scheduler extensions, which can be used to change the way Kubernetes manages pods \\nand assigns them to nodes\\n• The Container Network Interface (CNI)  which provides a common interface between \\ncontainers and the networking layer\\n• Device plugins, which allow Kubernetes nodes to discover new types of hardware \\nresources and perform vendor specific initialization and setup. Once made discoverable, \\nthe availability of these resources can then be considered by the Kubernetes scheduler \\nwhen making pod placement decisions.!\"#$%&#\\'()\\n!\"#$%&#\\'()\\nContainersPodCluster\\nCloud\\nStorag eData Lak e Files Database/\\nData warehous eVolume\\nPersistent\\nVolume!\"#$%&#\\'()\\n!\"#$%&#\\'()\\nContainersPod\\nVolume Volume\\nPersistent\\nVolume', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 16}),\n",
       " Document(page_content='page 18 |  twimlai.com\\n“Deep learning is an empirical science, and the quality of a group’s infrastructure \\nis a multiplier on progress. Fortunately, today’s open-source ecosystem makes it \\npossible for anyone to build great deep learning infrastructure.”  \\nOpenAI\\nKubernetes for Machine and Deep Learning\\nWhile the challenges faced by developers building traditional software are similar to those \\nfaced by data scientists and ML engineers, they aren’t the same. In many ways, those faced \\nby the latter are greater, significantly exacerbated by the highly iterative nature of the machine \\nlearning workflow. \\nStill, the capabilities provided by containers and Kubernetes can help organizations address \\nthe challenges presented by the machine learning process.\\nFigure 5. Containers and Kubernetes address major ML/DL challenges\\nData acquisition and preparation\\nBy providing a mechanism for connecting storage to containerized workloads, the Kubernetes \\nCSI enables support for a wide variety of stateful applications, including machine and deep \\nlearning. CSI drivers provide direct access to many different file, object, and block storage \\nsystems, as well as data fabric products which provide unified access to data stored in a \\nvariety of enterprise systems.Simplifying  \\nData ManagementDriving Efficient  \\nResource UseHiding  \\nComplexity\\nKubernetes provides  \\nconnectors for diverse  \\ndata sources and manages \\nvolume lifecycle\\nData workflow & pipeline \\nabstractions (3rd party) \\nallow complex data \\ntransformations to be  \\nexecuted across cluster\\nData fabrics (3rd party)  \\nextend scalable, multi-format \\nstorage to clusterKubernetes provides elasticity, \\nallowing cluster and workloads  \\nto be easily scaled up/down\\nMultitenancy allows multiple \\nteams to share cluster\\nUsers can deploy workloads \\non-demand via CLI, API  \\nwithout IT intervention\\nUsers and tools can  \\nprogram ma tically deploy \\nand control workloadsContainers provide a convenient \\nformat for packaging workloads \\nand declaring dependencies\\nKubernetes abstracts \\ninfrastructure, allowing users  \\nto think of cluster as  \\nunit of compute', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 17}),\n",
       " Document(page_content='page 19 |  twimlai.com\\nReady access to scalable storage is the foundation upon which users can construct automated \\nand repeatable data processing pipelines. This ensures the ability to reliably deliver properly \\ntransformed data to models, without manual intervention, and also facilitates the reuse of \\ntransformed data and processed features across projects.\\nDriving efficient resource use\\nFor resource-intensive workloads like machine learning, users face the constant conundrum \\nof over-allocation or under-utilization. That is, they must manage the fine balance between \\neither having more resources than is needed at any particular point in time, or not enough. \\nKubernetes offers two solutions to this problem. The first is multi-tenancy. A single physical \\nKubernetes cluster can be partitioned into multiple virtual clusters using Kubernetes’ \\nnamespaces feature. This allows a single cluster to easily support multiple teams, projects, \\nor functions. \\nEach namespace can be configured with its own resource quotas and access control policies, \\nmaking it possible to specify the actions a user can perform and the resources they have \\naccess to. Namespace-level configuration allows resources like storage to be configured on \\na per-team, -project, or -function basis.\\nMulti-tenancy features allow many users to share the same physical resources, with the \\naggregate usage having less variance and requiring less over-provisioning. Another solution \\nKubernetes offers to the resource efficiency problem is auto-scaling, of which there are  \\ntwo types. \\nWorkload (or pod) autoscaling which is the ability to scale up the number of pods running a \\ngiven workload as needed. This works with multi-tenancy to take full advantage of the resources \\navailable in a given Kubernetes cluster\\nCluster autoscaling is the ability to scale up or down the number of nodes in the entire cluster \\nwithout disrupting the cluster or any of its applications or users. This is helpful, but of limited \\nutility, when you’re running the cluster on physical nodes in your own datacenter. When you’re \\nrunning Kubernetes on virtual servers in the cloud, though, this allows you to continually \\nright-size your cluster so you’re only paying for the resources you need at any given time.\\nBecause Kubernetes-enabled workloads can be run on any Kubernetes cluster in any location \\nwithout any changes to the application’s code, users can move applications between \\ndeployment environments with relative ease. In theory a single Kubernetes cluster can span \\nclouds and/or on-premises data centers, but this is difficult to implement and not recommended.\\nHiding complexity\\nContainers provide an efficient way of packaging machine learning workloads that is \\nindependent of language or framework. Kubernetes builds on this to provide a reliable \\nabstraction layer for managing containerized workloads and provides the necessary \\nconfiguration options, APIs, and tools to manipulate these workloads declaratively.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 18}),\n",
       " Document(page_content='page 20 |  twimlai.com\\nWhile working with containers and containerized workloads may be a new skill for many data \\nscientists, these tools will be familiar to machine learning engineers exposed to modern \\nsoftware development practices. \\nCustom and third-party user interfaces, command-line tools, or integration with source code \\nrepositories can simplify creating and managing containers for end users. Because Kubernetes \\nis workload independent, once users are familiar with the tools required to work with their \\nKubernetes environment, they can apply these tools across a wide range of projects.\\nWhile introducing its own set of tools, the use of containers to facilitate data acquisition, \\nexperiment management and model deployment ensures that the correct and consistent \\ndependencies are in place wherever jobs are run—whether on the developer laptop, training \\nenvironment, or production cluster. This provides the valuable benefit of shielding those \\nworkloads from the complexity of the underlying technology stack, while supporting a wide \\nvariety of runtime configurations.\\nKubernetes also helps support and enforce a separation of concerns, giving end-users control \\nover defining their workloads and administrators control over the properties and configuration \\nof the infrastructure.\\nUnifying the ML workflow\\nFor all its benefits, Kubernetes alone won’t unify the end-to-end machine learning process. \\nIt will go a long way, however, in doing so for much of the software and hardware infrastructure \\nthat supports machine learning, and, in achieving that, it provides a strong platform for tools \\nthat go even further.\\nMLOps on Kubernetes with Kubeflow\\nKubeflow is an open source project designed to make machine learning workflows on \\nKubernetes simple, portable and scalable. Kubeflow is sponsored by Google and inspired by \\nTensorFlow Extended, or TFX, the company’s internal machine learning platform.\\nOriginally intended to simply allow TensorFlow users to run training jobs on their Kubernetes \\nclusters, the project now integrates a broad set of tools in support of many steps in an end-\\nto-end machine learning process.\\nKubeflow includes components for:\\n• Launching Jupyter Notebooks. Kubeflow includes support for setting up Jupyter notebook \\nservers and creating and managing notebooks. Notebooks launched via Kubeflow are \\nintegrated with the authentication and access control policies an organization has \\nestablished with its Kubernetes deployment, are easier to share across the organization, \\ncan help organizations standardize notebook images, and can readily take advantage \\nof the compute capacity and other features available in the Kubernetes environment.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 19}),\n",
       " Document(page_content='page 21 |  twimlai.com\\n• Building ML Pipelines. Kubeflow Pipelines is a platform for building and deploying \\ncomplex, repeatable ML workflows on Kubernetes. Pipelines themselves consist of a \\ngraph detailing the relationships between steps in a workflow. The workflow steps \\nthemselves are called components, and are provided by the user as Docker containers. \\nPipelines allow users to package up the various data access and transformation, model \\ntraining, and model evaluation steps that make up an experiment, simplifying \\nexperimentation and re-use.\\n• Training Models. Kubeflow provides a variety of Kubernetes Operators allowing users \\nto easily create and manage training jobs for ML frameworks like TensorFlow, PyTorch, \\nMXNet, XGBoost. In addition to the distributed training support provided by these \\nframeworks, it also supports MPI-based distributed training through the MPI Operator.\\n• Tracking Experiment Metadata . The Kubeflow Metadata project provides a Python SDK \\nfor tracking and managing information about the executions (runs), models, datasets, \\nand other artifacts generated during machine learning. It also provides a UI for viewing \\na list of logged artifacts and their details.\\n• Hyperparameter Tuning. Tuning model hyperparameters is an important element of \\noptimizing model performance and accuracy, yet manually adjusting hyperparameters \\nduring experimentation is tedious. Kubeflow includes Katib, an open-source hyperparameter \\ntuning project inspired by Google’s internal black-box optimization service, Vizier. Katib \\nis framework agnostic and deploys optimization tasks as Kubernetes pods.\\n• Serving Models. Users may choose from two model serving systems within Kubeflow— \\nKFServing and Seldon Core—as well as any of several standalone model serving systems. \\nThese options support a wide variety of ML frameworks, and offer features supporting \\nvarious strategies for model rollout, scaling, logging, and analytics.\\nFigure 6. Kubeflow builds on top of Kubernetes to address key challenges in the ML workflow\\nHyper-\\nparameter\\nTuning\\nJupyter\\nNotebooks\\nModel\\nServing\\nDashboard\\nFairing\\nPipelines\\nMetadata\\nStore\\nTraining\\nServerless\\nkubernetes', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 20}),\n",
       " Document(page_content='page 22 |  twimlai.com\\nIn addition, Kubeflow provides a central dashboard through which users can access and \\nmanage the various components of a Kubeflow deployment, support for multi-tenancy and \\nmulti-user isolation, and through Kubeflow Fairing, the ability to more easily build, train, and \\ndeploy training jobs across a hybrid cloud environment directly from a Jupyter notebook.\\nKubeflow exposes Kubernetes concepts like namespaces and pods to users and requires \\nthat they use command-line tools to perform tasks like configuring and submitting jobs. As \\na result, it is more geared towards users with engineering skill sets, rather than those of the \\ntraditional data scientist. The requisite job configuration files are not altogether unfamiliar, \\nhowever, with their configuration parameters resembling those of other distributed tools they \\nmay be familiar with, like Apache Spark.\\nFurthermore, the project is making progress in this regard, with the dashboard and pipeline \\nvisualizations representing solid progress. Some users choose to build custom user interfaces \\nfor their data scientists that aim to further abstract the underlying Kubeflow and Kubernetes \\ntooling.\\nUsing Kubeflow allows users to save time integrating support for common ML and DL \\ncomponents. In the past, Kubeflow’s integrations felt rather shallow, and the project felt a bit \\nlike a hodgepodge of third-party open source offerings. To its credit, the project is now a much \\nmore highly curated and unified offering.\\nThat said, Kubeflow is still a collection of components under the covers. Though the Kubeflow \\ncore has attained GA status, the overall project remains immature, with many of the \\naforementioned components still in beta. This state of affairs may prove problematic for some \\norganizations, who value the stability and predictability of more mature and tightly integrated \\nofferings. Further, some enterprise IT organizations might find that Kubeflow lacks an adequate \\nsecurity and governance framework that spans workloads, data and infrastructure, though \\nprogress is being made in this regard.\\nOn the topic of governance, though a different kind, there are concerns in some circles with \\nKubeflow’s lack of independent governance. While the Kubernetes project is overseen by the \\nCNCF, which provides for a certain level of transparency and community-driven governance, \\nKubeflow, remains under Google’s control. This creates some uncertainty about the future \\ndirection of the project, though the project does have a broad base of contributors and advocates.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 21}),\n",
       " Document(page_content='page 23 |  twimlai.com\\nThe Kubernetes MLOps Ecosystem\\nKubeflow is one of the many available offerings that take advantage of Kubernetes to deliver \\nMLOps capabilities. These offerings range in scope from narrow tools that aim to address \\nparticular pain-points experienced by data scientists, to end-to-end platforms that seek to \\nprovide comprehensive support for the ML workflow.    Licensing and delivery models vary \\nas well, including open source projects, commercially supported software products, and \\ncloud-based services.\\nThe ML/AI tools, infrastructure, and platforms landscape continues to evolve rapidly. For this \\nreason we have developed the TWIML ML/AI Solutions Guide , an online resource available \\nat twimlai.com/solutions.\\nVisit the Solutions Guide for profiles of ML/AI tools and platforms supporting Kubernetes and \\nKubeflow, and check back frequently for updates as we expand its coverage and features.\\nDriving Data Science Innovation with \\nKubernetes: Volvo Cars’ Journey\\nSweden-based Volvo Cars is a luxury automobile manufacturer whose brand has long been \\nassociated with safety and innovation. The company’s Consumer Digital group  is responsible \\nfor a variety of products that aim to enhance the owner experience, including the Volvo On \\nCall app, which offers features like roadside assistance, remote climate control, and the ability \\nto lock and unlock owners’ cars from a distance. \\nData scientists in the Consumer Digital group use machine learning to both deliver end-user \\nfeatures, like search and recommendations, as well as to better understand how their products \\nare being used.\\nThe group’s data scientists began using Kubernetes to support their workloads in 2018, when \\nthey needed a way to automate data acquisition, transformation, and reporting pipelines in \\nsupport of a high-profile collaboration with Amazon to allow Volvo owners to receive in-trunk \\ndeliveries. The team wanted to build these pipelines using Jupyter Notebooks, and found no \\noff-the-shelf tools at the company that let them easily do this. One of the most important distinctions among the various tools \\navailable to help you build out your organization’s machine learning \\nplatform is whether the tool aims to be wide or deep. This distinction, \\nand the interesting paradox that arises as a result, is explored in \\ndetail in TWIML ’s The Definitive Guide to Machine Learning Platforms.\\n THE DEFINITIVE GUIDE TO\\nMachine Learning \\nPlatforms\\nBy Sam Charrington\\nFounder, TWIML  \\nHost, TWIML AI Podcast\\nSPONSORED BY', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 22}),\n",
       " Document(page_content='page 24 |  twimlai.com\\nInspired by work done at Netflix6,  Leonard Aukea, senior data scientist and tech lead for the \\ncompany’s data science platform, set out to build a notebook-centric data science workflow \\nfor his team. \\nThe Netflix system, though container-based, used that company’s homegrown container \\norchestration system. At Volvo, however, Kubernetes was already gaining traction to support \\ncontainerized workloads. With the help of a DevOps colleague and considerable research, \\nLeonard was able to get a Kubernetes cluster up and running for data science workloads.\\nThe team’s first step was to set up the Apache Airflow workflow management system on \\nKubernetes, and use this in conjunction with the Netflix Papermill tool to run notebooks as \\nsteps of a pipeline. Pipeline outputs were fed into Nteract, another Netflix tool, providing a \\nconvenient, notebook-based dashboard.\\nAutomated pipelines were a big win for Leonard’s team, and other teams soon reached out \\nfor help with their own workloads. It was then that the limitations of the team’s initial approach \\nbecame apparent.\\nOne challenge was that the elements of the system weren’t tightly integrated. Users had to \\nvisit several different internal web sites to get anything done. \\n“Here, you visualized the notebook; here, you specify the DAG in this GitHub repository; here, \\nyou go to the Airflow UI; and then here, you go to JupyterHub. It became a mess. It was hard \\nto explain... and it didn’t scale in the way that we wanted.”\\nAfter obtaining the go-ahead to put a more enterprise-ready solution in place, Leonard \\neventually found the Kubeflow project.\\nThe team at Volvo appreciated that Kubeflow is Kubernetes-native, so everything fit together \\nunder a unified user interface and set of APIs. \\nKubeflow “glues all of these services together and gives a unified interface that a user could \\ngo in and enter the platform and do their job in an easier fashion.”\\n“Some things were working quite nicely... in terms of setup, and then moving from one version \\nto another, and just reliability of [the] service.”\\nThat didn’t mean that everything was perfect, however.\\nThe team found Kubeflow “hard to configure because you need to configure all of these \\nindividual services, dig into the manifests that you’re actually deploying, and [deploy them \\nusing the] kfctl tool. I’m not a hundred percent a fan of it… You have to spend significant [effort] \\nin order to make it work.”\\nThe integrated nature of the tool put more responsibility on Leonard and his team when it \\ncame to support.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 23}),\n",
       " Document(page_content='page 25 |  twimlai.com\\nThey often found themselves needing to “dig deeper into the individual applications and look \\nat source code, for example, where documentation is not centralized. [...] It’s also not what \\nyou would traditionally expect from a service in that sense.”\\nAs of this writing, Leonard’s data science platforms team has Kubeflow running in beta with \\nseveral teams on board. Adoption has been relatively smooth, though the team acknowledges \\na steep learning curve. The use of Docker containers and Kubeflow’s tooling is more natural \\nfor the Python data scientists they support than for the company’s R-based data scientists.\\nUltimately, the team sees in Kubeflow a way to help the company’s data scientists move more \\nquickly and be more self-sufficient. They want data science teams that deploy models to be \\nresponsible for them throughout their lifecycle, but also to have access to the tools and training \\nand best practices that are needed for them to be successful.\\nThe platform team is currently gathering statistics to compare user productivity on the new \\nplatform with previous results. \\n“I don’t have actual numbers on it, but what I hear from other teams and also [my own use], \\nit’s way more effective when you get the hang of it.”\\n“I’m hoping that we have a lot more models in production that would otherwise go to waste \\nor have been laying stagnant for a while in the organization because [a team] has not had the \\navailable tooling or the support in order to get their model to production. This could be very \\nvaluable for Volvo Cars as a company.”\\nGetting Started\\nMeeting the infrastructure needs of machine and deep learning development represents a \\nsignificant challenge for organizations moving beyond one-off ML projects to scaling the use \\nof AI more broadly in the enterprise.\\nFortunately, enterprises starting down this path need not start from scratch. The pairing of \\ncontainer technologies and Kubernetes addresses many of the key infrastructure challenges \\nfaced by organizations seeking to industrialize their use of machine learning. Together, they \\nprovide a means for delivering scalable data management and automation, the efficient \\nutilization of resources, and an effective abstraction between the concerns of data scientists \\nand MLEs and the needs of those providing the infrastructure upon which they depend.\\nIn addition to supplying many of the features required of a robust machine learning infrastructure \\nplatform right out of the box, the open source Kubernetes project—with its broad adoption, \\nactive community, plentiful third-party tools ecosystem, and multiple commercial support \\noptions—also checks the most important boxes for executives and business decision-makers \\ninvesting in a platform for future growth.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 24}),\n",
       " Document(page_content='page 26 |  twimlai.com\\nThis is a great time to be getting started with Kubernetes and MLOps. The community of ML/\\nAI-focused Kubernetes users is large, growing, and welcoming; the platform and tools are \\nviable for production workloads and maturing quickly; and there is a growing pool of talent \\nwith experience in the requisite tech stack. (And, conversely, the skills you learn helping your \\norganization scale its MLOps platform are portable and valued in the marketplace.)\\nWhile powerful, Kubernetes brings with it a degree of complexity that can be challenging for \\nsmall data science teams. It is most appropriate for those organizations whose scale, resources, \\nand requirements justify the investment not only in a centralized platform for data science \\nbut also a dedicated team to support it.\\nToday, organizations adopting Kubernetes for machine learning and AI have the benefit of \\nmany options to help mitigate its complexity. Each of the layers of the MLOps stack—from \\nthe physical infrastructure such as servers, network and storage; to the Kubernetes control \\nplane; to MLOps software providing abstractions such as pipelines and experiments—can be \\nimplemented on-premises using off-the-shelf components or can be obtained as a fully \\nmanaged service by various vendors.\\nAs with any new technology, the best course of action is to research and understand your \\nrequirements and options and start small, with your feet grounded firmly in your current needs \\nbut with an eye towards the future. Your exploration of Kubernetes can start with a “cluster” \\nrunning on your laptop or a handful of machines in the cloud, allowing you to experience and \\nappreciate firsthand what it brings to the table for your machine learning workloads. From \\nthere you can grow as your experience, needs, and resources allow.\\nGodspeed.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 25}),\n",
       " Document(page_content='HPE ML OPS AND INTEL:   \\nPARTNERING TO DELIVER ENTERPRISE MACHINE LEARNING AT SCALE\\nHewlett Packard Enterprise empowers large enterprises to overcome the barriers in deploying and \\noperationalizing AI/ML across the organization. HPE ML Ops brings DevOps-like speed and agility to \\nthe ML lifecycle. The HPE ML Ops solution supports every stage of ML lifecycle—data preparation, model \\nbuild, model training, model deployment, collaboration, and monitoring. HPE ML Ops is an end-to-end \\ndata science solution with the flexibility to run on-premises, in multiple public clouds, or in a hybrid \\nmodel, allowing customers to respond to dynamic business requirements for a variety of use cases.\\nHPE ML Ops offers: \\nLearn more at hpe.com/info/MLOps.\\nHPE solutions take advantage of the latest Intel® technology to deliver the performance required to \\nscale machine learning workloads across the enterprise on known and trusted infrastructure.\\nIntel innovations include:\\n• Proven, world-class 2nd generation Intel® Xeon® Scalable processors deliver scalable performance \\nfor a wide variety of analytics and other enterprise applications. Featuring Intel® Optane™ persistent \\nmemory (PMem), this powerful combination offers increased memory capacity close to the CPU \\nto support in-memory workloads, along with native persistence enabling greater resilience and \\nproductivity.\\n• Intel® Deep Learning Boost brings new embedded performance acceleration for advanced \\nanalytics and AI workloads, with up to 30x performance improvement  for Inference workloads \\ncompared to the previous generation – laying the foundations for future evolution of your analytics \\nand AI capabilities. \\npage 27SPONSOR CONTENT\\nModel building Prepackaged, self-service sandbox environments\\nModel training Scalable training environments with secure access to big data\\nModel deployment Flexible, scalable, endpoint deployment \\nModel monitoring End-to-end visibility to track, measure, report model performance across the ML pipeline\\nCollaboration Enable CI/CD workflows with code, model, and project repositories\\nSecurity and control Secure multitenancy with integration to enterprise authentication mechanisms\\nHybrid deployment Deploy on-premises, public cloud, or hybrid HPE ML Ops\\nBuild Deploy Train Data PrepData Engineers Data Scientists ML Architects\\nOn-Premises Public  Clouds EdgeMonito r\\nCollaborate \\nHPE C ontainer Pl atform\\nMulti-tenant multi-cluster management for containerized applica tions —with pre-integrated data fabric and persistent container storage', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 26}),\n",
       " Document(page_content='page 28• A track record of contributions to Kubernetes and Kubeflow, including support for AVX instruction \\nset feature discovery, enhancements to the Kubernetes scheduling framework, contribution of \\nthe CRI Resource Manager, and development of the Kubernetes Topology Manager which enhances \\nsupport for high-performance Non-Uniform Memory Access (NUMA) Nodes.\\nJD offers an example of what’s possible when Intel® innovations are delivered at scale. JD.com is China’s \\nlargest retailer, with hyperscale operations driven by its own internal needs as well as those of its cloud \\ncomputing and AI customers. The company uses machine learning and AI in support of a number of \\nretail use cases, including advertising, warehouse automation and robotics, drone delivery, facial \\nrecognition based payments, and personalized marketing.\\nIn 2016, JD.com began development of JDOS, its customized and optimized version Kubernetes which \\ntoday supports a wide range of workloads and applications. The company runs the world’s largest \\nproduction Kubernetes cluster with over 20,000 nodes.\\nIntel worked closely with JD to optimize their big data and machine learning frameworks and toolkits, \\nsuch as Apache Spark on Kubernetes, TensorFlow, and others, which are exposed to users via a single \\nKubernetes-based architecture, which it calls Moonshot.\\nIntel and JD collaborated on a custom version of the next-generation Intel® Xeon® Scalable processor \\nto support JD’s unique JD Cloud and eCommerce workloads. Beyond the CPU, JD also incorporated \\nother Intel innovations such as Intel® Optane™ DC SSDs , and plans to adopt our upcoming Intel® Optane™ \\nDC Persistent Memory, a new class of storage technology offering the unprecedented combination of \\nhigh-capacity, affordability, and persistence.\\nThese customizations provide significant performance gains on analytics and AI workloads, and as a \\nresult help JD differentiate their cloud service offerings, personalize the online shopping experience \\nand improve supply chain efficiency. SPONSOR CONTENT\\nMOONSHOT ARCHITECTURE\\n', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 27}),\n",
       " Document(page_content='page 29 |  twimlai.com\\nAbout TWIML\\nMachine learning and artificial intelligence are dramatically changing how businesses operate \\nand people live. Through our publications, podcast, and community, TWIML brings leading \\nideas and minds from the world of ML and AI to a broad and influential community of data \\nscientists, engineers and tech-savvy business and IT leaders.\\nTWIML has its origins in This Week in Machine Learning & AI, a podcast we launched in 2016 \\nto a small but enthusiastic reception. Fast forward to 2020 and the TWIML AI Podcast is now \\na leading voice in the field, with over eight million downloads and a large and engaged community \\nfollowing. Our offerings now include research reports like this one, as well as online meetups \\nand study groups, conferences, and a variety of educational content.\\nThe TWIML AI Podcast remains at the heart of our mission. By sharing and amplifying the \\nvoices of a broad and diverse spectrum of machine learning and AI researchers, practitioners, \\nand innovators, our programs help make ML and AI more accessible, and enhance the lives \\nof our audience and their communities.\\nTWIML was founded by Sam Charrington, a sought after industry analyst, speaker, commentator \\nand thought leader. Sam’s research is focused on the business application of machine learning \\nand AI, bringing AI-powered products to market, and AI-enabled and -enabling technology \\nplatforms.\\nConnect with us:\\n                  sam@twimlai.com \\n                  @samcharrington \\n                  @twimlai\\n                  www.linkedin.com/in/samcharrington\\n                  twimlai.com ', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 28}),\n",
       " Document(page_content='page 30 |  twimlai.com\\n1. Jeremy Lewi, et al, “Kubeflow 0.3 Simplifies Setup & Improves ML Development,” Kubeflow Blog, \\nOctober 29, 2018, https://medium.com/kubeflow/kubeflow-0-3-simplifies-setup-improves-\\nml- development-98b8ca10bd69\\n2. D. Sculley, et al, “Hidden Technical Debt in Machine Learning Systems,” Advances in Neural \\nInformation Processing Systems, https://papers.nips.cc/paper/5656-hidden-technical-debt-\\nin-machine-learning-systems.pdf\\n3. William Koehrsen, “Prediction Engineering: How to Set Up Your Machine Learning Problem,” \\nTowards Data Science, November 7, 2018, https://towardsdatascience.com/prediction-\\nengineering-how-to-set-up-your-machine-learning-problem-b3b8f622683b\\n4. Steve Lohr, “For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights,” The New York \\nTimes, August 17, 2014, https://www.nytimes.com/2014/08/18/technology/for-big-data-\\nscientists-hurdle-to-insights-is-janitor-work.html\\n5. Norman Jouppi, et al., “In-Datacenter Performance Analysis of a Tensor Processing Unit,”  \\narXiv [cs.AR], https://arxiv.org/abs/1704.04760v1\\n6. Michelle Ufford, et al, “Beyond Interactive: Notebook Innovation at Netflix,” The Netflix Tech Blog, \\nAugust 16, 2018, https://netflixtechblog.com/notebook-innovation-591ee3221233References', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 29}),\n",
       " Document(page_content='SPONSORED BY\\nCopyright © 2020 CloudPulse Strategies. CloudPulse, TWiML, and \\nthe CloudPulse Strategies and TWiML logos are trademarks of \\nCloudPulse Strategies, LLC.\\nThis document makes descriptive reference to trademarks that \\nmay be owned by others. The use of such trademarks herein is \\nnot an assertion of ownership of such trademarks by CloudPulse \\nand is not intended to represent or imply the existence of an \\nassociation between CloudPulse and the lawful owners of such \\ntrademarks. Information regarding third-party products, services \\nand organizations was obtained from publicly available sources, \\nand CloudPulse cannot confirm the accuracy or reliability of such \\nsources or information. Its inclusion does not imply an endorsement \\nby or of any third party.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 30})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=read_doc(r'C:\\github_repos\\gemini-pdfs\\LLM Generic Application\\documents')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide The Docs Into Chunks\n",
    "def chunk_data(docs,chunk_size=800,chunk_overlap=50):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    doc=text_splitter.split_documents(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Kubernetes \\nfor MLOps: \\nScaling Enterprise Machine Learning, \\nDeep Learning, and AI\\nBy Sam Charrington\\nFounder, TWIML  \\nHost, TWIML AI PodcastSPONSORED BY\\nSPONSORED BY', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 0}),\n",
       " Document(page_content='page 2 |  twimlai.com\\nTable of Contents\\n Preface to the Second Edition .............................................................. 4\\n Introduction .......................................................................................... 5\\n The Machine Learning Process ............................................................ 6\\n Machine Learning at Scale ................................................................. 10\\n Enter Containers and Kubernetes ....................................................... 14\\n Getting to Know Kubernetes ............................................................... 15\\n Kubernetes for Machine and Deep Learning  ....................................... 18\\n MLOps on Kubernetes with Kubeflow ................................................. 20\\n The Kubernetes MLOps Ecosystem .................................................... 23\\n Case Study: Volvo Cars ....................................................................... 23\\n Getting Started ................................................................................... 25\\n About TWIML ...................................................................................... 29', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 1}),\n",
       " Document(page_content='page 3 |  twimlai.com\\nSponsors\\nThis copy of Kubernetes for MLOps: Scaling Enterprise Machine Learning, Deep Learning, and \\nAI is brought to you by HPE and Intel. We thank them for their support.\\nHewlett Packard Enterprise is the global edge-to-cloud platform-as-a-service company that \\nhelps organizations accelerate outcomes by unlocking value from all of their data, everywhere. \\nBuilt on decades of reimagining the future and innovating to advance the way we live and \\nwork, HPE delivers unique, open and intelligent technology solutions, with a consistent \\nexperience across all clouds and edges, to help customers develop new business models, \\nengage in new ways, and increase operational performance. For more information, visit:  \\nwww.hpe.com.\\nIntel is an industry leader, creating world-changing technology that enables global progress \\nand enriches lives. Inspired by Moore’s Law, we continuously work to advance the design and \\nmanufacturing of semiconductors to help address our customers’ greatest challenges. By \\nembedding intelligence in the cloud, network, edge and every kind of computing device, we \\nunleash the potential of data to transform business and society for the better. To learn more \\nabout Intel’s innovations, visit www.intel.com.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 2}),\n",
       " Document(page_content='page 4 |  twimlai.com\\nPreface to the Second Edition\\nIn December 2018, I raced to complete the first edition of this ebook in time for Kubecon \\nSeattle. After weeks of research, interviews, and writing, I was excited to unveil a timely and \\nneeded resource for a small but growing community.\\nI made the deadline, but as I sat through the conference’s keynotes and sessions, listening \\nto the many new announcements relating to machine learning (ML) and artificial intelligence \\n(AI) workloads on Kubernetes, my heart sank. While I knew the ebook would prove a valuable \\nresource to readers, I knew too that it would soon need an update.\\nSince that time, the use of Kubernetes to support an organization’s ML/AI projects has evolved \\nfrom an obscure idea to one being explored and pursued by many organizations facing the \\nchallenge of getting ML models into production and scaling them.\\nAs interest in running ML/AI workloads atop Kubernetes has grown, so has the surrounding \\necosystem. When the first edition was written, Kubeflow was yet in its infancy, having just \\nseen its 0.3 release.1 At the time, the Google-founded project was far from production ready, \\nand still fairly narrowly focused on running TensorFlow jobs on Kubernetes. \\nKubeflow has since published a general availability (GA) release of the project, and now aspires \\nto support a wide variety of end-to-end machine learning workflows on Kubernetes. The \\nannouncement of version 1.0 of the project in March of 2020 was accompanied by testimonials \\nfrom companies such as US Bank, Chase Commercial Bank, and Volvo Cars. (I’m excited to \\ninclude the first public profile of Volvo’s journey in this book.)\\nThe broader Kubernetes ML/AI ecosystem has exploded as well, with a wide variety of open \\nsource and commercial offerings building upon the foundation offered by Kubernetes. We’ll \\nmention a few of these in this book, but with so many services and offerings to keep track of, \\nand with offerings evolving so quickly, we’ve decided to move our directory of open source \\nand commercial tools to a new home: the online TWIML AI Solutions Guide.\\nFollowing the evolution of this space has been a fascinating journey, and I’m pleased to bring \\nyou the second edition of this ebook. My hope is that the information shared here will help \\nyou better understand this promising direction for building, deploying, and scaling machine \\nlearning models in your organization, and ultimately contribute to your organization’s own \\ninnovative use of machine learning and AI.\\nThanks for reading!\\nSam ', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 3}),\n",
       " Document(page_content='page 5 |  twimlai.com\\nIntroduction\\nEnterprise interest in machine learning and artificial intelligence continues to grow, with \\norganizations dedicating increasingly large teams and resources to ML/AI projects. As \\nbusinesses scale their investments, it becomes critical to build repeatable, efficient, and \\nsustainable processes for model development and deployment.\\nThe move to drive more consistent and efficient processes in machine learning parallels \\nefforts towards the same goals in software development. Whereas the latter has come to be \\ncalled DevOps, the former is increasingly referred to as MLOps.\\nWhile DevOps, and likewise MLOps, are principally about practices rather than technology, to \\nthe extent that those practices are focused on automation and repeatability, tools have been \\nan important contributor to their rise. In particular, the advent of container technologies like \\nDocker was a significant enabler of DevOps, allowing users to drive increased agility, efficiency, \\nmanageability, and scalability in their software development efforts. \\nContainers remain a foundational technology for both DevOps and MLOps. Containers provide \\na core piece of functionality that allow us to run a given piece of code—whether a notebook, \\nan experiment, or a deployed model—anywhere, without the “dependency hell” that plagues \\nother methods of sharing software. But, additional technology is required to scale containers \\nto support large teams, workloads, or applications. This technology is known as a container \\norchestration system, the most popular of which is Kubernetes.\\nIn this book we explore the role that Kubernetes plays in supporting MLOps:\\n• Chapter 1 presents a high-level overview of the machine learning process and the \\nchallenges organizations tend to encounter when it comes time to scale it.\\n• Chapter 2 elaborates on why supporting machine learning at scale is so difficult, and \\ncovers a few of the many challenges faced by data scientists and machine learning \\nengineers in getting their models into production, at scale.\\n• Chapter 3 provides an introduction to container technology and Kubernetes, and shows \\nhow they help address the challenges referenced earlier.\\n• In chapter 4, we zoom in to discuss Kubernetes in more detail.\\n• In chapter 5 , we return to the ML workflow, and show how Kubernetes can help \\norganizations overcome the various challenges it presents.\\n• Chapters 6 and 7 look at the role of Kubeflow and other higher-level tools in delivering \\nMLOps.\\n• In chapter 8 , we get an opportunity to sit in the passenger seat and ride along with Volvo \\nCars, an early Kubernetes and Kubeflow user, to learn from their experiences building \\na ML platform atop these technologies.\\n• Chapter 8 offers some brief advice on how you can get started with your own project.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 4}),\n",
       " Document(page_content='page 6 |  twimlai.com\\nIf you’re just getting started with MLOps, know that there’s no better time than the present to \\nbegin this journey. The user and practitioner communities are exploding, the practices and \\ntechnology are maturing, and the support and educational resources are more plentiful and \\nof higher quality with each passing day.\\nThe Machine Learning Process\\nBefore we can discuss enabling and scaling the delivery of machine learning, we must \\nunderstand the full scope of the machine learning process. While much of the industry dialogue \\naround machine learning continues to be focused on modeling, the reality is that modeling \\nis a small part of the process. This was poignantly illustrated in a 2015 paper2 by authors \\nfrom Google, and remains true today.\\nFigure 1. Illustration from Hidden Technical Debt in Machine Learning Systems paper.  \\nMachine learning models are represented by the small box labeled “ML Code” in the center of the diagram.  \\nThe paper refers to the supporting infrastructure as “vast and complex.”\\nIn order for organizations to reap the rewards of their machine learning efforts and achieve \\nany level of scale and efficiency, models must be developed within a repeatable process that \\naccounts for the critical activities that precede and follow model development. \\nWhen presented with a new problem to solve, data scientists will develop and run a series of \\nexperiments that take available data as input and aim to produce an acceptable model as \\noutput. Figure 2 presents a high-level view of the end-to-end ML workflow, representing the \\nthree major “steps” in the process—data acquisition and preparation, experiment management \\nand model development, and model deployment and performance monitoring—as well as \\nthe iterative loops that connect them. Conﬁguration Data CollectionData\\nVeriﬁcationMachine\\nResource\\nManagement\\nAnalysis Tools\\nProcess\\nManagement ToolsFeature\\nExtractionServing\\nInfrastructureMonitoring\\nML\\nCode', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 5}),\n",
       " Document(page_content='page 7 |  twimlai.com\\nFigure 2. The machine learning process\\nData acquisition & preparation\\nTo build high quality models, data scientists and machine learning engineers must have \\naccess to large quantities of high-quality labeled    training data. This data very rarely exists \\nin a single place or in a form directly usable by data scientists. Rather, in the vast majority of \\ncases, the training dataset must be built by data scientists, data engineers, machine learning \\nengineers, and business domain experts working together.\\nIn the real world, as opposed to in academia or Kaggle competitions, the creation of training \\ndatasets usually involves combining data from multiple sources. For example, a data scientist \\nbuilding a product recommendation model might build the training dataset by joining data \\nfrom web activity logs, search history, mobile interactions, product catalogs, and transactional \\nsystems.\\nWhile the specific use of labeled training datasets is characteristic of supervised learning, \\nas opposed to unsupervised, self-supervised or reinforcement learning, the former remains \\nthe most popular type of machine learning in use today. While this book occasionally refers \\nto labels and other supervised learning concepts, its key messages apply broadly.\\nOnce compiled, training data must then be prepared for model development. To do this, data \\nscientists will apply a series of transformations to the raw data to cleanse and normalize it. \\nExamples include removing corrupt records, filling in missing values, and correcting \\ninconsistencies like differing representations for states or countries.\\nTransformations may also be required to extract labels. For example, developing a model that \\npredicts the likelihood of churn among customers will require a label indicating which of the \\ncustomers in our transactional database are examples of churn. This can, in turn, require a \\ncomplex query against the data warehouse that considers factors such as the products or \\nservices that we are basing the prediction on, the number of days without a transaction, the \\nwindow in which we want to make predictions, and more.3 Data Acquisition &\\nPreparatio nExperiment Management &\\nModel Developmen t Model Deployment &\\nMonitorin g', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 6}),\n",
       " Document(page_content='page 8 |  twimlai.com\\nAs a result of the organizational and technical complexity involved, the process of acquiring \\nand preparing enterprise data can consume the vast majority of a data scientist’s effort on \\na given project—50 to 80 percent, according to some reports.4\\nIf not accounted for during planning or supported by appropriate processes and tools, data \\nacquisition can consume all of the time and resources allocated for a project, leaving little \\navailable for the remaining steps in the machine learning process. \\nExperiment management and model development\\nExperimentation is central to the machine learning process. During modeling, data scientists \\nand machine learning engineers (MLEs) run a series of experiments to identify a robust \\npredictive model. Typically, many models—possibly hundreds or even thousands—will be \\ntrained and evaluated in order to identify the techniques, architectures, learning algorithms, \\nand parameters that work best for a particular problem.\\nFeature engineering is the iterative process of creating the features and labels needed to train \\nthe model through a series of data transformations. Feature engineering is often performed \\nin lockstep with model training, because the ability to identify helpful features can have a \\nsignificant impact on the overall success of the modeling effort. That said, a fine line separates \\nthe preliminary data cleansing and normalization steps associated with data preparation from \\nfeature engineering.\\nSimple examples of feature engineering include generating derived features (such as calculating \\nan age from a birthdate) or converting categorical variables (such as transaction types) into \\none-hot encoded, or binary, vectors.\\nWith deep learning, features are usually straightforward because deep neural networks (DNNs) \\ngenerate their own internal transformations. With traditional machine learning, feature \\nengineering can be quite challenging and relies heavily on the creativity and experience of the \\ndata scientist and their understanding of the business domain or ability to effectively collaborate \\nwith domain experts.\\nIn addition, a variety of tools under the broad banner of “AutoML” are finding popular use to \\nautomate aspects of experimentation and model development. Offerings exist to automate \\nfeature engineering by creating and testing a wide variety of candidate features, identify the \\nbest machine learning or deep learning models (the latter referred to as neural architecture \\nsearch), automatically tune or optimize model hyperparameters, and automate the compression \\nof large DNNs so that they’re more amenable to mobile or edge deployment.\\nWhether manually performed or automated, the various aspects of experimentation and \\nmodel training can be quite computationally intensive, especially in the case of deep learning. \\nHaving ready access to the right infrastructure for each experiment, such as CPUs, GPUs, \\nmemory, etc.—whether locally or in the cloud—can significantly impact the speed and agility \\nof the data science team during the model development phase of the process.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 7}),\n",
       " Document(page_content='page 9 |  twimlai.com\\nModel deployment\\nOnce a model has been developed, it must be deployed in order to be used. While deployed \\nmodels can take many forms, typically the model is embedded directly into application code \\nor put behind an API of some sort. HTTP-based (i.e. REST or gRPC) APIs are increasingly \\nused so that developers can access model predictions as microservices.\\nWhile model training might require large bursts of computing hardware over the course of \\nseveral hours, days or weeks, model inference—making queries against models—can be even \\nmore computationally expensive than training over time. Each inference against a deployed \\nmodel requires a small but significant amount of computing power.    Unlike the demands of \\ntraining, the computational burden of inference scales with the number of inferences made \\nand continues for as long as the model is in production. \\nOr not so small in the case of some deep learning models. Google’s efforts to build its \\nTensor Processing Unit (TPU), a specialized chip for inference, began in 2013 when engineer \\nJeff Dean projected that if people were to use voice search for 3 minutes a day, meeting \\nthe inference demands of speech recognition would require Google data centers to double \\nin capacity.5 \\nMeeting the requirements of inference at scale is a classic systems engineering problem. \\nAddressing issues like scalability, availability, latency, and cost are typically primary concerns. \\nWhen mobile or edge deployment is required, additional considerations like processing cycles, \\nmemory, size, weight, and power consumption come into play.\\nAt a certain point, we often end up needing to turn to distributed systems to meet our scalability \\nor performance goals. This of course brings along its own set of challenges and operational \\nconsiderations. How do we get our model onto multiple machines and ensure consistency \\nover time. How do we update to new models without taking our applications out of service? \\nWhat happens when problems arise during an upgrade? How can we test new models on live \\ntraffic? \\nFortunately for those deploying models, much progress has been made addressing these \\nquestions for microservices and other software systems.\\nModel monitoring\\nOnce a model is put into production, it is important to monitor its ongoing performance. \\nMachine learning models are perishable, meaning that the statistical distribution of the data \\na model sees in production will inevitably start to drift away from that seen during training. \\nThis will cause model performance to degrade, which can result in negative business outcomes \\nsuch as lost sales or undetected fraudulent activity if not detected and addressed.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 8}),\n",
       " Document(page_content='page 10 |  twimlai.com\\nProduction models should be instrumented so that the inputs to and results of each inference \\nare logged, allowing usage to be reviewed and performance to be monitored on an ongoing \\nbasis. Owners of models experiencing degraded performance can use this information to \\ntake corrective action, such as retraining or re-tuning the model.\\nBusiness or regulatory requirements may impose additional requirements dictating the form \\nor function of model monitoring for audit or compliance purposes, including in some cases \\nthe ability to explain model decisions.\\nModel monitoring is generally a concern shared by all of an organization’s production models, \\nand thus ideally supported by a common framework or platform. This has the advantage of \\nmaking monitoring “free” for data scientists, that is, something they get the benefit of but \\ndon’t need to worry about building. While some models or applications may have unique \\nmonitoring or reporting requirements requiring the involvement of data scientists or ML \\nengineering staff, they should ideally be able to take advantage of low level “plumbing” that \\nis already in place.\\nMachine Learning at Scale\\nWhen an enterprise is just getting started with machine learning, it has few established ML \\npractices or processes. During this period, its data scientists are typically working in an ad \\nhoc manner to meet the immediate needs of their projects. Data acquisition and preparation, \\nas well as model training, deployment, and evaluation, are all done hand crafted with little \\nautomation or integration between steps.\\nOnce a team has operated this way for more than a handful of projects, it becomes clear that \\na great deal of effort is spent on repetitive tasks, or worse, on reinventing the wheel. For \\nexample, they may find themselves repeatedly copying the same data, performing the same \\ndata transformations, engineering the same features, or following the same deployment steps.\\nLeft to their own devices, individual data scientists or MLEs will build scripts or tools to help \\nautomate some of the more tedious aspects of the ML process. This can be an effective \\nstopgap, but left unplanned and uncoordinated, these efforts can be a source of distraction \\nand lead to technical debt.\\nFor organizations at a certain level of scale—typically when multiple machine learning teams \\nand their projects must be supported simultaneously—”data science platform” or “ML \\ninfrastructure” teams are established to drive efficiency and ensure that data scientists and \\nMLEs have access to the tools and resources they need to work efficiently.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 9}),\n",
       " Document(page_content='page 11 |  twimlai.com\\nAt Airbnb, for example, after gaining experience with applying machine learning to applications \\nlike search ranking, smart pricing, and fraud prevention, the company realized that it would \\nneed to dramatically increase the number of models it was putting into production in order \\nto meet its business goals. To enable this, an ML infrastructure team was established. The \\nteam’s mission is to eliminate what it calls the incidental complexity of machine learning—that \\nis, getting access to data, setting up servers, and scaling model training and inference—as \\nopposed to its intrinsic complexity—such as identifying the right model, selecting the right \\nfeatures, and tuning the model’s performance to meet business goals.\\nCommon challenges encountered by ML infrastructure teams include:\\nSimplifying and automating data access & management\\nBecause so much of model building involves acquiring and manipulating data, providing a \\nway to simplify and automate data acquisition and data transformation, feature engineering, \\nand ETL pipelines is necessary to increase modeling efficiency and ensure reproducibility.\\nData acquisition is greatly facilitated when a centralized data repository or directory is available, \\nsuch as a data lake, fabric, warehouse, or catalog. These enable efficient data storage, \\nmanagement, and discovery, allowing data that is generated from a variety of disparate \\nsystems to be more easily worked with by minimizing the time data scientists spend looking \\nfor data or trying to figure out how to access new systems.\\nData and feature transformations create new data needed for training and often inference. \\nThe data produced by these transformations is not typically saved back to the systems of \\norigin, such as transactional databases or log storage. Rather, it is persisted back to facilities \\nsuch as those mentioned above. Ideally, transformation and feature engineering pipelines are \\ncataloged for easy sharing across projects and teams. \\nIn addition to the efficiency benefits they offer, feature data repositories can also help eliminate \\ntime consuming and wasteful data replication when architected in a way that meets the \\nlatency and throughput requirements of a wide range of machine learning training and inference \\nworkloads. Because of the scalability, security, and other technical requirements of feature \\ndata repositories, ML infrastructure teams typically work with data engineers and corporate \\nIT to establish them. \\nDriving efficient resource use\\nToday, we have more raw computing power at our disposal than ever before. In addition, \\ninnovations such as high-density CPU cores, GPUs, TPUs, FPGAs, and other hardware \\naccelerators are increasingly targeting ML and DL workloads, promising a continued proliferation \\nof computing resources for these applications. ', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 10}),\n",
       " Document(page_content='page 12 |  twimlai.com\\nDespite declining computing costs, the machine learning process is so bursty and resource-\\nintensive that efficient use of available computing capacity is critical to supporting ML at scale. \\nThe following are key requirements for efficiently delivering compute to machine learning teams:\\n• Multitenancy. Establishing dedicated hardware environments for each machine learning \\nteam or workload is inefficient. Rather, the focus should be on creating shared environments \\nthat can support the training and deployment needs of multiple concurrent projects.\\n• Elasticity. Data preparation, model training, and model inference are all highly variable \\nworkloads, with the amount and type of resources they require often varying widely in \\ntime. To efficiently meet the needs of a portfolio of machine learning workloads it is \\nbest when the resources dedicated to individual tasks can be scaled up when needed, \\nand scaled back down when done.\\n• Immediacy. Data scientists and MLEs should have direct, self-service access to the \\nnumber and type of computing resources they need for training and testing models \\nwithout waiting for manual provisioning.\\n• Programmability. The creation, configuration, deployment and scaling of new \\nenvironments and workloads should be available via APIs to enable automated \\ninfrastructure provisioning and to maximize resource utilization.\\nThese are, of course, the characteristics of modern, cloud-based environments. However, \\nthis does not mean that we’re required to use the third-party “public” cloud services to do \\nmachine learning at scale.\\nWhile the public cloud’s operating characteristics make it a strong choice for running some \\nmachine learning workloads, there are often other considerations at play. Performance \\nrequirements often demand co-locating training and inference workloads with production \\napplications and data in order to minimize latency.\\nEconomics is an important consideration as well. The cost of renting computing infrastructure \\nin the cloud can be high, as can the cost of inbound and outbound data transfers, leading \\nmany organizations to choose local servers instead.\\nAs both cloud and on-premises resources have their place, hybrid cloud deployments that \\nharness resources from both are a worthy consideration for many organizations. Hybrid cloud \\ndeployment allows organizations to distribute workloads across cloud and on-premises \\nresources in ways that allow them to quickly and cost effectively meet fluctuating workload \\ndemands and provide increased computational power when needed.\\nUltimately, in a world of rapid hardware innovation, dynamic infrastructure economics, and \\nshifting workloads, it is prudent to build flexibility into new tools and platforms, so that they \\ncan be efficiently operated in any of these environments.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 11}),\n",
       " Document(page_content='page 13 |  twimlai.com\\nHiding complexity\\nWith the rise of Platform-as-a-Service (PaaS) offerings and DevOps automation tools, software \\ndevelopers gained the ability to operate at a higher level of abstraction, allowing them to focus \\non the applications they are building and not worry about the underlying infrastructure on \\nwhich their software runs.\\nSimilarly, in order for the machine learning process to operate at full scale and efficiency, data \\nscientists and MLEs must be able to focus on their models and data products rather than \\ninfrastructure. \\nThis is especially important because data products are built on a complex stack of rapidly \\nevolving technologies. These include more established tools like the TensorFlow and PyTorch \\ndeep learning frameworks; language-specific libraries like SciPy, NumPy and Pandas; and \\ndata processing engines like Spark and MapReduce. In addition, there has been an explosion \\nof specialty tools aiming to address specific pain-points in the machine learning process, \\nmany of which we discuss in our accompanying book, The Definitive Guide to Machine Learning \\nPlatforms, and cover in our online ML/AI Solutions Guide.\\nThese high-level tools are supported by a variety of low-level, vendor-provided drivers and \\nlibraries, allowing training and inference workloads to take advantage of hardware acceleration \\ncapabilities. Some of these driver stacks are notoriously difficult to correctly install and \\nconfigure, requiring that complex sets of interdependencies be satisfied.\\nManually managing rapidly churning software tools and the resulting web of dependencies \\ncan be a constant drain on data scientist productivity, and the source of hard-to-debug \\ndiscrepancies between results seen in training and production.\\nUnifying the ML workflow\\nUltimately, as an organization’s use of data science and machine learning matures, both \\nbusiness and technical stakeholders alike benefit from a unified ML workflow, with a common \\nframework for working with the organization’s data, experiments, models, and tools. \\nThe benefits of a common platform apply across the ML workflow. A unified view of data, as \\nwe’ve previously discussed, helps data scientists find the data they need to build models \\nmore quickly. A unified view of experiments helps individual users and teams identify what’s \\nworking faster, and helps managers understand how resources are being allocated. A unified \\nview of deployed models helps operations and DevOps teams monitor performance across \\na wide fleet of services. And a unified view of infrastructure helps data scientists more readily \\naccess the resources they need for training.\\nWith a unified approach to the machine learning workflow, it becomes much easier to facilitate \\nand manage cross-team collaboration, promote the reuse of existing resources, and take \\nadvantage of shared skills. It also enables individual team members to more quickly become \\nproductive when transitioning to new projects, driving increased efficiency and better overall \\noutcomes for data science and ML/AI projects.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 12}),\n",
       " Document(page_content='page 14 |  twimlai.com\\nEnter Containers and Kubernetes\\nOrganizations are increasingly turning to containers and Kubernetes to overcome the \\nchallenges of scaling their machine and deep learning efforts. \\nIt turns out we’ve seen this before. The introduction of Docker containers in 2013 initiated a \\ndramatic shift in the way software applications are developed and deployed by allowing \\nengineering teams to overcome a similar set of challenges to those faced by data scientists \\nand ML engineers. \\nContainer images provide a standardized, executable package that provides everything needed \\nto run an application, including its code, dependencies, tools, libraries and configuration files. \\nA running Docker container is an instantiation of a container image. \\nCompared with virtual machines, container images are lighter weight, easier to move between \\nenvironments, and faster to spin up. This is in large part because they can share the host \\noperating system’s (OS) kernel, as opposed to containing their own copy of a complete OS. \\nThe fact that lightweight containers could be reliably run across disparate computing \\nenvironments helped address many of the difficulties faced by software development and \\noperations organizations as they sought to modernize their software delivery processes, \\napplications, and infrastructure, leading to their ultimate popularity.\\nBut containers alone offer a partial solution. For organizations operating at scale, supporting \\nmultiple concurrent machine learning projects in various stages of development, it quickly \\nbecomes necessary to use multiple containers, make the services they offer easily accessible, \\nand connect them to a variety of external data sources. In this case, a container orchestration \\nplatform is required—in practice if not in theory—to efficiently manage the containers and \\ntheir interactions with one another and the outside world.\\nThat’s where Kubernetes comes in. Kubernetes is an open source container orchestration \\nplatform developed and open-sourced by Google in 2014. Kubernetes has since become the \\nde facto standard for container orchestration, due to its popularity among users, the flexibility \\nit confers to them, the support it has gained from cloud computing and software vendors, \\nand the degree of portability it offers between on-premises and public cloud environments.\\nKubernetes provides the necessary features required for complete lifecycle management of \\ncontainerized applications and services in a manner that has proved to be highly scalable \\nand reliable. \\nBefore exploring what this means for machine learning workloads, let’s learn a bit about \\nKubernetes itself.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 13}),\n",
       " Document(page_content='page 15 |  twimlai.com\\nGetting to Know Kubernetes \\nKubernetes takes a hierarchical approach to managing the various resources that it is \\nresponsible for, with each layer hiding the complexity beneath it. \\nThe highest-level concept in Kubernetes is the cluster. A Kubernetes cluster consists of at \\nleast one Kubernetes Master which controls multiple worker machines called nodes. Clusters \\nabstract their underlying computing resources, allowing users to deploy workloads to the \\ncluster, as opposed to on particular nodes.\\nA pod is a collection of one or more containers that share common configuration and are run \\non the same machine. It is the basic workload unit in Kubernetes. All containers within a pod \\nshare the same context, resources, and lifecycle. Resources include local and remote storage \\nvolumes and networking. All containers in a pod share an IP address and port space. To run \\ncontainers in pods, Kubernetes uses a container runtime, such as Docker, running on each node.\\nFigure 3. Basic Kubernetes architecture\\nThe master can be thought of as the “brain” of the cluster. It responds to cluster configuration \\nand management requests submitted via the Kubernetes client or API. It is responsible for \\ndetermining which pods are deployed to which nodes based on their respective requirements \\nand capabilities, a process called scheduling. The master maintains the overall health of the \\ncluster by re-scheduling pods in reaction to faults such as server failures. \\nNodes in a Kubernetes cluster run an agent called the kubelet that listens for instructions \\nfrom the master and creates, runs, and destroys containers accordingly. Collectively, the \\nmaster and the kubelet processes govern the operation of the cluster and are referred to as \\nthe Kubernetes Control Plane.Container s\\nHost OSPod\\nContainer sPodNode\\nKubernetes\\nMasterContainer runtime (Docker)Admi n\\nCLIAPI', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 14}),\n",
       " Document(page_content='page 16 |  twimlai.com\\nTo run a workload on a Kubernetes cluster, the user provides a plan that defines which pods \\nto create and how to manage them. This plan can be specified via configuration documents \\nwhich are sent to the cluster via Kubernetes’ APIs or client libraries. In this way, Kubernetes \\nis said to be a “declarative” system; users declare the state of the system they want, and \\nKubernetes tries to affect that state given the resources at its disposal.\\nTo run a workload on a Kubernetes cluster, the user provides a plan that defines which pods \\nto create and how to manage them. This plan can be specified via configuration documents \\nwhich are sent to the cluster via Kubernetes’ APIs or client libraries. In this way, Kubernetes \\nis said to be a “declarative” system; users declare the state of the system they want, and \\nKubernetes tries to affect that state given the resources at its disposal.\\nWhen the master receives a new plan, it examines its requirements and compares them to \\nthe current state of the system. The master then takes the actions required to converge the \\nobserved and desired states. When pods are scheduled to a node, the node pulls the appropriate \\ncontainer images from an image registry and coordinates with the local container runtime to \\nlaunch the container.\\nFiles created within a container are ephemeral, meaning they don’t persist beyond the lifetime \\nof the container. This presents critical issues when building real-world applications in general \\nand with machine learning systems in particular. \\nFirst, most applications are stateful in some way. This means they store data about the requests \\nthey receive and the work they perform as those requests are processed. An example might \\nbe a long training job that stores intermediate checkpoints so that the job doesn’t need to \\nstart over from the beginning should a failure occur. If this intermediate data were stored in a \\ncontainer, it would not be available if a container fails or needs to be moved to another machine. \\nNext, many applications must access and process existing data. This is certainly the case \\nwhen building machine learning models, which requires that training data be accessible during \\ntraining. If our only option for data access was files created within a container, we would need \\nto copy data into each container that needed it, which wouldn’t be very practical.\\nTo address these issues, container runtimes like Docker provide mechanisms to attach \\npersistent storage to the containers they manage. However, these mechanisms are tied to \\nthe scope of a single container, are limited in capability, and lack flexibility.\\nKubernetes supports several abstractions designed to address the shortcomings of container-\\nbased storage mechanisms. Volumes allow data to be shared by all containers within a pod \\nand remain available until the pod is terminated. \\nPersistent volumes are volumes whose lifecycle is managed at the cluster level as opposed \\nto the pod level. Persistent volumes provide a way for cluster administrators to centrally \\nconfigure and maintain connections to external data sources and provide a mechanism for \\ngranting pods access to them. ', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 15}),\n",
       " Document(page_content='page 17 |  twimlai.com\\nFigure 4. Kubernetes volumes provide a flexible abstraction for accessing  \\n a wide variety of storage systems from containerized workloads\\nKubernetes has long supported first- and third-party volume plugins allowing users to access \\ndifferent types of storage resources. More recently, the general availability of support for the \\nContainer Storage Interface (CSI) specification has significantly expanded the ability to access \\na wide variety of software and hardware-based storage systems via community- and vendor-\\nprovided drivers.\\nBeyond CSI, Kubernetes offers additional capabilities that allow users to customize their \\ndeployments and support complex application scenarios, including:\\n• Custom resources, which extend the Kubernetes API to manage new types of objects\\n• Operators, which combine custom resources and controllers to manage stateful \\napplications and systems\\n• Scheduler extensions, which can be used to change the way Kubernetes manages pods \\nand assigns them to nodes\\n• The Container Network Interface (CNI)  which provides a common interface between \\ncontainers and the networking layer\\n• Device plugins, which allow Kubernetes nodes to discover new types of hardware \\nresources and perform vendor specific initialization and setup. Once made discoverable, \\nthe availability of these resources can then be considered by the Kubernetes scheduler \\nwhen making pod placement decisions.!\"#$%&#\\'()\\n!\"#$%&#\\'()\\nContainersPodCluster\\nCloud\\nStorag eData Lak e Files Database/\\nData warehous eVolume\\nPersistent\\nVolume!\"#$%&#\\'()\\n!\"#$%&#\\'()\\nContainersPod\\nVolume Volume\\nPersistent\\nVolume', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 16}),\n",
       " Document(page_content='page 18 |  twimlai.com\\n“Deep learning is an empirical science, and the quality of a group’s infrastructure \\nis a multiplier on progress. Fortunately, today’s open-source ecosystem makes it \\npossible for anyone to build great deep learning infrastructure.”  \\nOpenAI\\nKubernetes for Machine and Deep Learning\\nWhile the challenges faced by developers building traditional software are similar to those \\nfaced by data scientists and ML engineers, they aren’t the same. In many ways, those faced \\nby the latter are greater, significantly exacerbated by the highly iterative nature of the machine \\nlearning workflow. \\nStill, the capabilities provided by containers and Kubernetes can help organizations address \\nthe challenges presented by the machine learning process.\\nFigure 5. Containers and Kubernetes address major ML/DL challenges\\nData acquisition and preparation\\nBy providing a mechanism for connecting storage to containerized workloads, the Kubernetes \\nCSI enables support for a wide variety of stateful applications, including machine and deep \\nlearning. CSI drivers provide direct access to many different file, object, and block storage \\nsystems, as well as data fabric products which provide unified access to data stored in a \\nvariety of enterprise systems.Simplifying  \\nData ManagementDriving Efficient  \\nResource UseHiding  \\nComplexity\\nKubernetes provides  \\nconnectors for diverse  \\ndata sources and manages \\nvolume lifecycle\\nData workflow & pipeline \\nabstractions (3rd party) \\nallow complex data \\ntransformations to be  \\nexecuted across cluster\\nData fabrics (3rd party)  \\nextend scalable, multi-format \\nstorage to clusterKubernetes provides elasticity, \\nallowing cluster and workloads  \\nto be easily scaled up/down\\nMultitenancy allows multiple \\nteams to share cluster\\nUsers can deploy workloads \\non-demand via CLI, API  \\nwithout IT intervention\\nUsers and tools can  \\nprogram ma tically deploy \\nand control workloadsContainers provide a convenient \\nformat for packaging workloads \\nand declaring dependencies\\nKubernetes abstracts \\ninfrastructure, allowing users  \\nto think of cluster as  \\nunit of compute', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 17}),\n",
       " Document(page_content='page 19 |  twimlai.com\\nReady access to scalable storage is the foundation upon which users can construct automated \\nand repeatable data processing pipelines. This ensures the ability to reliably deliver properly \\ntransformed data to models, without manual intervention, and also facilitates the reuse of \\ntransformed data and processed features across projects.\\nDriving efficient resource use\\nFor resource-intensive workloads like machine learning, users face the constant conundrum \\nof over-allocation or under-utilization. That is, they must manage the fine balance between \\neither having more resources than is needed at any particular point in time, or not enough. \\nKubernetes offers two solutions to this problem. The first is multi-tenancy. A single physical \\nKubernetes cluster can be partitioned into multiple virtual clusters using Kubernetes’ \\nnamespaces feature. This allows a single cluster to easily support multiple teams, projects, \\nor functions. \\nEach namespace can be configured with its own resource quotas and access control policies, \\nmaking it possible to specify the actions a user can perform and the resources they have \\naccess to. Namespace-level configuration allows resources like storage to be configured on \\na per-team, -project, or -function basis.\\nMulti-tenancy features allow many users to share the same physical resources, with the \\naggregate usage having less variance and requiring less over-provisioning. Another solution \\nKubernetes offers to the resource efficiency problem is auto-scaling, of which there are  \\ntwo types. \\nWorkload (or pod) autoscaling which is the ability to scale up the number of pods running a \\ngiven workload as needed. This works with multi-tenancy to take full advantage of the resources \\navailable in a given Kubernetes cluster\\nCluster autoscaling is the ability to scale up or down the number of nodes in the entire cluster \\nwithout disrupting the cluster or any of its applications or users. This is helpful, but of limited \\nutility, when you’re running the cluster on physical nodes in your own datacenter. When you’re \\nrunning Kubernetes on virtual servers in the cloud, though, this allows you to continually \\nright-size your cluster so you’re only paying for the resources you need at any given time.\\nBecause Kubernetes-enabled workloads can be run on any Kubernetes cluster in any location \\nwithout any changes to the application’s code, users can move applications between \\ndeployment environments with relative ease. In theory a single Kubernetes cluster can span \\nclouds and/or on-premises data centers, but this is difficult to implement and not recommended.\\nHiding complexity\\nContainers provide an efficient way of packaging machine learning workloads that is \\nindependent of language or framework. Kubernetes builds on this to provide a reliable \\nabstraction layer for managing containerized workloads and provides the necessary \\nconfiguration options, APIs, and tools to manipulate these workloads declaratively.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 18}),\n",
       " Document(page_content='page 20 |  twimlai.com\\nWhile working with containers and containerized workloads may be a new skill for many data \\nscientists, these tools will be familiar to machine learning engineers exposed to modern \\nsoftware development practices. \\nCustom and third-party user interfaces, command-line tools, or integration with source code \\nrepositories can simplify creating and managing containers for end users. Because Kubernetes \\nis workload independent, once users are familiar with the tools required to work with their \\nKubernetes environment, they can apply these tools across a wide range of projects.\\nWhile introducing its own set of tools, the use of containers to facilitate data acquisition, \\nexperiment management and model deployment ensures that the correct and consistent \\ndependencies are in place wherever jobs are run—whether on the developer laptop, training \\nenvironment, or production cluster. This provides the valuable benefit of shielding those \\nworkloads from the complexity of the underlying technology stack, while supporting a wide \\nvariety of runtime configurations.\\nKubernetes also helps support and enforce a separation of concerns, giving end-users control \\nover defining their workloads and administrators control over the properties and configuration \\nof the infrastructure.\\nUnifying the ML workflow\\nFor all its benefits, Kubernetes alone won’t unify the end-to-end machine learning process. \\nIt will go a long way, however, in doing so for much of the software and hardware infrastructure \\nthat supports machine learning, and, in achieving that, it provides a strong platform for tools \\nthat go even further.\\nMLOps on Kubernetes with Kubeflow\\nKubeflow is an open source project designed to make machine learning workflows on \\nKubernetes simple, portable and scalable. Kubeflow is sponsored by Google and inspired by \\nTensorFlow Extended, or TFX, the company’s internal machine learning platform.\\nOriginally intended to simply allow TensorFlow users to run training jobs on their Kubernetes \\nclusters, the project now integrates a broad set of tools in support of many steps in an end-\\nto-end machine learning process.\\nKubeflow includes components for:\\n• Launching Jupyter Notebooks. Kubeflow includes support for setting up Jupyter notebook \\nservers and creating and managing notebooks. Notebooks launched via Kubeflow are \\nintegrated with the authentication and access control policies an organization has \\nestablished with its Kubernetes deployment, are easier to share across the organization, \\ncan help organizations standardize notebook images, and can readily take advantage \\nof the compute capacity and other features available in the Kubernetes environment.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 19}),\n",
       " Document(page_content='page 21 |  twimlai.com\\n• Building ML Pipelines. Kubeflow Pipelines is a platform for building and deploying \\ncomplex, repeatable ML workflows on Kubernetes. Pipelines themselves consist of a \\ngraph detailing the relationships between steps in a workflow. The workflow steps \\nthemselves are called components, and are provided by the user as Docker containers. \\nPipelines allow users to package up the various data access and transformation, model \\ntraining, and model evaluation steps that make up an experiment, simplifying \\nexperimentation and re-use.\\n• Training Models. Kubeflow provides a variety of Kubernetes Operators allowing users \\nto easily create and manage training jobs for ML frameworks like TensorFlow, PyTorch, \\nMXNet, XGBoost. In addition to the distributed training support provided by these \\nframeworks, it also supports MPI-based distributed training through the MPI Operator.\\n• Tracking Experiment Metadata . The Kubeflow Metadata project provides a Python SDK \\nfor tracking and managing information about the executions (runs), models, datasets, \\nand other artifacts generated during machine learning. It also provides a UI for viewing \\na list of logged artifacts and their details.\\n• Hyperparameter Tuning. Tuning model hyperparameters is an important element of \\noptimizing model performance and accuracy, yet manually adjusting hyperparameters \\nduring experimentation is tedious. Kubeflow includes Katib, an open-source hyperparameter \\ntuning project inspired by Google’s internal black-box optimization service, Vizier. Katib \\nis framework agnostic and deploys optimization tasks as Kubernetes pods.\\n• Serving Models. Users may choose from two model serving systems within Kubeflow— \\nKFServing and Seldon Core—as well as any of several standalone model serving systems. \\nThese options support a wide variety of ML frameworks, and offer features supporting \\nvarious strategies for model rollout, scaling, logging, and analytics.\\nFigure 6. Kubeflow builds on top of Kubernetes to address key challenges in the ML workflow\\nHyper-\\nparameter\\nTuning\\nJupyter\\nNotebooks\\nModel\\nServing\\nDashboard\\nFairing\\nPipelines\\nMetadata\\nStore\\nTraining\\nServerless\\nkubernetes', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 20}),\n",
       " Document(page_content='page 22 |  twimlai.com\\nIn addition, Kubeflow provides a central dashboard through which users can access and \\nmanage the various components of a Kubeflow deployment, support for multi-tenancy and \\nmulti-user isolation, and through Kubeflow Fairing, the ability to more easily build, train, and \\ndeploy training jobs across a hybrid cloud environment directly from a Jupyter notebook.\\nKubeflow exposes Kubernetes concepts like namespaces and pods to users and requires \\nthat they use command-line tools to perform tasks like configuring and submitting jobs. As \\na result, it is more geared towards users with engineering skill sets, rather than those of the \\ntraditional data scientist. The requisite job configuration files are not altogether unfamiliar, \\nhowever, with their configuration parameters resembling those of other distributed tools they \\nmay be familiar with, like Apache Spark.\\nFurthermore, the project is making progress in this regard, with the dashboard and pipeline \\nvisualizations representing solid progress. Some users choose to build custom user interfaces \\nfor their data scientists that aim to further abstract the underlying Kubeflow and Kubernetes \\ntooling.\\nUsing Kubeflow allows users to save time integrating support for common ML and DL \\ncomponents. In the past, Kubeflow’s integrations felt rather shallow, and the project felt a bit \\nlike a hodgepodge of third-party open source offerings. To its credit, the project is now a much \\nmore highly curated and unified offering.\\nThat said, Kubeflow is still a collection of components under the covers. Though the Kubeflow \\ncore has attained GA status, the overall project remains immature, with many of the \\naforementioned components still in beta. This state of affairs may prove problematic for some \\norganizations, who value the stability and predictability of more mature and tightly integrated \\nofferings. Further, some enterprise IT organizations might find that Kubeflow lacks an adequate \\nsecurity and governance framework that spans workloads, data and infrastructure, though \\nprogress is being made in this regard.\\nOn the topic of governance, though a different kind, there are concerns in some circles with \\nKubeflow’s lack of independent governance. While the Kubernetes project is overseen by the \\nCNCF, which provides for a certain level of transparency and community-driven governance, \\nKubeflow, remains under Google’s control. This creates some uncertainty about the future \\ndirection of the project, though the project does have a broad base of contributors and advocates.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 21}),\n",
       " Document(page_content='page 23 |  twimlai.com\\nThe Kubernetes MLOps Ecosystem\\nKubeflow is one of the many available offerings that take advantage of Kubernetes to deliver \\nMLOps capabilities. These offerings range in scope from narrow tools that aim to address \\nparticular pain-points experienced by data scientists, to end-to-end platforms that seek to \\nprovide comprehensive support for the ML workflow.    Licensing and delivery models vary \\nas well, including open source projects, commercially supported software products, and \\ncloud-based services.\\nThe ML/AI tools, infrastructure, and platforms landscape continues to evolve rapidly. For this \\nreason we have developed the TWIML ML/AI Solutions Guide , an online resource available \\nat twimlai.com/solutions.\\nVisit the Solutions Guide for profiles of ML/AI tools and platforms supporting Kubernetes and \\nKubeflow, and check back frequently for updates as we expand its coverage and features.\\nDriving Data Science Innovation with \\nKubernetes: Volvo Cars’ Journey\\nSweden-based Volvo Cars is a luxury automobile manufacturer whose brand has long been \\nassociated with safety and innovation. The company’s Consumer Digital group  is responsible \\nfor a variety of products that aim to enhance the owner experience, including the Volvo On \\nCall app, which offers features like roadside assistance, remote climate control, and the ability \\nto lock and unlock owners’ cars from a distance. \\nData scientists in the Consumer Digital group use machine learning to both deliver end-user \\nfeatures, like search and recommendations, as well as to better understand how their products \\nare being used.\\nThe group’s data scientists began using Kubernetes to support their workloads in 2018, when \\nthey needed a way to automate data acquisition, transformation, and reporting pipelines in \\nsupport of a high-profile collaboration with Amazon to allow Volvo owners to receive in-trunk \\ndeliveries. The team wanted to build these pipelines using Jupyter Notebooks, and found no \\noff-the-shelf tools at the company that let them easily do this. One of the most important distinctions among the various tools \\navailable to help you build out your organization’s machine learning \\nplatform is whether the tool aims to be wide or deep. This distinction, \\nand the interesting paradox that arises as a result, is explored in \\ndetail in TWIML ’s The Definitive Guide to Machine Learning Platforms.\\n THE DEFINITIVE GUIDE TO\\nMachine Learning \\nPlatforms\\nBy Sam Charrington\\nFounder, TWIML  \\nHost, TWIML AI Podcast\\nSPONSORED BY', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 22}),\n",
       " Document(page_content='page 24 |  twimlai.com\\nInspired by work done at Netflix6,  Leonard Aukea, senior data scientist and tech lead for the \\ncompany’s data science platform, set out to build a notebook-centric data science workflow \\nfor his team. \\nThe Netflix system, though container-based, used that company’s homegrown container \\norchestration system. At Volvo, however, Kubernetes was already gaining traction to support \\ncontainerized workloads. With the help of a DevOps colleague and considerable research, \\nLeonard was able to get a Kubernetes cluster up and running for data science workloads.\\nThe team’s first step was to set up the Apache Airflow workflow management system on \\nKubernetes, and use this in conjunction with the Netflix Papermill tool to run notebooks as \\nsteps of a pipeline. Pipeline outputs were fed into Nteract, another Netflix tool, providing a \\nconvenient, notebook-based dashboard.\\nAutomated pipelines were a big win for Leonard’s team, and other teams soon reached out \\nfor help with their own workloads. It was then that the limitations of the team’s initial approach \\nbecame apparent.\\nOne challenge was that the elements of the system weren’t tightly integrated. Users had to \\nvisit several different internal web sites to get anything done. \\n“Here, you visualized the notebook; here, you specify the DAG in this GitHub repository; here, \\nyou go to the Airflow UI; and then here, you go to JupyterHub. It became a mess. It was hard \\nto explain... and it didn’t scale in the way that we wanted.”\\nAfter obtaining the go-ahead to put a more enterprise-ready solution in place, Leonard \\neventually found the Kubeflow project.\\nThe team at Volvo appreciated that Kubeflow is Kubernetes-native, so everything fit together \\nunder a unified user interface and set of APIs. \\nKubeflow “glues all of these services together and gives a unified interface that a user could \\ngo in and enter the platform and do their job in an easier fashion.”\\n“Some things were working quite nicely... in terms of setup, and then moving from one version \\nto another, and just reliability of [the] service.”\\nThat didn’t mean that everything was perfect, however.\\nThe team found Kubeflow “hard to configure because you need to configure all of these \\nindividual services, dig into the manifests that you’re actually deploying, and [deploy them \\nusing the] kfctl tool. I’m not a hundred percent a fan of it… You have to spend significant [effort] \\nin order to make it work.”\\nThe integrated nature of the tool put more responsibility on Leonard and his team when it \\ncame to support.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 23}),\n",
       " Document(page_content='page 25 |  twimlai.com\\nThey often found themselves needing to “dig deeper into the individual applications and look \\nat source code, for example, where documentation is not centralized. [...] It’s also not what \\nyou would traditionally expect from a service in that sense.”\\nAs of this writing, Leonard’s data science platforms team has Kubeflow running in beta with \\nseveral teams on board. Adoption has been relatively smooth, though the team acknowledges \\na steep learning curve. The use of Docker containers and Kubeflow’s tooling is more natural \\nfor the Python data scientists they support than for the company’s R-based data scientists.\\nUltimately, the team sees in Kubeflow a way to help the company’s data scientists move more \\nquickly and be more self-sufficient. They want data science teams that deploy models to be \\nresponsible for them throughout their lifecycle, but also to have access to the tools and training \\nand best practices that are needed for them to be successful.\\nThe platform team is currently gathering statistics to compare user productivity on the new \\nplatform with previous results. \\n“I don’t have actual numbers on it, but what I hear from other teams and also [my own use], \\nit’s way more effective when you get the hang of it.”\\n“I’m hoping that we have a lot more models in production that would otherwise go to waste \\nor have been laying stagnant for a while in the organization because [a team] has not had the \\navailable tooling or the support in order to get their model to production. This could be very \\nvaluable for Volvo Cars as a company.”\\nGetting Started\\nMeeting the infrastructure needs of machine and deep learning development represents a \\nsignificant challenge for organizations moving beyond one-off ML projects to scaling the use \\nof AI more broadly in the enterprise.\\nFortunately, enterprises starting down this path need not start from scratch. The pairing of \\ncontainer technologies and Kubernetes addresses many of the key infrastructure challenges \\nfaced by organizations seeking to industrialize their use of machine learning. Together, they \\nprovide a means for delivering scalable data management and automation, the efficient \\nutilization of resources, and an effective abstraction between the concerns of data scientists \\nand MLEs and the needs of those providing the infrastructure upon which they depend.\\nIn addition to supplying many of the features required of a robust machine learning infrastructure \\nplatform right out of the box, the open source Kubernetes project—with its broad adoption, \\nactive community, plentiful third-party tools ecosystem, and multiple commercial support \\noptions—also checks the most important boxes for executives and business decision-makers \\ninvesting in a platform for future growth.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 24}),\n",
       " Document(page_content='page 26 |  twimlai.com\\nThis is a great time to be getting started with Kubernetes and MLOps. The community of ML/\\nAI-focused Kubernetes users is large, growing, and welcoming; the platform and tools are \\nviable for production workloads and maturing quickly; and there is a growing pool of talent \\nwith experience in the requisite tech stack. (And, conversely, the skills you learn helping your \\norganization scale its MLOps platform are portable and valued in the marketplace.)\\nWhile powerful, Kubernetes brings with it a degree of complexity that can be challenging for \\nsmall data science teams. It is most appropriate for those organizations whose scale, resources, \\nand requirements justify the investment not only in a centralized platform for data science \\nbut also a dedicated team to support it.\\nToday, organizations adopting Kubernetes for machine learning and AI have the benefit of \\nmany options to help mitigate its complexity. Each of the layers of the MLOps stack—from \\nthe physical infrastructure such as servers, network and storage; to the Kubernetes control \\nplane; to MLOps software providing abstractions such as pipelines and experiments—can be \\nimplemented on-premises using off-the-shelf components or can be obtained as a fully \\nmanaged service by various vendors.\\nAs with any new technology, the best course of action is to research and understand your \\nrequirements and options and start small, with your feet grounded firmly in your current needs \\nbut with an eye towards the future. Your exploration of Kubernetes can start with a “cluster” \\nrunning on your laptop or a handful of machines in the cloud, allowing you to experience and \\nappreciate firsthand what it brings to the table for your machine learning workloads. From \\nthere you can grow as your experience, needs, and resources allow.\\nGodspeed.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 25}),\n",
       " Document(page_content='HPE ML OPS AND INTEL:   \\nPARTNERING TO DELIVER ENTERPRISE MACHINE LEARNING AT SCALE\\nHewlett Packard Enterprise empowers large enterprises to overcome the barriers in deploying and \\noperationalizing AI/ML across the organization. HPE ML Ops brings DevOps-like speed and agility to \\nthe ML lifecycle. The HPE ML Ops solution supports every stage of ML lifecycle—data preparation, model \\nbuild, model training, model deployment, collaboration, and monitoring. HPE ML Ops is an end-to-end \\ndata science solution with the flexibility to run on-premises, in multiple public clouds, or in a hybrid \\nmodel, allowing customers to respond to dynamic business requirements for a variety of use cases.\\nHPE ML Ops offers: \\nLearn more at hpe.com/info/MLOps.\\nHPE solutions take advantage of the latest Intel® technology to deliver the performance required to \\nscale machine learning workloads across the enterprise on known and trusted infrastructure.\\nIntel innovations include:\\n• Proven, world-class 2nd generation Intel® Xeon® Scalable processors deliver scalable performance \\nfor a wide variety of analytics and other enterprise applications. Featuring Intel® Optane™ persistent \\nmemory (PMem), this powerful combination offers increased memory capacity close to the CPU \\nto support in-memory workloads, along with native persistence enabling greater resilience and \\nproductivity.\\n• Intel® Deep Learning Boost brings new embedded performance acceleration for advanced \\nanalytics and AI workloads, with up to 30x performance improvement  for Inference workloads \\ncompared to the previous generation – laying the foundations for future evolution of your analytics \\nand AI capabilities. \\npage 27SPONSOR CONTENT\\nModel building Prepackaged, self-service sandbox environments\\nModel training Scalable training environments with secure access to big data\\nModel deployment Flexible, scalable, endpoint deployment \\nModel monitoring End-to-end visibility to track, measure, report model performance across the ML pipeline\\nCollaboration Enable CI/CD workflows with code, model, and project repositories\\nSecurity and control Secure multitenancy with integration to enterprise authentication mechanisms\\nHybrid deployment Deploy on-premises, public cloud, or hybrid HPE ML Ops\\nBuild Deploy Train Data PrepData Engineers Data Scientists ML Architects\\nOn-Premises Public  Clouds EdgeMonito r\\nCollaborate \\nHPE C ontainer Pl atform\\nMulti-tenant multi-cluster management for containerized applica tions —with pre-integrated data fabric and persistent container storage', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 26}),\n",
       " Document(page_content='page 28• A track record of contributions to Kubernetes and Kubeflow, including support for AVX instruction \\nset feature discovery, enhancements to the Kubernetes scheduling framework, contribution of \\nthe CRI Resource Manager, and development of the Kubernetes Topology Manager which enhances \\nsupport for high-performance Non-Uniform Memory Access (NUMA) Nodes.\\nJD offers an example of what’s possible when Intel® innovations are delivered at scale. JD.com is China’s \\nlargest retailer, with hyperscale operations driven by its own internal needs as well as those of its cloud \\ncomputing and AI customers. The company uses machine learning and AI in support of a number of \\nretail use cases, including advertising, warehouse automation and robotics, drone delivery, facial \\nrecognition based payments, and personalized marketing.\\nIn 2016, JD.com began development of JDOS, its customized and optimized version Kubernetes which \\ntoday supports a wide range of workloads and applications. The company runs the world’s largest \\nproduction Kubernetes cluster with over 20,000 nodes.\\nIntel worked closely with JD to optimize their big data and machine learning frameworks and toolkits, \\nsuch as Apache Spark on Kubernetes, TensorFlow, and others, which are exposed to users via a single \\nKubernetes-based architecture, which it calls Moonshot.\\nIntel and JD collaborated on a custom version of the next-generation Intel® Xeon® Scalable processor \\nto support JD’s unique JD Cloud and eCommerce workloads. Beyond the CPU, JD also incorporated \\nother Intel innovations such as Intel® Optane™ DC SSDs , and plans to adopt our upcoming Intel® Optane™ \\nDC Persistent Memory, a new class of storage technology offering the unprecedented combination of \\nhigh-capacity, affordability, and persistence.\\nThese customizations provide significant performance gains on analytics and AI workloads, and as a \\nresult help JD differentiate their cloud service offerings, personalize the online shopping experience \\nand improve supply chain efficiency. SPONSOR CONTENT\\nMOONSHOT ARCHITECTURE\\n', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 27}),\n",
       " Document(page_content='page 29 |  twimlai.com\\nAbout TWIML\\nMachine learning and artificial intelligence are dramatically changing how businesses operate \\nand people live. Through our publications, podcast, and community, TWIML brings leading \\nideas and minds from the world of ML and AI to a broad and influential community of data \\nscientists, engineers and tech-savvy business and IT leaders.\\nTWIML has its origins in This Week in Machine Learning & AI, a podcast we launched in 2016 \\nto a small but enthusiastic reception. Fast forward to 2020 and the TWIML AI Podcast is now \\na leading voice in the field, with over eight million downloads and a large and engaged community \\nfollowing. Our offerings now include research reports like this one, as well as online meetups \\nand study groups, conferences, and a variety of educational content.\\nThe TWIML AI Podcast remains at the heart of our mission. By sharing and amplifying the \\nvoices of a broad and diverse spectrum of machine learning and AI researchers, practitioners, \\nand innovators, our programs help make ML and AI more accessible, and enhance the lives \\nof our audience and their communities.\\nTWIML was founded by Sam Charrington, a sought after industry analyst, speaker, commentator \\nand thought leader. Sam’s research is focused on the business application of machine learning \\nand AI, bringing AI-powered products to market, and AI-enabled and -enabling technology \\nplatforms.\\nConnect with us:\\n                  sam@twimlai.com \\n                  @samcharrington \\n                  @twimlai\\n                  www.linkedin.com/in/samcharrington\\n                  twimlai.com ', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 28}),\n",
       " Document(page_content='page 30 |  twimlai.com\\n1. Jeremy Lewi, et al, “Kubeflow 0.3 Simplifies Setup & Improves ML Development,” Kubeflow Blog, \\nOctober 29, 2018, https://medium.com/kubeflow/kubeflow-0-3-simplifies-setup-improves-\\nml- development-98b8ca10bd69\\n2. D. Sculley, et al, “Hidden Technical Debt in Machine Learning Systems,” Advances in Neural \\nInformation Processing Systems, https://papers.nips.cc/paper/5656-hidden-technical-debt-\\nin-machine-learning-systems.pdf\\n3. William Koehrsen, “Prediction Engineering: How to Set Up Your Machine Learning Problem,” \\nTowards Data Science, November 7, 2018, https://towardsdatascience.com/prediction-\\nengineering-how-to-set-up-your-machine-learning-problem-b3b8f622683b\\n4. Steve Lohr, “For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights,” The New York \\nTimes, August 17, 2014, https://www.nytimes.com/2014/08/18/technology/for-big-data-\\nscientists-hurdle-to-insights-is-janitor-work.html\\n5. Norman Jouppi, et al., “In-Datacenter Performance Analysis of a Tensor Processing Unit,”  \\narXiv [cs.AR], https://arxiv.org/abs/1704.04760v1\\n6. Michelle Ufford, et al, “Beyond Interactive: Notebook Innovation at Netflix,” The Netflix Tech Blog, \\nAugust 16, 2018, https://netflixtechblog.com/notebook-innovation-591ee3221233References', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 29}),\n",
       " Document(page_content='SPONSORED BY\\nCopyright © 2020 CloudPulse Strategies. CloudPulse, TWiML, and \\nthe CloudPulse Strategies and TWiML logos are trademarks of \\nCloudPulse Strategies, LLC.\\nThis document makes descriptive reference to trademarks that \\nmay be owned by others. The use of such trademarks herein is \\nnot an assertion of ownership of such trademarks by CloudPulse \\nand is not intended to represent or imply the existence of an \\nassociation between CloudPulse and the lawful owners of such \\ntrademarks. Information regarding third-party products, services \\nand organizations was obtained from publicly available sources, \\nand CloudPulse cannot confirm the accuracy or reliability of such \\nsources or information. Its inclusion does not imply an endorsement \\nby or of any third party.', metadata={'source': 'C:\\\\github_repos\\\\gemini-pdfs\\\\LLM Generic Application\\\\documents\\\\(This Week in ML) Sam Charrington - Kubernetes for MLOps - Scaling Enterprise Machine Learning, Deep Learning, and AI.pdf', 'page': 30})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=chunk_data(docs=doc)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, async_client=None, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding Technique of OpenAI\n",
    "embeddings=OpenAIEmbeddings(api_key=os.environ['OPENAI_API_KEY'])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.017560580216857424,\n",
       " -0.01885354648415544,\n",
       " 0.01628080892739946,\n",
       " -0.025199630470665424,\n",
       " -0.014974649544906484,\n",
       " 0.014077490703295432,\n",
       " -0.018127902796691586,\n",
       " 0.020634671052182585,\n",
       " -0.010482255053963557,\n",
       " -0.0072168575290536665,\n",
       " 0.026914789462717775,\n",
       " 0.0033280664519004023,\n",
       " 0.023655989426727912,\n",
       " 0.007117906371123836,\n",
       " -0.011682865789606676,\n",
       " 0.0038492105674152047,\n",
       " 0.036414126701012864,\n",
       " -0.01795638671122184,\n",
       " 0.01314734721105189,\n",
       " -0.018787579045535553,\n",
       " -0.01444690910462484,\n",
       " -0.007632453696210522,\n",
       " 0.02514685800988559,\n",
       " -0.0066693264846570365,\n",
       " -0.0185896748670308,\n",
       " 0.01597735796468994,\n",
       " 0.024368439998996808,\n",
       " -0.026584950406973246,\n",
       " 0.0031581997386606643,\n",
       " -0.020647866030022638,\n",
       " 0.05258938884420756,\n",
       " 0.004597943137792783,\n",
       " -0.009453160403790184,\n",
       " -0.00011750482745975717,\n",
       " -0.025133663032045537,\n",
       " 0.0015197291047232425,\n",
       " 0.021267961070636634,\n",
       " -0.02050273617494281,\n",
       " 0.00812721134850477,\n",
       " -0.010488851611561037,\n",
       " 0.01680855029900365,\n",
       " 0.017864031179566944,\n",
       " 0.012236993391600786,\n",
       " -0.03860425087859939,\n",
       " -0.025740564957464577,\n",
       " 0.007361986452810947,\n",
       " -0.005122385764936963,\n",
       " -0.01823344958089635,\n",
       " -0.007975484935827466,\n",
       " -0.003796436476820913,\n",
       " 0.015383649154465861,\n",
       " 0.013589329608598663,\n",
       " -0.04364417734526149,\n",
       " -0.002960296957139729,\n",
       " -0.0080414533057699,\n",
       " 0.008166791625412194,\n",
       " -0.030292331260752457,\n",
       " -0.0037469608978559976,\n",
       " 0.014328167342580023,\n",
       " 0.011247479018334835,\n",
       " 0.01935489976272462,\n",
       " -0.001698666201074833,\n",
       " 0.009169499113873103,\n",
       " 0.0077050181580891625,\n",
       " -0.010469061007446052,\n",
       " -0.008747306389118768,\n",
       " -0.018510514313215952,\n",
       " 0.004376951473358124,\n",
       " -0.010462464449848573,\n",
       " -0.011438785707919565,\n",
       " 0.03921115280401843,\n",
       " 0.048578555165073735,\n",
       " -0.011029786098360188,\n",
       " -0.005514893049180094,\n",
       " 0.010396497011228686,\n",
       " -0.015238520230708581,\n",
       " -0.011379414826897156,\n",
       " -0.023220601724133524,\n",
       " -0.02585930671950939,\n",
       " 0.009367402361055313,\n",
       " -0.004686999459326395,\n",
       " -0.013681684208931014,\n",
       " -0.013167136883844329,\n",
       " 0.0038129285693065213,\n",
       " -0.01056141653910095,\n",
       " 0.0030081233967052742,\n",
       " -0.007190470833002476,\n",
       " -0.004096589393562329,\n",
       " -0.0009507583168122573,\n",
       " -0.0014521122938733486,\n",
       " 0.01998818978117867,\n",
       " 0.024249698236951993,\n",
       " 0.02690159634752282,\n",
       " 0.016096099726734755,\n",
       " -0.01111554414109506,\n",
       " 0.019829866810903884,\n",
       " 0.024711470307291202,\n",
       " 0.0002704672433995755,\n",
       " 0.005165264786304399,\n",
       " -0.03789180030633049,\n",
       " -0.021492251479531307,\n",
       " 0.011940138986488746,\n",
       " -0.015924583641265012,\n",
       " -0.024276084467341908,\n",
       " -0.0013548100425122508,\n",
       " -0.002226407107017752,\n",
       " 0.0377334791987008,\n",
       " -0.0012525602565377249,\n",
       " 0.021399895947876408,\n",
       " -0.01041628761534367,\n",
       " -0.03256161785479892,\n",
       " 0.01041628761534367,\n",
       " 0.009690643927879817,\n",
       " -0.029843752305608207,\n",
       " -0.019843059926098843,\n",
       " -0.025186437355470468,\n",
       " 0.014433715058107333,\n",
       " -0.008852855035968625,\n",
       " -0.008298726502651968,\n",
       " -0.02145267027130134,\n",
       " 0.030793684539321638,\n",
       " 0.018919513922775327,\n",
       " 0.0321658132230796,\n",
       " -0.022455378691084795,\n",
       " 0.02366918254192287,\n",
       " -0.0155419711934181,\n",
       " -0.00843066231121429,\n",
       " -0.02564821128845477,\n",
       " -0.002439152841624926,\n",
       " -0.040530505301706356,\n",
       " 0.015159359676893734,\n",
       " 0.017652935748512323,\n",
       " -0.005640231368822389,\n",
       " -0.005874416614113281,\n",
       " 0.0006741066260420673,\n",
       " 0.005257619386636751,\n",
       " -0.020661059145217597,\n",
       " 0.008932015589783472,\n",
       " -0.011049576702475174,\n",
       " -0.013866394340918265,\n",
       " 0.011372818269299678,\n",
       " 0.02575375993530463,\n",
       " 0.006619850905692121,\n",
       " -0.0006460703831378924,\n",
       " -0.01741545222442269,\n",
       " -0.006243835481103961,\n",
       " 0.023814310534357605,\n",
       " 0.029553494458093646,\n",
       " -0.002229705618647128,\n",
       " -0.01600374419507986,\n",
       " 0.007361986452810947,\n",
       " -0.007751195458255338,\n",
       " -0.013813620017493337,\n",
       " 0.008687935508096361,\n",
       " -0.006590165465180917,\n",
       " -0.0004601241533006842,\n",
       " -0.013589329608598663,\n",
       " -0.009486144123100127,\n",
       " 0.0068870194046316835,\n",
       " -0.010178804091254039,\n",
       " -0.007427953891430835,\n",
       " -0.0019328513299504066,\n",
       " 0.010891254663522934,\n",
       " -0.003016369559363397,\n",
       " 0.0027310595957082194,\n",
       " 0.02700714313172758,\n",
       " 0.04287895431221276,\n",
       " 0.008450451984006728,\n",
       " 0.0022593910591583322,\n",
       " -0.0009664256649752033,\n",
       " -0.0027986764065581135,\n",
       " 0.030688137755116876,\n",
       " -0.043828888408571284,\n",
       " 0.007955695263035027,\n",
       " -0.008067840467482363,\n",
       " 0.022349830044234936,\n",
       " 0.004066903953051125,\n",
       " 0.006171271019225322,\n",
       " -0.031664456219220226,\n",
       " -0.021795702442240827,\n",
       " -0.012896669640444752,\n",
       " -0.015344068877558438,\n",
       " -0.006966180889769076,\n",
       " 0.024394826229386726,\n",
       " -0.0011923648058156321,\n",
       " 0.005475312306611398,\n",
       " 0.005966771214445633,\n",
       " -0.009598288396224917,\n",
       " 0.018945900153165246,\n",
       " 0.0008184108054766851,\n",
       " 0.021584605148541113,\n",
       " 0.014196232465340249,\n",
       " -0.02630788753729874,\n",
       " -0.013041798564202033,\n",
       " -0.667908946440711,\n",
       " -0.025226018563700436,\n",
       " 0.03567528989835405,\n",
       " -0.02679604770067296,\n",
       " 0.02971181556572334,\n",
       " 0.01662384109833895,\n",
       " 0.029500720134668715,\n",
       " 0.016148874050159686,\n",
       " -0.008694532065693839,\n",
       " 0.0257009856118797,\n",
       " 0.016518292451489093,\n",
       " 0.0016772268068064333,\n",
       " -0.014460102219819798,\n",
       " -0.0031812886215743887,\n",
       " -0.02083257523068734,\n",
       " -0.03725851215052153,\n",
       " 0.015317681715845974,\n",
       " -0.025806532396084464,\n",
       " -0.0074543410531432985,\n",
       " 0.007447744495545819,\n",
       " -0.01703283884525323,\n",
       " 0.00928164431832044,\n",
       " -0.010660367697030783,\n",
       " -0.022402604367659864,\n",
       " -0.0005953577750004719,\n",
       " 0.0065802701631234245,\n",
       " 0.02287757141583913,\n",
       " -0.011841187828558915,\n",
       " -0.003885492798354525,\n",
       " 0.004099887672361069,\n",
       " -0.043221986483152244,\n",
       " 0.023959440389437432,\n",
       " -0.011465172869632029,\n",
       " -0.002844853706724289,\n",
       " 0.04309005160591247,\n",
       " 0.0031153209501238645,\n",
       " -0.0361238707161434,\n",
       " 0.002110963856602312,\n",
       " 0.0024210116097399476,\n",
       " 0.05377680273936553,\n",
       " -0.018457739989791024,\n",
       " 0.0002129517315129474,\n",
       " 0.03103116806341127,\n",
       " 0.0217429281188159,\n",
       " -0.030872846955781578,\n",
       " 0.01942086720134451,\n",
       " 0.021294347301026553,\n",
       " -0.01469758574390943,\n",
       " -0.014209425580535206,\n",
       " 0.015832228109610113,\n",
       " 0.007599469976900579,\n",
       " 0.0065934637439796565,\n",
       " -0.006379069102803749,\n",
       " 0.010693351416340726,\n",
       " -0.01944725529437952,\n",
       " -0.012764734763204978,\n",
       " 0.029843752305608207,\n",
       " -0.01628080892739946,\n",
       " -0.007335599291098483,\n",
       " 0.012580025562540275,\n",
       " -0.00026984878702098223,\n",
       " 0.024473986783201573,\n",
       " -0.015106585353468805,\n",
       " -0.014710779790426934,\n",
       " -0.020331220089473065,\n",
       " 0.019552802078584283,\n",
       " -0.029263236610579085,\n",
       " -0.005030031164604611,\n",
       " 0.0011989614798284297,\n",
       " -0.022811603977219243,\n",
       " 0.022323443813845018,\n",
       " -0.006161375717167829,\n",
       " 0.010607593373605853,\n",
       " -0.017455031570007568,\n",
       " 0.0030724419287564285,\n",
       " 0.019882641134328815,\n",
       " 0.012632798954642657,\n",
       " -0.006840842104465507,\n",
       " 0.025846113604314432,\n",
       " 0.002912470517574183,\n",
       " -0.019618771379849263,\n",
       " -0.00998090084407183,\n",
       " -0.011386011384494635,\n",
       " 0.0022494957571008397,\n",
       " 0.03889451058875904,\n",
       " 0.008628564627073952,\n",
       " -0.029210462287154154,\n",
       " -0.011682865789606676,\n",
       " -4.7465763081507474e-05,\n",
       " -0.014341360457774982,\n",
       " 0.02432885879076684,\n",
       " 0.03572806235913389,\n",
       " -0.01370807137064348,\n",
       " -0.05010900495646138,\n",
       " 0.00789632438201262,\n",
       " 0.009644466162052367,\n",
       " -0.005158667763045646,\n",
       " 0.008443855426409248,\n",
       " 0.0018916215643976588,\n",
       " -0.008226162506434601,\n",
       " 0.002889381867491095,\n",
       " -0.008681338950498881,\n",
       " 0.0075532926767344025,\n",
       " -0.0006489564935021079,\n",
       " 0.01572668132540535,\n",
       " -0.007170680228887491,\n",
       " -0.0301867826139026,\n",
       " 0.024289277582536867,\n",
       " 0.014407328827717416,\n",
       " -0.023260182932363493,\n",
       " -0.008714322669808825,\n",
       " 0.0040174279084249365,\n",
       " -0.0067682781082481405,\n",
       " -0.00017543264602409996,\n",
       " 0.025252404794090355,\n",
       " -0.023853891742587573,\n",
       " 0.00824595217922704,\n",
       " 0.004403338635070588,\n",
       " -0.001695367805860775,\n",
       " -0.008674742392901402,\n",
       " 0.012388718872955545,\n",
       " -0.002952051027312243,\n",
       " 0.01369487825544852,\n",
       " -0.01968473881846915,\n",
       " -0.009248660599010496,\n",
       " 0.0060360373975255335,\n",
       " -0.007520308957424459,\n",
       " -0.00972362764718976,\n",
       " -0.021835281787825702,\n",
       " -0.015159359676893734,\n",
       " -0.026439822414538513,\n",
       " -0.0016862973063336041,\n",
       " -0.0045187821183166645,\n",
       " -0.011155124418002484,\n",
       " 0.029843752305608207,\n",
       " 0.009571902165835,\n",
       " 0.011306849899357244,\n",
       " -0.014512876543244726,\n",
       " -0.0040174279084249365,\n",
       " -0.022732441560759303,\n",
       " -0.0004155960216377078,\n",
       " 0.0052048450632118225,\n",
       " 0.009552111561720015,\n",
       " -0.011946736475408773,\n",
       " -0.01067356081222574,\n",
       " 0.0051520712054481665,\n",
       " -0.015845423087450166,\n",
       " 0.0054027483103940305,\n",
       " 0.0017662830119247268,\n",
       " 0.025028116247840775,\n",
       " 0.0074675346339995305,\n",
       " -0.0032010787600281003,\n",
       " 0.005973368237704386,\n",
       " -0.0019361497251644648,\n",
       " 0.004347266032846919,\n",
       " -0.0062867145024713975,\n",
       " -0.01230296176154322,\n",
       " -0.03335323084352775,\n",
       " -0.029210462287154154,\n",
       " -0.024764244630716133,\n",
       " 0.017890419272601953,\n",
       " 0.020951316992732158,\n",
       " -0.036968256165652066,\n",
       " -0.008450451984006728,\n",
       " 0.0030213169775615066,\n",
       " 0.014090683818490391,\n",
       " 0.006243835481103961,\n",
       " 0.0015725031953175343,\n",
       " -0.0024325560511968096,\n",
       " -0.00463752388036148,\n",
       " -0.0301867826139026,\n",
       " -0.04435662791753039,\n",
       " 0.00913651539456316,\n",
       " -0.005439030308502714,\n",
       " 0.010013884563381774,\n",
       " -0.014895488991091638,\n",
       " -0.008753902946716248,\n",
       " 0.00441983049472556,\n",
       " -0.0010159014185630963,\n",
       " 0.012316154876738178,\n",
       " 0.012507460635000361,\n",
       " -0.0009540567120263153,\n",
       " -0.004528676954712883,\n",
       " 0.007962291820632506,\n",
       " 0.025780146165694545,\n",
       " 0.014460102219819798,\n",
       " 0.0025100679312735595,\n",
       " 0.040345797963686744,\n",
       " -0.029157687963729226,\n",
       " 0.015621132678555492,\n",
       " -0.0038129285693065213,\n",
       " 0.023260182932363493,\n",
       " -0.011874171547868859,\n",
       " 0.011425591661402059,\n",
       " 0.004089992370303577,\n",
       " -0.017771677510557138,\n",
       " -0.012613009281850218,\n",
       " 0.018695223513880654,\n",
       " 0.006563778303468453,\n",
       " -0.007124502928721315,\n",
       " 0.025133663032045537,\n",
       " 0.0017877225226084449,\n",
       " 0.02286437830064417,\n",
       " -0.021835281787825702,\n",
       " -0.006718802063621952,\n",
       " -0.013318862365199087,\n",
       " -0.019302125439299694,\n",
       " -0.008470242588121714,\n",
       " 0.008800080712543696,\n",
       " 0.01829941888216133,\n",
       " 0.020674252260412557,\n",
       " 0.0007594522093041196,\n",
       " -0.0019031658894392027,\n",
       " 0.009618079000339903,\n",
       " 0.005082805022368266,\n",
       " 0.002778886035273765,\n",
       " -0.021716540025780887,\n",
       " -0.014816327505954246,\n",
       " -0.01914380433167,\n",
       " 0.020357608182508077,\n",
       " 0.011504753146539452,\n",
       " 0.018141095911886545,\n",
       " 0.017547387101662464,\n",
       " -0.007243244690766131,\n",
       " -0.009472950076582623,\n",
       " 0.006939793728056612,\n",
       " 0.01829941888216133,\n",
       " 0.037575158091071106,\n",
       " 0.01513297251518127,\n",
       " -0.0023006207082957616,\n",
       " -0.03132142777357092,\n",
       " 0.014433715058107333,\n",
       " 0.005570965651403762,\n",
       " 0.017995966056806718,\n",
       " 0.021571412033346154,\n",
       " 0.010106239163714125,\n",
       " 0.021545025802956235,\n",
       " 0.0036776949476067335,\n",
       " 0.010013884563381774,\n",
       " -0.011729042624111579,\n",
       " 0.01980348058051397,\n",
       " -0.006233940179046469,\n",
       " 0.02000138289637363,\n",
       " -0.01410387693368535,\n",
       " 0.023576827010267972,\n",
       " 0.02826052819079563,\n",
       " 0.03382819416641683,\n",
       " 0.011603704304469283,\n",
       " -0.02913130173333931,\n",
       " -0.021901249226445593,\n",
       " -0.01847093310498598,\n",
       " -0.03377542170563699,\n",
       " -0.00843725886881177,\n",
       " 0.005801852152234641,\n",
       " 0.004729878480693832,\n",
       " -0.03177000486607008,\n",
       " -0.004677104157268903,\n",
       " 0.0016046623449277931,\n",
       " 0.02025205953565822,\n",
       " 0.018154289027081504,\n",
       " 0.015027423868331414,\n",
       " 0.0015518882543335013,\n",
       " 0.011207898741427412,\n",
       " -0.021795702442240827,\n",
       " 0.019790285602673915,\n",
       " -0.01230296176154322,\n",
       " -0.010211787810563982,\n",
       " -0.0032489054324242824,\n",
       " 0.004802442942572472,\n",
       " 0.0109638186597403,\n",
       " -0.004228524270802104,\n",
       " -0.00020315966073897827,\n",
       " -0.00198562530412938,\n",
       " -0.017890419272601953,\n",
       " 0.007513711934165707,\n",
       " 0.0237219568653478,\n",
       " -2.7073137156695793e-06,\n",
       " 0.004314282313536976,\n",
       " -0.01597735796468994,\n",
       " 0.015462810639603253,\n",
       " 0.008179984740607153,\n",
       " -0.007335599291098483,\n",
       " 0.007790776200824035,\n",
       " 0.002140649297113516,\n",
       " 0.015119778468663764,\n",
       " -0.025318372232710242,\n",
       " -0.012289767715025714,\n",
       " -0.004531975699172896,\n",
       " -0.014196232465340249,\n",
       " 0.027495305157746897,\n",
       " 0.002905873727146067,\n",
       " 0.019183383677254875,\n",
       " -0.005735884713614754,\n",
       " 0.0089847899132084,\n",
       " -0.003605130485728094,\n",
       " -0.009321224595227863,\n",
       " 0.019552802078584283,\n",
       " -0.006814455408414317,\n",
       " 0.004772757502061268,\n",
       " -0.00812721134850477,\n",
       " -0.007916114054805056,\n",
       " 0.006913406566344147,\n",
       " 0.013140749722131863,\n",
       " 0.007665437415520466,\n",
       " 0.032324134330709284,\n",
       " 0.010699947973938204,\n",
       " 0.004261508455773321,\n",
       " -0.040293021777616726,\n",
       " 0.014275393019155095,\n",
       " -0.03467258334121569,\n",
       " 0.024144149590102134,\n",
       " -0.01911741623863499,\n",
       " -0.020924928899697146,\n",
       " -0.015080198191756341,\n",
       " -0.011353027665184692,\n",
       " 0.023787924303967686,\n",
       " -0.018932707037970287,\n",
       " -0.0036677996455492415,\n",
       " 0.02423650325911194,\n",
       " 0.014671198582196966,\n",
       " -0.006952987308912844,\n",
       " -0.0035820418356450057,\n",
       " 0.0039019848908401333,\n",
       " 0.0038887913099839013,\n",
       " 0.05335461187725629,\n",
       " 0.009703837043074775,\n",
       " 0.014961456429711526,\n",
       " 0.04063605394855622,\n",
       " -0.011451978823114523,\n",
       " 0.007540099095878171,\n",
       " 0.006464826679877348,\n",
       " -0.015225327115513622,\n",
       " 0.004235121294060857,\n",
       " -0.005112490462879471,\n",
       " -0.03770709296831088,\n",
       " -0.0019674843050750384,\n",
       " -0.0045946448589940445,\n",
       " -0.00042260506781183676,\n",
       " 0.028867430116214666,\n",
       " 0.013523362169978776,\n",
       " -0.018378579435976178,\n",
       " -0.010891254663522934,\n",
       " -0.004324177615594469,\n",
       " -0.027073112432992562,\n",
       " 0.014222618695730166,\n",
       " -0.0025216123727304215,\n",
       " 0.026584950406973246,\n",
       " 0.034092067646186566,\n",
       " 0.019605576402009214,\n",
       " 0.02630788753729874,\n",
       " 0.01703283884525323,\n",
       " 0.024962146946575794,\n",
       " 0.020225673305268303,\n",
       " -0.02738975651089704,\n",
       " -0.006966180889769076,\n",
       " 0.021505444594726267,\n",
       " 0.014314973296062518,\n",
       " 0.006570374861065932,\n",
       " -0.0160829066115398,\n",
       " 0.02257412045312961,\n",
       " -0.011108947583497581,\n",
       " 0.032904650025738405,\n",
       " 0.010627383977720837,\n",
       " 0.020634671052182585,\n",
       " -0.011003398936647724,\n",
       " 0.016966871406633343,\n",
       " 0.014473296266337303,\n",
       " -0.017745289417522125,\n",
       " 0.030978395602631437,\n",
       " -0.022626894776554538,\n",
       " -0.01072633513565067,\n",
       " 0.036968256165652066,\n",
       " -0.018035547265036686,\n",
       " -0.0018602868680717667,\n",
       " -0.0005108367614381046,\n",
       " 0.01216442939538342,\n",
       " -0.036229415637703065,\n",
       " -0.011465172869632029,\n",
       " 0.005257619386636751,\n",
       " 0.01906464191521006,\n",
       " -0.007863340662702676,\n",
       " -0.010403093568826165,\n",
       " -0.008120613859584744,\n",
       " -0.04031941173329683,\n",
       " -0.025912081042934323,\n",
       " -0.004221927713204624,\n",
       " -0.02340531278744332,\n",
       " -0.007065132047698907,\n",
       " 0.020964510107927114,\n",
       " -0.011471769427229508,\n",
       " 0.0011956630846143717,\n",
       " -0.03240329674716923,\n",
       " -0.047496686191475436,\n",
       " -0.01762654765547731,\n",
       " 0.003750259176654737,\n",
       " -0.02030483385908315,\n",
       " -0.03250884539401908,\n",
       " 0.010191997206448998,\n",
       " 0.026109985221439078,\n",
       " -0.001668156190863944,\n",
       " 0.00918269316039061,\n",
       " -0.007361986452810947,\n",
       " -0.014460102219819798,\n",
       " 0.004393443333013096,\n",
       " 0.020238866420463263,\n",
       " -0.0018965690990110867,\n",
       " 0.008938613078703497,\n",
       " -0.0323505205610992,\n",
       " -0.009717030158269734,\n",
       " -0.007586276396044347,\n",
       " 0.0011255725646654232,\n",
       " 0.018035547265036686,\n",
       " 0.0037931381980221735,\n",
       " 0.008344904268479418,\n",
       " 0.015766260670990226,\n",
       " 0.003822823638533377,\n",
       " 0.022125539635340263,\n",
       " -0.026083597128404065,\n",
       " 0.01339802385033648,\n",
       " -0.017468226547847618,\n",
       " 0.02823414196040571,\n",
       " 0.013602523655116168,\n",
       " -0.0132528949265792,\n",
       " -0.03295742248651824,\n",
       " 0.009934724009566928,\n",
       " -0.01657106677491402,\n",
       " 0.005841432894803338,\n",
       " -0.004482499654546708,\n",
       " -0.016214841488779573,\n",
       " -0.0036150255549549496,\n",
       " -0.010851673455292964,\n",
       " -0.003177990109945012,\n",
       " -0.010891254663522934,\n",
       " -0.0020037665360143585,\n",
       " 0.0026584951338295792,\n",
       " -0.01885354648415544,\n",
       " -0.005422538448847742,\n",
       " -0.019526415848194367,\n",
       " 0.011478365984826988,\n",
       " 0.006821051966011796,\n",
       " -0.005848029452400818,\n",
       " 0.018668837283490735,\n",
       " -0.006573673605525945,\n",
       " 0.023748343095737718,\n",
       " 0.012467880358092938,\n",
       " -0.029263236610579085,\n",
       " 0.011009995494245204,\n",
       " -0.004357161334904412,\n",
       " 0.013681684208931014,\n",
       " 0.0015213782441226122,\n",
       " -0.0022544434081295858,\n",
       " -0.015489197801315718,\n",
       " -0.028999366856099534,\n",
       " 5.303178317735822e-05,\n",
       " -0.01485590778286167,\n",
       " 0.027627240034986674,\n",
       " -0.007401567195379643,\n",
       " -0.002234653269675874,\n",
       " -0.029949299089812968,\n",
       " -0.03169084617490033,\n",
       " -0.0006967829912753128,\n",
       " 0.027336982187472113,\n",
       " -0.006834245546868028,\n",
       " -0.0296590431049435,\n",
       " -0.03031871935378747,\n",
       " 0.016439131897674247,\n",
       " 0.01940767408614955,\n",
       " -0.02312824805512372,\n",
       " -0.04287895431221276,\n",
       " -0.030292331260752457,\n",
       " -0.039290315220478364,\n",
       " 0.0014776746530554912,\n",
       " -0.0006765803933487813,\n",
       " 0.01354974933169124,\n",
       " -0.007916114054805056,\n",
       " 0.02855078603831019,\n",
       " -0.012606412724252739,\n",
       " -0.03026594503036254,\n",
       " 0.0042911938962845245,\n",
       " -0.01965835072543414,\n",
       " 0.0075532926767344025,\n",
       " -0.01723074302375799,\n",
       " 0.03710019104289184,\n",
       " 0.016637034213533908,\n",
       " 0.023035892523468823,\n",
       " -0.010356916734321263,\n",
       " 0.03733767456698147,\n",
       " 0.009947917124761885,\n",
       " 0.005973368237704386,\n",
       " -0.014090683818490391,\n",
       " -0.025793339280889505,\n",
       " 0.009505933795892567,\n",
       " 0.007051938466842675,\n",
       " -0.012883476525249793,\n",
       " -0.004113081253217301,\n",
       " -0.027152272986807408,\n",
       " 0.0004370354741137667,\n",
       " 0.030081235829697836,\n",
       " -0.012639396443562682,\n",
       " -0.0008699479833137905,\n",
       " 0.0036743964359773574,\n",
       " -0.013919167733020647,\n",
       " -0.040952699889105784,\n",
       " -0.03601832206929354,\n",
       " -0.010442674777056135,\n",
       " 0.011320043945874748,\n",
       " -0.006606657324835888,\n",
       " -0.006972777447366556,\n",
       " 0.008681338950498881,\n",
       " -0.000601954565428588,\n",
       " -0.00213570164608477,\n",
       " 0.0155419711934181,\n",
       " 0.015027423868331414,\n",
       " -0.01855009552144592,\n",
       " 0.026136371451828993,\n",
       " -0.008081033582677321,\n",
       " 0.006382367381602489,\n",
       " -0.012870282478732288,\n",
       " 0.023774731188772727,\n",
       " -0.023880277972977492,\n",
       " -0.0035688482547887737,\n",
       " -0.0026139669730627733,\n",
       " -0.01056141653910095,\n",
       " 0.031242265357110985,\n",
       " 0.016676615421763876,\n",
       " 0.0010686755091573882,\n",
       " 0.00754669565347565,\n",
       " 0.0055445784896912985,\n",
       " -0.01126726962244982,\n",
       " 0.019249351115874763,\n",
       " -0.03345877576508743,\n",
       " 0.004475903096949228,\n",
       " -0.0027607450362194235,\n",
       " -0.02343169901783324,\n",
       " -0.004848620242738647,\n",
       " -0.040477732840926525,\n",
       " -0.021782509327045868,\n",
       " -0.005221337388528068,\n",
       " -0.017586968309892436,\n",
       " 0.005501699468323862,\n",
       " -0.01558155240164807,\n",
       " 0.005653424949678622,\n",
       " -0.0054588204469564255,\n",
       " -0.01513297251518127,\n",
       " 0.0026403541347752375,\n",
       " -0.0052048450632118225,\n",
       " 0.03609748076046329,\n",
       " -0.008529613469144121,\n",
       " 0.013226507764866736,\n",
       " 0.014750360067334357,\n",
       " 0.007170680228887491,\n",
       " 0.014486489381532262,\n",
       " -0.022442183713244742,\n",
       " 0.0006815279861698683,\n",
       " -0.01200610735643118,\n",
       " 0.02051592929013777,\n",
       " 0.02968542933533342,\n",
       " 0.0008988088541253095,\n",
       " 0.0009837421525375192,\n",
       " -0.002785482825701881,\n",
       " -0.009486144123100127,\n",
       " 0.0019410972597778927,\n",
       " -0.02630788753729874,\n",
       " 0.00729601901419106,\n",
       " 0.01329907269240665,\n",
       " 0.001078570578384244,\n",
       " -0.011293656784162285,\n",
       " -0.024988535039610803,\n",
       " 0.014222618695730166,\n",
       " 0.014367747619487446,\n",
       " -0.004512185095057911,\n",
       " -0.006069021116835477,\n",
       " 0.00448909667780546,\n",
       " -0.002712918363823241,\n",
       " -0.007922711543725083,\n",
       " 0.01935489976272462,\n",
       " -0.012335945480853164,\n",
       " 0.027020338109567634,\n",
       " 0.008588984350166529,\n",
       " -0.009717030158269734,\n",
       " 0.0037040818764885615,\n",
       " 0.0008002696900070251,\n",
       " 0.0031169700895232345,\n",
       " -0.007929308101322563,\n",
       " -0.004766160478802515,\n",
       " 0.019275739208909775,\n",
       " 0.007117906371123836,\n",
       " -0.00854280658433908,\n",
       " 0.008351500826076897,\n",
       " 0.021822088672630746,\n",
       " -0.012824105644227386,\n",
       " 0.0015296241739500983,\n",
       " -0.02169015379539097,\n",
       " 0.02253453924489964,\n",
       " -0.011887365594386363,\n",
       " -0.013015411402489569,\n",
       " -0.004802442942572472,\n",
       " -0.001120624913636677,\n",
       " -0.00012925530220217975,\n",
       " -0.01829941888216133,\n",
       " -0.0012368928501671197,\n",
       " 0.004314282313536976,\n",
       " -0.007038745351647716,\n",
       " 0.010594400258410894,\n",
       " 0.026453015529733472,\n",
       " -0.009862159082027014,\n",
       " 0.010238174972276446,\n",
       " -0.0282077538673707,\n",
       " -0.013239701811384242,\n",
       " -0.01455245682015215,\n",
       " -0.016729387882543714,\n",
       " -0.007777582619967803,\n",
       " 0.019315318554494653,\n",
       " -0.01112873725629002,\n",
       " -0.028313302514220557,\n",
       " 0.0018833756345701727,\n",
       " -0.011834591270961436,\n",
       " 0.009743417319982198,\n",
       " 0.004024024931683689,\n",
       " -0.004439621098840545,\n",
       " 0.006873826289436724,\n",
       " 0.04087353747264585,\n",
       " -0.01703283884525323,\n",
       " 0.00361172727615621,\n",
       " 0.039949989606677234,\n",
       " -0.0011478366450488262,\n",
       " -0.021914444204285642,\n",
       " 0.030767298308931722,\n",
       " -0.015251714277226085,\n",
       " -0.021676960680196013,\n",
       " 0.016148874050159686,\n",
       " 0.0001690420271247347,\n",
       " -0.02682243393106288,\n",
       " -0.0028069223363855994,\n",
       " 0.0018190571025190187,\n",
       " 0.010640577092915797,\n",
       " 0.006913406566344147,\n",
       " -0.009763207924097184,\n",
       " 0.004235121294060857,\n",
       " 0.01056141653910095,\n",
       " 0.01156412402756186,\n",
       " -0.025898887927739363,\n",
       " -0.013048395121799512,\n",
       " 0.007315809152644771,\n",
       " -0.026189145775253924,\n",
       " -0.00032880735408971813,\n",
       " 0.026743273377248033,\n",
       " -0.036229415637703065,\n",
       " -0.011432189150322085,\n",
       " -0.009717030158269734,\n",
       " 0.0014702532929276903,\n",
       " -0.005472014027812658,\n",
       " -0.01711200126171317,\n",
       " -0.02199360475810049,\n",
       " 0.008668145835303922,\n",
       " -0.0018091620332921629,\n",
       " -0.011762026343421522,\n",
       " 0.0009037563887387374,\n",
       " -0.00017532956025972455,\n",
       " -0.02249495803666967,\n",
       " -0.02081938025284729,\n",
       " 0.009848965966832055,\n",
       " 0.009261853714205456,\n",
       " -0.001015076732448093,\n",
       " 0.019302125439299694,\n",
       " 0.017652935748512323,\n",
       " 0.020436768736322924,\n",
       " 0.014130264095397815,\n",
       " 0.008212968459917097,\n",
       " -0.021861669880860714,\n",
       " -0.01185438187507642,\n",
       " -0.005577562209001242,\n",
       " -0.016373162596409266,\n",
       " 0.0027277610840788433,\n",
       " -0.004297790453882004,\n",
       " 0.044039981976980815,\n",
       " 0.0026387047625452313,\n",
       " -0.019381287855759634,\n",
       " -0.025832920489119476,\n",
       " 0.0002925251813580573,\n",
       " 0.015423229431373284,\n",
       " -0.005735884713614754,\n",
       " 0.011999509867511153,\n",
       " -0.002366588379746286,\n",
       " 0.010772512901478118,\n",
       " 0.016399550689444275,\n",
       " 0.010093046048519166,\n",
       " 0.020476349944552892,\n",
       " 0.008344904268479418,\n",
       " 0.017943193596026884,\n",
       " 0.007665437415520466,\n",
       " 0.014262199903960136,\n",
       " 0.004627628578303988,\n",
       " -0.021201993632016747,\n",
       " 0.007421357333833355,\n",
       " -0.02744253083432197,\n",
       " -0.029236850380189167,\n",
       " 0.0002451109393308653,\n",
       " 0.01455245682015215,\n",
       " 0.006425246402969925,\n",
       " 0.007427953891430835,\n",
       " 0.026637724730398174,\n",
       " 0.02738975651089704,\n",
       " -0.001137941459406652,\n",
       " -0.025766953050499586,\n",
       " -0.015396842269660819,\n",
       " -0.007263034829219843,\n",
       " 0.032693552732038694,\n",
       " 0.006794664804299332,\n",
       " -0.02512046991685058,\n",
       " 0.008899031870473528,\n",
       " 0.03607109453007337,\n",
       " 0.0007318283094574462,\n",
       " -0.002165387086595974,\n",
       " 0.0010637278581286421,\n",
       " 0.011188108137312427,\n",
       " 0.0003797261046520598,\n",
       " 0.022996313177883945,\n",
       " 0.004014129629626197,\n",
       " -0.00526091766543549,\n",
       " -0.011161721906922509,\n",
       " -0.01823344958089635,\n",
       " -0.0031614982502900404,\n",
       " -0.00707172907095766,\n",
       " 0.0002599536595864673,\n",
       " 0.019539608963389323,\n",
       " -0.001152784179662254,\n",
       " -0.03345877576508743,\n",
       " -0.018959095131005296,\n",
       " -0.007724808296542874,\n",
       " -0.019777092487478956,\n",
       " -0.0214658633864963,\n",
       " 0.01329907269240665,\n",
       " -0.006355980219890024,\n",
       " -0.01741545222442269,\n",
       " 0.019869448019133856,\n",
       " -0.004851918521537387,\n",
       " -0.010198593764046476,\n",
       " -0.02670369216901806,\n",
       " -0.009862159082027014,\n",
       " 0.0032126232014849623,\n",
       " 0.01066696425462826,\n",
       " -0.007665437415520466,\n",
       " -0.0017910209178225028,\n",
       " 0.002816817405612455,\n",
       " 0.004235121294060857,\n",
       " -0.004314282313536976,\n",
       " -0.0017761781975669008,\n",
       " 0.001118975774237307,\n",
       " -0.012078671352648547,\n",
       " 0.009024370190115823,\n",
       " -0.00827893682985953,\n",
       " 0.00023274201548580684,\n",
       " 0.02826052819079563,\n",
       " -0.023840698627392614,\n",
       " -0.011610300862066762,\n",
       " 0.016359969481214306,\n",
       " 0.018444546874596065,\n",
       " 0.0025001728620467037,\n",
       " 0.015067005076561382,\n",
       " 0.23896110828798497,\n",
       " -0.0017893716620078148,\n",
       " 0.002475435072564246,\n",
       " 0.036414126701012864,\n",
       " 0.01513297251518127,\n",
       " -0.01572668132540535,\n",
       " 0.015119778468663764,\n",
       " 0.012942847406272202,\n",
       " 0.0036677996455492415,\n",
       " 0.024619116638281396,\n",
       " 0.010257964645068885,\n",
       " 0.01042288417294115,\n",
       " -0.012975831125582146,\n",
       " -0.00607891641889297,\n",
       " -0.004198839295952174,\n",
       " -0.022217895166995162,\n",
       " -0.027231433540622255,\n",
       " 0.006059125814777985,\n",
       " -0.021267961070636634,\n",
       " -0.0063625772431487775,\n",
       " 0.005864521312055789,\n",
       " -0.021201993632016747,\n",
       " 0.00035045297809452785,\n",
       " -0.009994094890589335,\n",
       " 0.017600161425087392,\n",
       " -0.0023369029392350817,\n",
       " 0.005254321107838011,\n",
       " 0.037575158091071106,\n",
       " 0.009077144513540752,\n",
       " 0.022059572196720376,\n",
       " -0.021558218918151194,\n",
       " -0.009624675557937381,\n",
       " 0.017151580607298048,\n",
       " -0.010502045658078543,\n",
       " 0.0026964265041682692,\n",
       " -0.01825983767393136,\n",
       " 0.011293656784162285,\n",
       " -0.00785014661618517,\n",
       " 0.030424266137992234,\n",
       " 0.025635018173259812,\n",
       " 0.002823414196040571,\n",
       " -0.037548768135391,\n",
       " 0.011254076507254861,\n",
       " -0.00957849872343248,\n",
       " 0.005963472935646893,\n",
       " 0.007243244690766131,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors=embeddings.embed_query(\"What about Kubernetes?\")\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Search DB In Pinecone\n",
    "#pinecone.init(\n",
    "#    api_key=os.environ['PINECONE_API_KEY'],\n",
    "#    environment=os.environ['PINECONE_ENVIRONMENT']\n",
    "#)\n",
    "#index_name=os.environ['PINECONE_INDEX_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "try:\n",
    "    pinecone_api_key = os.environ['PINECONE_API_KEY']\n",
    "    pinecone_environment = os.environ['PINECONE_ENVIRONMENT']\n",
    "    pinecone_client = Pinecone(\n",
    "        api_key=pinecone_api_key,\n",
    "        environment=pinecone_environment\n",
    "    )\n",
    "except KeyError as e:\n",
    "    print(f\"Environment variable {e.args[0]} not found\")\n",
    "    exit(1)\n",
    "\n",
    "# Get the index name\n",
    "try:\n",
    "    index_name = os.environ['PINECONE_INDEX_NAME']\n",
    "except KeyError:\n",
    "    print(\"PINECONE_INDEX_NAME environment variable not found\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=pinecone.from_documents(doc,embeddings,index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity Retrieve Results from VectorDB\n",
    "def retrieve_query(query, k=2):\n",
    "    matching_results=index.similarity_search(query,k=k)\n",
    "    return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\gemini\\lib\\site-packages\\langchain_community\\llms\\openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\anaconda3\\envs\\gemini\\lib\\site-packages\\langchain_community\\llms\\openai.py:1069: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm=OpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "chain=load_qa_chain(llm,chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Answers From VectorDB\n",
    "def retrieve_answers(query):\n",
    "    doc_search=retrieve_query(query)\n",
    "    print(doc_search)\n",
    "    response=chain.run(input_documents=doc_search,question=query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_query= \"What about Kubernetes?\"\n",
    "answer=retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
